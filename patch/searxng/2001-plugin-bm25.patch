diff --git a/searx/autocomplete.py b/searx/autocomplete.py
index 31121c7e762..06d32825313 100644
--- a/searx/autocomplete.py
+++ b/searx/autocomplete.py
@@ -2,12 +2,17 @@
 """This module implements functions needed for the autocompleter.
 
 """
-# pylint: disable=use-dict-literal
+# pylint: disable=use-dict-literal,too-many-locals
 
 import json
 import html
+import re
 from urllib.parse import urlencode, quote_plus
 
+import numpy as np
+import bm25s
+import tiktoken
+
 import lxml.etree
 import lxml.html
 from httpx import HTTPError
@@ -22,6 +27,58 @@
 from searx.exceptions import SearxEngineResponseException
 from searx.utils import extr, gen_useragent
 
+# Tiktoken 配置常量
+PRIMARY_MODEL = "gpt-oss-120b"
+FALLBACK_ENCODING = "o200k_harmony"
+
+# 初始化 tiktoken 编码器
+try:
+    _tokenizer = tiktoken.encoding_for_model(PRIMARY_MODEL)
+except (KeyError, ImportError, ValueError) as e:
+    _tokenizer = tiktoken.get_encoding(FALLBACK_ENCODING)
+
+
+def _preprocess_text(text: str) -> str:
+    """预处理文本：统一大小写、清理空白符、移除特殊字符"""
+    if not text:
+        return ""
+    text = text.lower()
+    text = re.sub(r'\s+', ' ', text)  # 合并多个空白符
+    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)  # 保留字母数字中文，其他替换为空格
+    return text.strip()
+
+
+def _tokenize_for_bm25(text: str) -> list[str]:
+    """使用 tiktoken 对文本进行分词，为 BM25 算法准备 token 列表"""
+    if not text:
+        return []
+
+    preprocessed_text = _preprocess_text(text)
+    if not preprocessed_text:
+        return []
+
+    try:
+        # 使用 tiktoken 编码后解码，获得高质量的 token
+        tokens = _tokenizer.encode(preprocessed_text)
+        token_strings = []
+        for token in tokens:
+            try:
+                token_str = _tokenizer.decode([token])
+                if token_str.strip():
+                    token_strings.append(token_str.strip())
+            except (UnicodeDecodeError, ValueError):
+                continue
+
+        # 如果 tiktoken 分词失败，回退到简单空格分词
+        if not token_strings:
+            token_strings = preprocessed_text.split()
+
+        return token_strings
+
+    except (ValueError, TypeError, AttributeError):
+        # 异常情况下使用简单分词
+        return preprocessed_text.split()
+
 
 def update_kwargs(**kwargs):
     if 'timeout' not in kwargs:
@@ -371,14 +428,175 @@ def yandex(query, _lang):
     'swisscows': swisscows,
     'wikipedia': wikipedia,
     'yandex': yandex,
+    'custom': 'custom',
 }
 
 
+def deduplicate_results(results):
+    """去除重复的自动补全结果，保持原有顺序"""
+    seen = set()
+    unique_results = []
+    for result in results:
+        if result not in seen:
+            unique_results.append(result)
+            seen.add(result)
+    return unique_results
+
+
+def _get_strategy_params(query_length):
+    """根据查询长度选择自动补全策略参数"""
+    if query_length <= 2:
+        # 极短查询：强调前缀匹配，降低 BM25 权重
+        return {
+            'prefix_bonus': 2.0,
+            'length_penalty_rate': 0.1,
+            'exact_match_bonus': 3.0,
+            'bm25_weight': 0.3,
+        }
+    # 修复：移除不必要的 elif，改为 if
+    if query_length <= 5:
+        # 中等查询：平衡前缀匹配和语义相关性
+        return {
+            'prefix_bonus': 1.5,
+            'length_penalty_rate': 0.05,
+            'exact_match_bonus': 2.0,
+            'bm25_weight': 0.6,
+        }
+
+    # 长查询：依赖 BM25 语义理解能力
+    return {
+        'prefix_bonus': 1.2,
+        'length_penalty_rate': 0.02,
+        'exact_match_bonus': 1.5,
+        'bm25_weight': 0.8,
+    }
+
+
+def _normalize_bm25_scores(raw_scores):
+    """标准化 BM25 分数到 [0,1] 区间"""
+    if len(raw_scores) == 0:
+        return []
+
+    min_score, max_score = float(np.min(raw_scores)), float(np.max(raw_scores))
+    if max_score > min_score:
+        return ((raw_scores - min_score) / (max_score - min_score)).tolist()
+
+    return [0.5] * len(raw_scores)
+
+
+def _calculate_suggestion_score(suggestion, query_lower, bm25_score, params):
+    """计算单个建议的综合分数"""
+    suggestion_lower = suggestion.lower()
+
+    # 前缀匹配加分：自动补全的核心特性
+    prefix_boost = params['prefix_bonus'] if suggestion_lower.startswith(query_lower) else 1.0
+
+    # 完全匹配加分：用户可能已经知道想要的结果
+    exact_match_boost = params['exact_match_bonus'] if suggestion_lower == query_lower else 1.0
+
+    # 长度惩罚：自动补全偏好简洁的建议
+    length_penalty = 1.0
+    if len(suggestion) > len(query_lower) * 3:
+        excess_length = len(suggestion) - len(query_lower) * 2
+        length_penalty = 1.0 - (excess_length * params['length_penalty_rate'])
+        length_penalty = max(0.1, length_penalty)  # 避免过度惩罚
+
+    # 综合计算最终分数
+    final_score = (
+        (bm25_score * params['bm25_weight'] + (1.0 - params['bm25_weight']))
+        * prefix_boost
+        * exact_match_boost
+        * length_penalty
+    )
+
+    # 添加微小随机因子避免相同分数的不稳定排序
+    final_score += hash(suggestion) % 1000 * 0.000001
+
+    return final_score
+
+
+def rerank_results(results_list, query):
+    """使用 BM25 算法和 tiktoken 分词对自动补全结果进行重排
+
+    结合 BM25 语义相关性和自动补全特有的前缀匹配、长度偏好等特性，
+    根据查询长度动态调整各评分因子的权重。
+    """
+    # 合并并去重所有后端返回的结果
+    corpus = deduplicate_results([result for results in results_list for result in results])
+
+    if len(corpus) < 2:
+        return corpus
+
+    try:
+        # 使用 tiktoken 进行高质量分词，支持中英文混合
+        corpus_tokens = [_tokenize_for_bm25(doc) for doc in corpus]
+        query_tokens = _tokenize_for_bm25(query)
+
+        if not query_tokens:
+            return corpus
+
+        # 构建 BM25 索引并检索
+        retriever = bm25s.BM25()
+        retriever.index(corpus_tokens)
+        documents, scores = retriever.retrieve([query_tokens], k=len(corpus), return_as='tuple', show_progress=False)
+
+        # 标准化 BM25 分数到 [0,1] 区间
+        raw_scores = scores[0]
+        normalized_scores = _normalize_bm25_scores(raw_scores)
+
+        if not normalized_scores:
+            return corpus
+
+        # 获取策略参数
+        query_length = len(query.strip())
+        params = _get_strategy_params(query_length)
+
+        # 计算每个建议的综合分数
+        final_scores = []
+        query_lower = query.lower()
+
+        for idx, doc_index in enumerate(documents[0]):
+            if doc_index >= len(corpus):
+                continue
+
+            suggestion = corpus[doc_index]
+            bm25_score = float(normalized_scores[idx])
+
+            final_score = _calculate_suggestion_score(suggestion, query_lower, bm25_score, params)
+            final_scores.append((doc_index, final_score))
+
+        # 按最终分数降序排列
+        final_scores.sort(key=lambda x: x[1], reverse=True)
+
+        # 返回重排后的建议列表
+        return [corpus[doc_index] for doc_index, _ in final_scores]
+
+    except (ValueError, TypeError, AttributeError, ImportError):
+        # 异常情况下返回原始结果
+        return corpus
+
+
 def search_autocomplete(backend_name, query, sxng_locale):
+    if backend_name == 'custom':
+        custom_backends = settings.get('search', {}).get('autocomplete_engines', [])
+        custom_backends = [backend.strip() for backend in custom_backends if backend.strip() in backends]
+
+        results_list = []
+        for backend_key in custom_backends:
+            backend = backends.get(backend_key)
+            if backend is not None:
+                try:
+                    results_list.append(backend(query, sxng_locale))
+                except (HTTPError, SearxEngineResponseException, ValueError):
+                    results_list.append([])
+        return rerank_results(results_list, query)
+
     backend = backends.get(backend_name)
     if backend is None:
         return []
+
+    # 修复：移除不必要的 else，直接执行代码
     try:
         return backend(query, sxng_locale)
-    except (HTTPError, SearxEngineResponseException):
+    except (HTTPError, SearxEngineResponseException, ValueError):
         return []
diff --git a/searx/plugins/bm25_rerank.py b/searx/plugins/bm25_rerank.py
new file mode 100644
index 00000000000..47d0e37ec71
--- /dev/null
+++ b/searx/plugins/bm25_rerank.py
@@ -0,0 +1,234 @@
+# SPDX-License-Identifier: AGPL-3.0-or-later
+# pylint: disable=missing-module-docstring, missing-class-docstring, protected-access
+from __future__ import annotations
+import re
+import typing
+
+import numpy as np
+import bm25s
+import tiktoken
+
+from searx.plugins import Plugin, PluginInfo
+from searx.result_types import EngineResults
+
+if typing.TYPE_CHECKING:
+    from searx.search import SearchWithPlugins
+    from searx.extended_types import SXNG_Request
+    from searx.plugins import PluginCfg
+
+# 常量定义
+PRIMARY_MODEL = "gpt-oss-120b"
+FALLBACK_ENCODING = "o200k_harmony"
+TITLE_PREVIEW_LENGTH = 100
+CONTENT_PREVIEW_LENGTH = 150
+DISPLAY_TOP_K = 5
+MIN_POSITION_VALUE = 0.01
+MAX_MULTIPLIER = 1.0
+MIN_MULTIPLIER = 0.1
+MULTIPLIER_SCALE = 0.8
+
+
+class SXNGPlugin(Plugin):
+    """Rerank search results using BM25 algorithm with tiktoken tokenizer."""
+
+    id = "bm25_rerank"
+    default_on = True
+
+    def __init__(self, plg_cfg: "PluginCfg") -> None:
+        super().__init__(plg_cfg)
+        self.info = PluginInfo(
+            id=self.id,
+            name="BM25 Rerank",
+            description="Rerank search results using BM25 algorithm with tiktoken tokenizer",
+            preference_section="general",
+        )
+        self._init_tokenizer()
+
+    def _init_tokenizer(self) -> None:
+        """初始化tiktoken编码器"""
+        try:
+            self.tokenizer = tiktoken.encoding_for_model(PRIMARY_MODEL)
+        except (KeyError, ValueError, ImportError):
+            # 捕获更具体的异常：模型不存在、参数错误、导入错误
+            self.tokenizer = tiktoken.get_encoding(FALLBACK_ENCODING)
+
+    def _preprocess_text(self, text: str) -> str:
+        """文本预处理"""
+        if not text:
+            return ""
+        text = text.lower()
+        text = re.sub(r'\s+', ' ', text)
+        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
+        return text.strip()
+
+    def _tokenize_text(self, text: str) -> list[str]:
+        """使用tiktoken进行分词"""
+        if not text:
+            return []
+
+        preprocessed_text = self._preprocess_text(text)
+        if not preprocessed_text:
+            return []
+
+        try:
+            tokens = self.tokenizer.encode(preprocessed_text)
+            token_strings = []
+            for token in tokens:
+                try:
+                    token_str = self.tokenizer.decode([token])
+                    if token_str.strip():
+                        token_strings.append(token_str.strip())
+                except (UnicodeDecodeError, ValueError):
+                    # 捕获解码相关的特定异常
+                    continue
+            return token_strings
+        except (AttributeError, ValueError, TypeError):
+            # 捕获编码相关的特定异常
+            return preprocessed_text.split()
+
+    def _extract_result_text(self, result) -> str:
+        """提取搜索结果的文本内容"""
+        text_parts = []
+
+        if hasattr(result, 'title') and result.title:
+            text_parts.append(str(result.title))
+
+        if hasattr(result, 'content') and result.content:
+            text_parts.append(str(result.content))
+
+        if hasattr(result, 'url') and result.url:
+            url_words = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', str(result.url))
+            if url_words:
+                text_parts.append(' '.join(url_words))
+
+        return " ".join(text_parts)
+
+    def _build_corpus(self, results: list) -> list[str]:
+        """构建语料库"""
+        return [self._extract_result_text(result) for result in results]
+
+    def _normalize_scores(self, scores: np.ndarray) -> list[float]:
+        """标准化分数到0-1范围"""
+        if len(scores) == 0:
+            return []
+
+        scores = scores.astype(float)
+        min_score, max_score = float(np.min(scores)), float(np.max(scores))
+
+        if max_score > min_score:
+            normalized = (scores - min_score) / (max_score - min_score)
+            return normalized.tolist()
+        return [0.5] * len(scores)
+
+    def _calculate_multiplier(self, bm25_score: float) -> float:
+        """计算位置权重倍数"""
+        multiplier = MAX_MULTIPLIER - bm25_score * MULTIPLIER_SCALE
+        return max(MIN_MULTIPLIER, min(MAX_MULTIPLIER, multiplier))
+
+    def _perform_bm25_search(self, corpus_tokens: list, query_tokens: list) -> tuple:
+        """执行BM25搜索"""
+        retriever = bm25s.BM25()
+        retriever.index(corpus_tokens)
+
+        # 尝试多种检索方式
+        for method_func in [
+            lambda: self._try_get_scores(retriever, query_tokens),
+            lambda: self._try_retrieve_batch(retriever, query_tokens),
+            lambda: self._try_retrieve_fallback(retriever, query_tokens),
+        ]:
+            try:
+                documents, scores_array = method_func()
+                return documents, scores_array
+            except (AttributeError, ValueError, RuntimeError, IndexError):
+                # 捕获BM25检索可能出现的具体异常
+                continue
+
+        raise RuntimeError("所有BM25检索方法都失败")
+
+    def _try_get_scores(self, retriever, query_tokens: list) -> tuple:
+        """尝试使用get_scores方法"""
+        scores = retriever.get_scores(query_tokens)
+        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
+        return [sorted_indices], [scores[sorted_indices]]
+
+    def _try_retrieve_batch(self, retriever, query_tokens: list) -> tuple:
+        """尝试使用retrieve方法（批处理）"""
+        query_batch = [query_tokens]
+        return retriever.retrieve(query_batch, k=len(query_tokens), return_as="tuple", show_progress=False)
+
+    def _try_retrieve_fallback(self, retriever, query_tokens: list) -> tuple:
+        """回退检索方法"""
+        scores = retriever.get_scores(query_tokens)
+        sorted_pairs = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)
+        return [[pair[0] for pair in sorted_pairs]], [[pair[1] for pair in sorted_pairs]]
+
+    def _update_result_positions(self, result, doc_index: int, multiplier: float, total_results: int) -> None:
+        """更新单个结果的位置权重"""
+        if hasattr(result, 'positions') and result.positions:
+            for i, position in enumerate(result.positions):
+                if isinstance(position, (int, float)):
+                    result.positions[i] = float(max(MIN_POSITION_VALUE, position * multiplier))
+        else:
+            initial_position = float(doc_index + 1.0) / total_results
+            result.positions = [float(initial_position * multiplier)]
+
+    def _apply_rerank(self, results: list, documents: list, normalized_scores: list[float]) -> None:
+        """应用重排序到搜索结果"""
+        if not documents or not documents[0] or not normalized_scores:
+            return
+
+        for idx, doc_index in enumerate(documents[0]):
+            if doc_index >= len(results) or idx >= len(normalized_scores):
+                continue
+
+            result = results[doc_index]
+            bm25_score = normalized_scores[idx]
+            multiplier = self._calculate_multiplier(bm25_score)
+
+            self._update_result_positions(result, doc_index, multiplier, len(results))
+
+    def _process_results(self, results: list, query: str) -> None:
+        """处理搜索结果并重新排序"""
+        if len(results) < 2:
+            return
+
+        try:
+            # 构建语料库和分词
+            corpus = self._build_corpus(results)
+            if not corpus:
+                return
+
+            corpus_tokens = [self._tokenize_text(doc) for doc in corpus]
+            query_tokens = self._tokenize_text(query)
+
+            if not query_tokens:
+                return
+
+            # 执行BM25搜索
+            documents, scores_array = self._perform_bm25_search(corpus_tokens, query_tokens)
+
+            # 标准化分数并重排
+            if documents and scores_array:
+                normalized_scores = self._normalize_scores(np.array(scores_array[0]))
+                if normalized_scores:
+                    self._apply_rerank(results, documents, normalized_scores)
+
+        except (RuntimeError, ValueError, TypeError, AttributeError):
+            # 捕获处理过程中可能出现的具体异常
+            pass
+
+    def post_search(self, request: "SXNG_Request", search: "SearchWithPlugins") -> EngineResults:
+        """搜索后处理钩子"""
+        try:
+            results = search.result_container.get_ordered_results()
+
+            if len(results) >= 2:
+                query = search.search_query.query
+                if query:
+                    self._process_results(results, query)
+
+        except (AttributeError, TypeError):
+            # 捕获访问搜索对象属性时可能出现的具体异常
+            pass
+
+        return search.result_container
diff --git a/searx/settings_defaults.py b/searx/settings_defaults.py
index 9f85397556a..a6b23da9124 100644
--- a/searx/settings_defaults.py
+++ b/searx/settings_defaults.py
@@ -156,6 +156,7 @@ def apply_schema(settings, schema, path_list):
     'search': {
         'safe_search': SettingsValue((0, 1, 2), 0),
         'autocomplete': SettingsValue(str, ''),
+        'autocomplete_engines': SettingsValue((list, str, False), ['']),
         'autocomplete_min': SettingsValue(int, 4),
         'favicon_resolver': SettingsValue(str, ''),
         'default_lang': SettingsValue(tuple(SXNG_LOCALE_TAGS + ['']), ''),
