diff --git a/searx/autocomplete.py b/searx/autocomplete.py
index fabd8be2419..37b3f0c474a 100644
--- a/searx/autocomplete.py
+++ b/searx/autocomplete.py
@@ -1,12 +1,17 @@
 # SPDX-License-Identifier: AGPL-3.0-or-later
 """This module implements functions needed for the autocompleter."""
-# pylint: disable=use-dict-literal
+# pylint: disable=use-dict-literal,too-many-locals
 
 import json
 import html
+import re
 import typing as t
 from urllib.parse import urlencode, quote_plus
 
+import numpy as np
+import bm25s
+import tiktoken
+
 import lxml.etree
 import lxml.html
 from httpx import HTTPError
@@ -23,6 +28,58 @@
 if t.TYPE_CHECKING:
     from searx.extended_types import SXNG_Response
 
+# Tiktoken 配置常量
+PRIMARY_MODEL = "gpt-oss-120b"
+FALLBACK_ENCODING = "o200k_harmony"
+
+# 初始化 tiktoken 编码器
+try:
+    _tokenizer = tiktoken.encoding_for_model(PRIMARY_MODEL)
+except (KeyError, ImportError, ValueError) as e:
+    _tokenizer = tiktoken.get_encoding(FALLBACK_ENCODING)
+
+
+def _preprocess_text(text: str) -> str:
+    """预处理文本：统一大小写、清理空白符、移除特殊字符"""
+    if not text:
+        return ""
+    text = text.lower()
+    text = re.sub(r'\s+', ' ', text)  # 合并多个空白符
+    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)  # 保留字母数字中文，其他替换为空格
+    return text.strip()
+
+
+def _tokenize_for_bm25(text: str) -> list[str]:
+    """使用 tiktoken 对文本进行分词，为 BM25 算法准备 token 列表"""
+    if not text:
+        return []
+
+    preprocessed_text = _preprocess_text(text)
+    if not preprocessed_text:
+        return []
+
+    try:
+        # 使用 tiktoken 编码后解码，获得高质量的 token
+        tokens = _tokenizer.encode(preprocessed_text)
+        token_strings = []
+        for token in tokens:
+            try:
+                token_str = _tokenizer.decode([token])
+                if token_str.strip():
+                    token_strings.append(token_str.strip())
+            except (UnicodeDecodeError, ValueError):
+                continue
+
+        # 如果 tiktoken 分词失败，回退到简单空格分词
+        if not token_strings:
+            token_strings = preprocessed_text.split()
+
+        return token_strings
+
+    except (ValueError, TypeError, AttributeError):
+        # 异常情况下使用简单分词
+        return preprocessed_text.split()
+
 
 def update_kwargs(**kwargs) -> None:  # type: ignore
     if 'timeout' not in kwargs:
@@ -359,14 +416,175 @@ def yandex(query: str, _sxng_locale: str) -> list[str]:
     'swisscows': swisscows,
     'wikipedia': wikipedia,
     'yandex': yandex,
+    'custom': 'custom',
 }
 
 
+def deduplicate_results(results):
+    """去除重复的自动补全结果，保持原有顺序"""
+    seen = set()
+    unique_results = []
+    for result in results:
+        if result not in seen:
+            unique_results.append(result)
+            seen.add(result)
+    return unique_results
+
+
+def _get_strategy_params(query_length):
+    """根据查询长度选择自动补全策略参数"""
+    if query_length <= 2:
+        # 极短查询：强调前缀匹配，降低 BM25 权重
+        return {
+            'prefix_bonus': 2.0,
+            'length_penalty_rate': 0.1,
+            'exact_match_bonus': 3.0,
+            'bm25_weight': 0.3,
+        }
+    # 修复：移除不必要的 elif，改为 if
+    if query_length <= 5:
+        # 中等查询：平衡前缀匹配和语义相关性
+        return {
+            'prefix_bonus': 1.5,
+            'length_penalty_rate': 0.05,
+            'exact_match_bonus': 2.0,
+            'bm25_weight': 0.6,
+        }
+
+    # 长查询：依赖 BM25 语义理解能力
+    return {
+        'prefix_bonus': 1.2,
+        'length_penalty_rate': 0.02,
+        'exact_match_bonus': 1.5,
+        'bm25_weight': 0.8,
+    }
+
+
+def _normalize_bm25_scores(raw_scores):
+    """标准化 BM25 分数到 [0,1] 区间"""
+    if len(raw_scores) == 0:
+        return []
+
+    min_score, max_score = float(np.min(raw_scores)), float(np.max(raw_scores))
+    if max_score > min_score:
+        return ((raw_scores - min_score) / (max_score - min_score)).tolist()
+
+    return [0.5] * len(raw_scores)
+
+
+def _calculate_suggestion_score(suggestion, query_lower, bm25_score, params):
+    """计算单个建议的综合分数"""
+    suggestion_lower = suggestion.lower()
+
+    # 前缀匹配加分：自动补全的核心特性
+    prefix_boost = params['prefix_bonus'] if suggestion_lower.startswith(query_lower) else 1.0
+
+    # 完全匹配加分：用户可能已经知道想要的结果
+    exact_match_boost = params['exact_match_bonus'] if suggestion_lower == query_lower else 1.0
+
+    # 长度惩罚：自动补全偏好简洁的建议
+    length_penalty = 1.0
+    if len(suggestion) > len(query_lower) * 3:
+        excess_length = len(suggestion) - len(query_lower) * 2
+        length_penalty = 1.0 - (excess_length * params['length_penalty_rate'])
+        length_penalty = max(0.1, length_penalty)  # 避免过度惩罚
+
+    # 综合计算最终分数
+    final_score = (
+        (bm25_score * params['bm25_weight'] + (1.0 - params['bm25_weight']))
+        * prefix_boost
+        * exact_match_boost
+        * length_penalty
+    )
+
+    # 添加微小随机因子避免相同分数的不稳定排序
+    final_score += hash(suggestion) % 1000 * 0.000001
+
+    return final_score
+
+
+def rerank_results(results_list, query):
+    """使用 BM25 算法和 tiktoken 分词对自动补全结果进行重排
+
+    结合 BM25 语义相关性和自动补全特有的前缀匹配、长度偏好等特性，
+    根据查询长度动态调整各评分因子的权重。
+    """
+    # 合并并去重所有后端返回的结果
+    corpus = deduplicate_results([result for results in results_list for result in results])
+
+    if len(corpus) < 2:
+        return corpus
+
+    try:
+        # 使用 tiktoken 进行高质量分词，支持中英文混合
+        corpus_tokens = [_tokenize_for_bm25(doc) for doc in corpus]
+        query_tokens = _tokenize_for_bm25(query)
+
+        if not query_tokens:
+            return corpus
+
+        # 构建 BM25 索引并检索
+        retriever = bm25s.BM25()
+        retriever.index(corpus_tokens)
+        documents, scores = retriever.retrieve([query_tokens], k=len(corpus), return_as='tuple', show_progress=False)
+
+        # 标准化 BM25 分数到 [0,1] 区间
+        raw_scores = scores[0]
+        normalized_scores = _normalize_bm25_scores(raw_scores)
+
+        if not normalized_scores:
+            return corpus
+
+        # 获取策略参数
+        query_length = len(query.strip())
+        params = _get_strategy_params(query_length)
+
+        # 计算每个建议的综合分数
+        final_scores = []
+        query_lower = query.lower()
+
+        for idx, doc_index in enumerate(documents[0]):
+            if doc_index >= len(corpus):
+                continue
+
+            suggestion = corpus[doc_index]
+            bm25_score = float(normalized_scores[idx])
+
+            final_score = _calculate_suggestion_score(suggestion, query_lower, bm25_score, params)
+            final_scores.append((doc_index, final_score))
+
+        # 按最终分数降序排列
+        final_scores.sort(key=lambda x: x[1], reverse=True)
+
+        # 返回重排后的建议列表
+        return [corpus[doc_index] for doc_index, _ in final_scores]
+
+    except (ValueError, TypeError, AttributeError, ImportError):
+        # 异常情况下返回原始结果
+        return corpus
+
+
 def search_autocomplete(backend_name: str, query: str, sxng_locale: str) -> list[str]:
+    if backend_name == 'custom':
+        custom_backends = settings.get('search', {}).get('autocomplete_engines', [])
+        custom_backends = [backend.strip() for backend in custom_backends if backend.strip() in backends]
+
+        results_list = []
+        for backend_key in custom_backends:
+            backend = backends.get(backend_key)
+            if backend is not None:
+                try:
+                    results_list.append(backend(query, sxng_locale))
+                except (HTTPError, SearxEngineResponseException, ValueError):
+                    results_list.append([])
+        return rerank_results(results_list, query)
+
     backend = backends.get(backend_name)
     if backend is None:
         return []
+
+    # 修复：移除不必要的 else，直接执行代码
     try:
         return backend(query, sxng_locale)
-    except (HTTPError, SearxEngineResponseException):
+    except (HTTPError, SearxEngineResponseException, ValueError):
         return []
diff --git a/searx/plugins/bm25_rerank.py b/searx/plugins/bm25_rerank.py
new file mode 100644
index 00000000000..ac58c531c24
--- /dev/null
+++ b/searx/plugins/bm25_rerank.py
@@ -0,0 +1,164 @@
+# SPDX-License-Identifier: AGPL-3.0-or-later
+# pylint: disable=missing-module-docstring, missing-class-docstring, protected-access
+from __future__ import annotations
+import re
+import typing
+
+import bm25s
+import tiktoken
+
+from searx.plugins import Plugin, PluginInfo
+from searx.result_types import EngineResults
+
+if typing.TYPE_CHECKING:
+    from searx.search import SearchWithPlugins
+    from searx.extended_types import SXNG_Request
+    from searx.plugins import PluginCfg
+
+# 常量定义
+PRIMARY_MODEL = "gpt-oss-120b"
+FALLBACK_ENCODING = "o200k_harmony"
+
+
+class SXNGPlugin(Plugin):
+    """Rerank search results using BM25 algorithm with tiktoken tokenizer."""
+
+    id = "bm25_rerank"
+    default_on = True
+
+    def __init__(self, plg_cfg: "PluginCfg") -> None:
+        super().__init__(plg_cfg)
+        self.info = PluginInfo(
+            id=self.id,
+            name="BM25 Rerank",
+            description="Rerank search results using BM25 algorithm with tiktoken tokenizer",
+            preference_section="general",
+        )
+        self._init_tokenizer()
+
+    def _init_tokenizer(self) -> None:
+        """初始化tiktoken编码器"""
+        try:
+            self.tokenizer = tiktoken.encoding_for_model(PRIMARY_MODEL)
+        except (KeyError, ValueError):
+            self.tokenizer = tiktoken.get_encoding(FALLBACK_ENCODING)
+
+    def _preprocess_text(self, text: str) -> str:
+        """文本预处理"""
+        if not text:
+            return ""
+        text = text.lower()
+        text = re.sub(r'\s+', ' ', text)
+        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
+        return text.strip()
+
+    def _tokenize_text(self, text: str) -> list[str]:
+        """使用tiktoken进行分词"""
+        if not text:
+            return []
+
+        preprocessed_text = self._preprocess_text(text)
+        if not preprocessed_text:
+            return []
+
+        try:
+            tokens = self.tokenizer.encode(preprocessed_text)
+            token_strings = []
+            for token in tokens:
+                try:
+                    token_str = self.tokenizer.decode([token]).strip()
+                    if token_str:
+                        token_strings.append(token_str)
+                except (ValueError, UnicodeDecodeError):
+                    continue
+            return token_strings
+        except (ValueError, TypeError):
+            return preprocessed_text.split()
+
+    def _extract_result_text(self, result) -> str:
+        """提取搜索结果的文本内容"""
+        text_parts = []
+
+        for attr in ['title', 'content']:
+            if hasattr(result, attr) and getattr(result, attr):
+                text_parts.append(str(getattr(result, attr)))
+
+        if hasattr(result, 'url') and result.url:
+            url_words = re.findall(r'[a-zA-Z\u4e00-\u9fff]+', str(result.url))
+            if url_words:
+                text_parts.append(' '.join(url_words))
+
+        return " ".join(text_parts)
+
+    def _get_bm25_scores(self, results: list, query: str) -> list[float]:
+        """获取所有文档的BM25分数"""
+        if not results or not query:
+            return [0.0] * len(results)
+
+        try:
+            # 构建语料库和分词
+            corpus_tokens = []
+            for result in results:
+                text = self._extract_result_text(result)
+                tokens = self._tokenize_text(text)
+                corpus_tokens.append(tokens)
+
+            query_tokens = self._tokenize_text(query)
+            if not query_tokens or not any(corpus_tokens):
+                return [0.0] * len(results)
+
+            # 计算BM25分数
+            retriever = bm25s.BM25()
+            retriever.index(corpus_tokens)
+            scores = retriever.get_scores(query_tokens)
+
+            # 转换为列表格式
+            if hasattr(scores, 'tolist'):
+                return scores.tolist()
+
+            return [float(score) for score in scores]
+
+        except (ValueError, TypeError, AttributeError, ImportError):
+            return [0.0] * len(results)
+
+    def _update_positions(self, results: list, bm25_scores: list[float]) -> None:
+        """将BM25分数更新到结果的positions中"""
+        # 确保分数数量与结果数量一致
+        while len(bm25_scores) < len(results):
+            bm25_scores.append(0.0)
+
+        for i, result in enumerate(results):
+            # BM25分数
+            position_score = bm25_scores[i]
+
+            try:
+                # 获取原有positions
+                original_positions = getattr(result, 'positions', [])
+
+                # 将BM25分数插入第一位
+                result.positions = [position_score] + list(original_positions)
+            except AttributeError:
+                try:
+                    result.positions = [position_score]
+                except AttributeError:
+                    pass
+
+    def post_search(self, request: "SXNG_Request", search: "SearchWithPlugins") -> EngineResults:
+        """搜索后处理钩子"""
+        try:
+            # 获取搜索结果
+            results = getattr(search.result_container, 'results', None)
+            if not results and hasattr(search.result_container, 'get_ordered_results'):
+                results = search.result_container.get_ordered_results()
+
+            if results and len(results) > 0:
+                query = search.search_query.query
+                if query:
+                    # 计算BM25分数并更新positions
+                    bm25_scores = self._get_bm25_scores(results, query)
+                    self._update_positions(results, bm25_scores)
+
+        except AttributeError:
+            pass
+
+        return search.result_container
diff --git a/searx/settings_defaults.py b/searx/settings_defaults.py
index c71103a4c68..8c52f6711e9 100644
--- a/searx/settings_defaults.py
+++ b/searx/settings_defaults.py
@@ -190,6 +190,7 @@ def apply_schema(settings: dict[str, t.Any], schema: dict[str, t.Any], path_list
     'search': {
         'safe_search': SettingsValue((0, 1, 2), 0),
         'autocomplete': SettingsValue(str, ''),
+        'autocomplete_engines': SettingsValue((list, str, False), ['']),
         'autocomplete_min': SettingsValue(int, 4),
         'favicon_resolver': SettingsValue(str, ''),
         'default_lang': SettingsValue(tuple(SXNG_LOCALE_TAGS + ['']), ''),
