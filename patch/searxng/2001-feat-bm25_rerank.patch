diff --git a/searx/autocomplete.py b/searx/autocomplete.py
index fabd8be2419..53c18b5afef 100644
--- a/searx/autocomplete.py
+++ b/searx/autocomplete.py
@@ -1,12 +1,17 @@
 # SPDX-License-Identifier: AGPL-3.0-or-later
 """This module implements functions needed for the autocompleter."""
-# pylint: disable=use-dict-literal
+# pylint: disable=use-dict-literal,too-many-locals
 
 import json
 import html
+import re
 import typing as t
 from urllib.parse import urlencode, quote_plus
 
+import numpy as np
+import bm25s
+import bm25s.stopwords as stopwords_module
+
 import lxml.etree
 import lxml.html
 from httpx import HTTPError
@@ -359,14 +364,144 @@ def yandex(query: str, _sxng_locale: str) -> list[str]:
     'swisscows': swisscows,
     'wikipedia': wikipedia,
     'yandex': yandex,
+    'custom': 'custom',
 }
 
 
+def deduplicate_results(results):
+    """去除重复的自动补全结果"""
+    seen = set()
+    unique_results = []
+    for result in results:
+        if result not in seen:
+            unique_results.append(result)
+            seen.add(result)
+    return unique_results
+
+
+def rerank_results(results_list, query):
+    """使用BM25算法对自动补全结果进行重排"""
+    # 合并并去重结果
+    corpus = deduplicate_results([result for results in results_list for result in results])
+
+    if len(corpus) < 2:
+        return corpus
+
+    # 获取停用词
+    stopwords = {
+        word
+        for name, value in stopwords_module.__dict__.items()
+        if name.startswith("STOPWORDS_") and isinstance(value, tuple)
+        for word in value
+    }
+
+    # 分词
+    corpus_tokens = bm25s.tokenize(corpus, stopwords=stopwords)
+    query_tokens = bm25s.tokenize(query, stopwords=stopwords)
+
+    # BM25检索
+    retriever = bm25s.BM25()
+    retriever.index(corpus_tokens)
+
+    documents, scores = retriever.retrieve(query_tokens, k=len(corpus), return_as='tuple', show_progress=False)
+
+    # 标准化分数并转换为Python原生类型
+    raw_scores = scores[0]
+    if len(raw_scores) == 0:
+        return corpus
+
+    min_score, max_score = float(np.min(raw_scores)), float(np.max(raw_scores))
+    if max_score > min_score:
+        normalized_scores = ((raw_scores - min_score) / (max_score - min_score)).tolist()
+    else:
+        normalized_scores = [0.5] * len(raw_scores)
+
+    # 选择自动补全策略参数（根据查询长度）
+    query_length = len(query.strip())
+
+    if query_length <= 2:
+        # 极短查询：优先考虑前缀匹配
+        prefix_bonus = 2.0
+        length_penalty_rate = 0.1
+        exact_match_bonus = 3.0
+        bm25_weight = 0.3
+    elif query_length <= 5:
+        # 短查询：平衡前缀和相关性
+        prefix_bonus = 1.5
+        length_penalty_rate = 0.05
+        exact_match_bonus = 2.0
+        bm25_weight = 0.6
+    else:
+        # 长查询：主要依靠BM25相关性
+        prefix_bonus = 1.2
+        length_penalty_rate = 0.02
+        exact_match_bonus = 1.5
+        bm25_weight = 0.8
+
+    # 计算最终分数并重排
+    final_scores = []
+    query_lower = query.lower()
+
+    for idx, doc_index in enumerate(documents[0]):
+        if doc_index >= len(corpus):
+            continue
+
+        suggestion = corpus[doc_index]
+        suggestion_lower = suggestion.lower()
+        bm25_score = float(normalized_scores[idx])
+
+        # 计算各种加成
+        # 前缀匹配加成
+        prefix_boost = prefix_bonus if suggestion_lower.startswith(query_lower) else 1.0
+
+        # 完全匹配巨大加成
+        exact_match_boost = exact_match_bonus if suggestion_lower == query_lower else 1.0
+
+        # 长度惩罚（自动补全倾向于简短的结果）
+        length_penalty = 1.0
+        if len(suggestion) > len(query) * 3:  # 如果建议比查询长太多
+            excess_length = len(suggestion) - len(query) * 2
+            length_penalty = 1.0 - (excess_length * length_penalty_rate)
+            length_penalty = max(0.1, length_penalty)  # 不要过度惩罚
+
+        # 计算最终分数
+        final_score = (
+            (bm25_score * bm25_weight + (1.0 - bm25_weight)) * prefix_boost * exact_match_boost * length_penalty
+        )
+
+        # 添加微小随机因子避免相同分数结果的不稳定排序
+        final_score += hash(suggestion) % 1000 * 0.000001
+
+        final_scores.append((doc_index, float(final_score)))
+
+    # 按最终分数排序
+    final_scores.sort(key=lambda x: x[1], reverse=True)
+
+    # 返回重排后的结果
+    return [corpus[doc_index] for doc_index, _ in final_scores]
+
+
 def search_autocomplete(backend_name: str, query: str, sxng_locale: str) -> list[str]:
+    if backend_name == 'custom':
+        custom_backends = settings.get('search', {}).get('autocomplete_engines', [])
+        custom_backends = [backend.strip() for backend in custom_backends if backend.strip() in backends]
+
+        results_list = []
+        for backend_key in custom_backends:
+            backend = backends.get(backend_key)
+            if backend is not None:
+                try:
+                    results_list.append(backend(query, sxng_locale))
+                except (HTTPError, SearxEngineResponseException, ValueError):
+                    results_list.append([])
+        return rerank_results(results_list, query)
+
     backend = backends.get(backend_name)
     if backend is None:
         return []
+
+    # 修复：移除不必要的 else，直接执行代码
     try:
         return backend(query, sxng_locale)
-    except (HTTPError, SearxEngineResponseException):
+    except (HTTPError, SearxEngineResponseException, ValueError):
         return []
diff --git a/searx/plugins/bm25_rerank.py b/searx/plugins/bm25_rerank.py
new file mode 100644
index 00000000000..03c2705ed4f
--- /dev/null
+++ b/searx/plugins/bm25_rerank.py
@@ -0,0 +1,271 @@
+# SPDX-License-Identifier: AGPL-3.0-or-later
+# pylint: disable=missing-module-docstring, missing-class-docstring, protected-access
+from __future__ import annotations
+import math
+import re
+import typing
+
+import numpy as np
+import bm25s
+import bm25s.stopwords as stopwords_module
+
+from searx.plugins import Plugin, PluginInfo
+from searx.result_types import EngineResults
+
+if typing.TYPE_CHECKING:
+    from searx.search import SearchWithPlugins
+    from searx.extended_types import SXNG_Request
+    from searx.plugins import PluginCfg
+
+
+class SXNGPlugin(Plugin):
+    """Rerank search results using the Okapi BM25 algorithm with adaptive strategies.
+    Optimized for large-scale result sets from multiple search engines.
+    """
+
+    id = "bm25_rerank"
+    default_on = True
+
+    def __init__(self, plg_cfg: "PluginCfg") -> None:
+        super().__init__(plg_cfg)
+
+        self.info = PluginInfo(
+            id=self.id,
+            name="Adaptive BM25 Rerank",
+            description="Intelligently rerank search results using adaptive BM25 algorithm",
+            preference_section="general",
+        )
+
+    def _select_adaptive_strategy(self, results_count: int) -> tuple[str, dict]:
+        """基于结果数量自适应选择重排策略"""
+
+        # 定义策略配置映射
+        strategy_configs = [
+            (
+                1000000,
+                "ultra_aggressive",
+                {
+                    'scaling_strength': 2.0,
+                    'boost_factor': 5.0,
+                    'top_tier_threshold': 0.95,
+                    'mid_tier_threshold': 0.8,
+                    'low_tier_threshold': 0.5,
+                },
+            ),
+            (
+                500000,
+                "mega_aggressive",
+                {'scaling_strength': 1.8, 'boost_factor': 4.0, 'top_tier_threshold': 0.9, 'mid_tier_threshold': 0.75},
+            ),
+            (
+                100000,
+                "super_aggressive",
+                {'scaling_strength': 1.6, 'boost_factor': 3.5, 'top_tier_threshold': 0.85, 'mid_tier_threshold': 0.7},
+            ),
+            (
+                50000,
+                "rank_based_aggressive",
+                {'scaling_strength': 1.5, 'boost_factor': 3.0, 'top_tier_threshold': 0.9, 'mid_tier_threshold': 0.7},
+            ),
+            (10000, "exponential_capped", {'scaling_strength': 1.2, 'boost_factor': 2.5, 'cap_threshold': 0.8}),
+            (1000, "exponential", {'scaling_strength': 1.0, 'boost_factor': 2.0, 'decay_rate': 0.1}),
+            (100, "power", {'scaling_strength': 1.1, 'boost_factor': 2.0}),
+            (0, "linear_boosted", {'scaling_strength': 0.8, 'boost_factor': 1.5}),
+        ]
+
+        # 选择合适的策略
+        for threshold, strategy, params in strategy_configs:
+            if results_count > threshold:
+                return strategy, params
+
+        # 默认返回（理论上不会到达这里）
+        return "linear_boosted", {'scaling_strength': 0.8, 'boost_factor': 1.5}
+
+    def _calculate_position_multiplier(self, bm25_score: float, strategy: str, params: dict) -> float:
+        """根据策略和参数计算位置调整倍数"""
+
+        # 定义策略计算器映射
+        strategy_calculators = {
+            "ultra_aggressive": self._calc_ultra_aggressive,
+            "mega_aggressive": self._calc_mega_aggressive,
+            "super_aggressive": self._calc_super_aggressive,
+            "rank_based_aggressive": self._calc_rank_based_aggressive,
+            "exponential_capped": self._calc_exponential_capped,
+            "power": self._calc_power,
+            "linear_boosted": self._calc_linear_boosted,
+            "exponential": self._calc_exponential,
+        }
+
+        calculator = strategy_calculators.get(strategy, self._calc_exponential)
+        return calculator(bm25_score, params)
+
+    def _calc_ultra_aggressive(self, bm25_score: float, params: dict) -> float:
+        """百万级数据的极致重排策略"""
+        thresholds = [
+            (params.get('top_tier_threshold', 0.95), 0.05),
+            (params.get('mid_tier_threshold', 0.8), 0.15),
+            (params.get('low_tier_threshold', 0.5), 0.4),
+        ]
+
+        for threshold, multiplier in thresholds:
+            if bm25_score > threshold:
+                return multiplier
+        return 1.5
+
+    def _calc_mega_aggressive(self, bm25_score: float, params: dict) -> float:
+        """50万级数据策略"""
+        thresholds = [
+            (params.get('top_tier_threshold', 0.9), 0.08),
+            (params.get('mid_tier_threshold', 0.75), 0.2),
+            (0.5, 0.5),
+        ]
+
+        for threshold, multiplier in thresholds:
+            if bm25_score > threshold:
+                return multiplier
+        return 1.3
+
+    def _calc_super_aggressive(self, bm25_score: float, params: dict) -> float:
+        """10万级数据策略"""
+        thresholds = [
+            (params.get('top_tier_threshold', 0.85), 0.1),
+            (params.get('mid_tier_threshold', 0.7), 0.25),
+            (0.5, 0.6),
+        ]
+
+        for threshold, multiplier in thresholds:
+            if bm25_score > threshold:
+                return multiplier
+        return 1.2
+
+    def _calc_rank_based_aggressive(self, bm25_score: float, params: dict) -> float:
+        """5万级数据策略"""
+        thresholds = [
+            (params.get('top_tier_threshold', 0.9), 0.1),
+            (params.get('mid_tier_threshold', 0.7), 0.3),
+            (0.5, 0.7),
+        ]
+
+        for threshold, multiplier in thresholds:
+            if bm25_score > threshold:
+                return multiplier
+        return 1.2
+
+    def _calc_exponential_capped(self, bm25_score: float, params: dict) -> float:
+        """带上限的指数策略"""
+        cap = params.get('cap_threshold', 0.8)
+        boost = params.get('boost_factor', 2.5)
+
+        if bm25_score > cap:
+            return 0.2
+        return 1.0 - boost * math.exp(-3.0 * bm25_score)
+
+    def _calc_power(self, bm25_score: float, params: dict) -> float:
+        """幂函数策略"""
+        power = 1.5 + params.get('scaling_strength', 1.0) * 0.5
+        return 0.2 + 0.8 * (bm25_score**power)
+
+    def _calc_linear_boosted(self, bm25_score: float, params: dict) -> float:
+        """增强线性策略"""
+        boost = params.get('scaling_strength', 0.8)
+        return 0.4 + 0.6 * bm25_score * boost
+
+    def _calc_exponential(self, bm25_score: float, params: dict) -> float:
+        """指数策略"""
+        decay = params.get('decay_rate', 0.1)
+        boost = params.get('boost_factor', 2.0)
+        return 1.0 - boost * math.exp(-3.0 * bm25_score) * (1.0 + decay)
+
+    def _build_corpus(self, results: list) -> list:
+        """构建语料库"""
+        corpus = []
+        for result in results:
+            text_parts = []
+
+            if hasattr(result, 'title') and result.title:
+                text_parts.append(result.title)
+            if hasattr(result, 'content') and result.content:
+                text_parts.append(result.content)
+            if hasattr(result, 'url') and result.url:
+                url_words = re.findall(r'[a-zA-Z]+', result.url)
+                text_parts.append(' '.join(url_words))
+
+            corpus.append(" ".join(text_parts))
+        return corpus
+
+    def _get_stopwords(self) -> set:
+        """获取停用词集合"""
+        return {
+            word
+            for name, value in stopwords_module.__dict__.items()
+            if name.startswith("STOPWORDS_") and isinstance(value, tuple)
+            for word in value
+        }
+
+    def _normalize_scores(self, raw_scores: np.ndarray) -> list:
+        """标准化分数"""
+        if len(raw_scores) == 0:
+            return []
+
+        min_score, max_score = float(np.min(raw_scores)), float(np.max(raw_scores))
+        if max_score > min_score:
+            return ((raw_scores - min_score) / (max_score - min_score)).tolist()
+        return [0.5] * len(raw_scores)
+
+    def _apply_rerank(
+        self, results: list, documents: list, normalized_scores: list, strategy: str, params: dict
+    ) -> None:
+        """应用重排序"""
+        for idx, doc_index in enumerate(documents[0]):
+            if doc_index >= len(results):
+                continue
+
+            score = float(normalized_scores[idx])
+            multiplier = self._calculate_position_multiplier(score, strategy, params)
+            result = results[doc_index]
+
+            if hasattr(result, 'positions') and result.positions:
+                for i, position in enumerate(result.positions):
+                    if isinstance(position, (int, float)):
+                        position_boost = float(max(0.01, position * multiplier))
+                        result.positions[i] = position_boost
+            else:
+                initial_position = float(doc_index + 1.0) / len(results)
+                bm25_position = float(initial_position * multiplier)
+                result.positions = [bm25_position]
+
+    def _process_results(self, results: list, query: str, strategy: str, params: dict) -> None:
+        """处理搜索结果并重新排序"""
+        if len(results) < 2:
+            return
+
+        # 构建语料库和获取停用词
+        corpus = self._build_corpus(results)
+        stopwords = self._get_stopwords()
+
+        # 分词
+        corpus_tokens = bm25s.tokenize(corpus, stopwords=stopwords)
+        query_tokens = bm25s.tokenize(query, stopwords=stopwords)
+
+        # BM25检索
+        retriever = bm25s.BM25()
+        retriever.index(corpus_tokens)
+
+        documents, scores = retriever.retrieve(query_tokens, k=len(corpus), return_as="tuple", show_progress=False)
+
+        # 标准化分数并应用重排
+        normalized_scores = self._normalize_scores(scores[0])
+        if normalized_scores:
+            self._apply_rerank(results, documents, normalized_scores, strategy, params)
+
+    def post_search(self, request: "SXNG_Request", search: "SearchWithPlugins") -> EngineResults:
+        results = search.result_container.get_ordered_results()
+
+        if len(results) < 2:
+            return search.result_container
+
+        query = search.search_query.query
+        strategy, strategy_params = self._select_adaptive_strategy(len(results))
+        self._process_results(results, query, strategy, strategy_params)
+
+        return search.result_container
diff --git a/searx/settings_defaults.py b/searx/settings_defaults.py
index c71103a4c68..8c52f6711e9 100644
--- a/searx/settings_defaults.py
+++ b/searx/settings_defaults.py
@@ -190,6 +190,7 @@ def apply_schema(settings: dict[str, t.Any], schema: dict[str, t.Any], path_list
     'search': {
         'safe_search': SettingsValue((0, 1, 2), 0),
         'autocomplete': SettingsValue(str, ''),
+        'autocomplete_engines': SettingsValue((list, str, False), ['']),
         'autocomplete_min': SettingsValue(int, 4),
         'favicon_resolver': SettingsValue(str, ''),
         'default_lang': SettingsValue(tuple(SXNG_LOCALE_TAGS + ['']), ''),
