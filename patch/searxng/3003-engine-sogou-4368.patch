diff --git a/searx/autocomplete.py b/searx/autocomplete.py
index a0aa2a73003..b8e8a44fb52 100644
--- a/searx/autocomplete.py
+++ b/searx/autocomplete.py
@@ -171,6 +171,27 @@ def seznam(query, _lang):
     ]
 
 
+def sogou(query, _lang):
+    # Sogou search autocompleter
+    base_url = "https://sor.html5.qq.com/api/getsug?"
+    response = get(base_url + urlencode({'m': 'searxng', 'key': query}))
+
+    if response.ok:
+        data = response.text
+
+        start_idx = data.find("[")
+        end_idx = data.rfind("]")
+
+        if start_idx != -1 and end_idx != -1:
+            try:
+                data_list = json.loads(data[start_idx : end_idx + 1])
+                return data_list[1] if isinstance(data_list, list) else []
+            except json.JSONDecodeError:
+                return []
+
+    return []
+
+
 def stract(query, _lang):
     # stract autocompleter (beta)
     url = f"https://stract.com/beta/api/autosuggest?q={quote_plus(query)}"
@@ -254,6 +275,7 @@ def yandex(query, _lang):
     'mwmbl': mwmbl,
     'qwant': qwant,
     'seznam': seznam,
+    'sogou': sogou,
     'stract': stract,
     'swisscows': swisscows,
     'wikipedia': wikipedia,
diff --git a/searx/engines/sogou.py b/searx/engines/sogou.py
new file mode 100644
index 00000000000..fa6595790ab
--- /dev/null
+++ b/searx/engines/sogou.py
@@ -0,0 +1,71 @@
+# SPDX-License-Identifier: AGPL-3.0-or-later
+"""Sogou search engine for searxng"""
+
+from urllib.parse import urlencode
+from lxml import html
+
+# Metadata
+about = {
+    "website": "https://www.sogou.com/",
+    "official_api_documentation": None,
+    "use_official_api": False,
+    "require_api_key": False,
+    "results": "HTML",
+}
+
+# Engine Configuration
+categories = ["general"]
+paging = True
+max_page = 10
+time_range_support = True
+
+time_range_dict = {'day': 'inttime_day', 'week': 'inttime_week', 'month': 'inttime_month', 'year': 'inttime_year'}
+
+# Base URL
+base_url = "https://www.sogou.com/web"
+
+
+def request(query, params):
+    query_params = {
+        "query": query,
+        "page": params["pageno"],
+    }
+
+    if 'time_range' in params:
+        query_params["s_from"] = time_range_dict[params['time_range']]
+        query_params["tsn"] = 1
+
+    params["url"] = f"{base_url}?{urlencode(query_params)}"
+    return params
+
+
+def response(resp):
+    dom = html.fromstring(resp.text)
+    results = []
+
+    for item in dom.xpath('//div[contains(@class, "vrwrap")]'):
+        title_elem = item.xpath('.//h3[contains(@class, "vr-title")]/a')
+        title = title_elem[0].text_content().strip() if title_elem else ""
+
+        url_elem = item.xpath('.//h3[contains(@class, "vr-title")]/a/@href')
+        url = url_elem[0] if url_elem else ""
+
+        if url.startswith("/link?url="):
+            url = f"https://www.sogou.com{url}"
+
+        content_elem = item.xpath('.//div[contains(@class, "text-layout")]//p[contains(@class, "star-wiki")]/text()')
+        content = " ".join(content_elem).strip() if content_elem else ""
+        if not content:
+            content_elem = item.xpath('.//div[contains(@class, "fz-mid space-txt")]/text()')
+            content = " ".join(content_elem).strip() if content_elem else ""
+
+        if title and url:
+            results.append(
+                {
+                    "title": title,
+                    "url": url,
+                    "content": content,
+                }
+            )
+
+    return results
