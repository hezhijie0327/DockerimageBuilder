diff --git a/searx/network/client.py b/searx/network/client.py
index c7c07c385b9..9629fc99fad 100644
--- a/searx/network/client.py
+++ b/searx/network/client.py
@@ -11,11 +11,16 @@
 import threading
 
 import httpx
-from httpx_socks import AsyncProxyTransport
+import httpx_curl_cffi
+import httpx_socks  # pyright: ignore[reportMissingTypeStubs]
 from python_socks import parse_proxy_url, ProxyConnectionError, ProxyTimeoutError, ProxyError
 
 from searx import logger
 
+if t.TYPE_CHECKING:
+    from curl_cffi import BrowserTypeLiteral
+
+
 CertTypes = str | tuple[str, str] | tuple[str, str, str]
 SslContextKeyType = tuple[str | None, CertTypes | None, bool, bool]
 
@@ -94,7 +99,7 @@ async def __aexit__(
         pass
 
 
-class AsyncProxyTransportFixed(AsyncProxyTransport):
+class AsyncProxyTransportFixed(httpx_socks.AsyncProxyTransport):
     """Fix httpx_socks.AsyncProxyTransport
 
     Map python_socks exceptions to httpx.ProxyError exceptions
@@ -112,7 +117,7 @@ async def handle_async_request(self, request: httpx.Request):
 
 
 def get_transport_for_socks_proxy(
-    verify: bool, http2: bool, local_address: str, proxy_url: str, limit: httpx.Limits, retries: int
+    verify: bool, http2: bool, local_address: str | None, proxy_url: str, limit: httpx.Limits, retries: int
 ):
     # support socks5h (requests compatibility):
     # https://requests.readthedocs.io/en/master/user/advanced/#socks
@@ -143,7 +148,7 @@ def get_transport_for_socks_proxy(
 
 
 def get_transport(
-    verify: bool, http2: bool, local_address: str, proxy_url: str | None, limit: httpx.Limits, retries: int
+    verify: bool, http2: bool, local_address: str | None, proxy_url: str | None, limit: httpx.Limits, retries: int
 ):
     _verify = get_sslcontexts(None, None, verify, True) if verify is True else verify
     return httpx.AsyncHTTPTransport(
@@ -159,6 +164,7 @@ def get_transport(
 
 def new_client(
     # pylint: disable=too-many-arguments
+    impersonate: "BrowserTypeLiteral | None",
     enable_http: bool,
     verify: bool,
     enable_http2: bool,
@@ -166,7 +172,7 @@ def new_client(
     max_keepalive_connections: int,
     keepalive_expiry: float,
     proxies: dict[str, str],
-    local_address: str,
+    local_address: str | None,
     retries: int,
     max_redirects: int,
     hook_log_response: t.Callable[..., t.Any] | None,
@@ -176,13 +182,31 @@ def new_client(
         max_keepalive_connections=max_keepalive_connections,
         keepalive_expiry=keepalive_expiry,
     )
+
     # See https://www.python-httpx.org/advanced/#routing
     mounts = {}
     mounts: None | (dict[str, t.Any | None]) = {}
+
+    # build transport object
+
     for pattern, proxy_url in proxies.items():
         if not enable_http and pattern.startswith('http://'):
             continue
-        if proxy_url.startswith('socks4://') or proxy_url.startswith('socks5://') or proxy_url.startswith('socks5h://'):
+        if impersonate:
+            mounts[pattern] = httpx_curl_cffi.AsyncCurlTransport(
+                impersonate=impersonate,
+                default_headers=True,
+                # required for parallel requests, see curl_cffi issues below
+                curl_options={httpx_curl_cffi.CurlOpt.FRESH_CONNECT: True},
+                http_version=(
+                    httpx_curl_cffi.CurlHttpVersion.V3 if enable_http2 else httpx_curl_cffi.CurlHttpVersion.V1_1
+                ),
+                proxy=proxy_url,
+                local_address=local_address,
+            )
+        elif (
+            proxy_url.startswith('socks4://') or proxy_url.startswith('socks5://') or proxy_url.startswith('socks5h://')
+        ):
             mounts[pattern] = get_transport_for_socks_proxy(
                 verify, enable_http2, local_address, proxy_url, limit, retries
             )
@@ -192,12 +216,26 @@ def new_client(
     if not enable_http:
         mounts['http://'] = AsyncHTTPTransportNoHttp()
 
-    transport = get_transport(verify, enable_http2, local_address, None, limit, retries)
+    if impersonate:
+        logger.debug("transport layer for this client is impersonate: %s", impersonate)
+        transport = httpx_curl_cffi.AsyncCurlTransport(
+            impersonate=impersonate,
+            default_headers=True,
+            # required for parallel requests, see curl_cffi issues below
+            curl_options={httpx_curl_cffi.CurlOpt.FRESH_CONNECT: True},
+            http_version=httpx_curl_cffi.CurlHttpVersion.V3 if enable_http2 else httpx_curl_cffi.CurlHttpVersion.V1_1,
+            local_address=local_address,
+        )
+    else:
+        logger.debug("transport layer for this client is httpx.AsyncHTTPTransport")
+        transport = get_transport(verify, enable_http2, local_address, None, limit, retries)
 
     event_hooks = None
     if hook_log_response:
         event_hooks = {'response': [hook_log_response]}
 
+    # build client ..
+
     return httpx.AsyncClient(
         transport=transport,
         mounts=mounts,
diff --git a/searx/network/network.py b/searx/network/network.py
index eb53afb7f08..a15de747a49 100644
--- a/searx/network/network.py
+++ b/searx/network/network.py
@@ -1,5 +1,5 @@
 # SPDX-License-Identifier: AGPL-3.0-or-later
-# pylint: disable=global-statement
+# pylint: disable=global-statement, too-many-arguments, too-many-positional-arguments
 # pylint: disable=missing-module-docstring, missing-class-docstring
 
 __all__ = ["get_network"]
@@ -20,6 +20,8 @@
 from .client import new_client, get_loop, AsyncHTTPTransportNoHttp
 from .raise_for_httperror import raise_for_httperror
 
+if t.TYPE_CHECKING:
+    from curl_cffi import BrowserTypeLiteral
 
 logger = logger.getChild('network')
 DEFAULT_NAME = '__DEFAULT__'
@@ -44,29 +46,9 @@
 @t.final
 class Network:
 
-    __slots__ = (
-        'enable_http',
-        'verify',
-        'enable_http2',
-        'max_connections',
-        'max_keepalive_connections',
-        'keepalive_expiry',
-        'local_addresses',
-        'proxies',
-        'using_tor_proxy',
-        'max_redirects',
-        'retries',
-        'retry_on_http_error',
-        '_local_addresses_cycle',
-        '_proxies_cycle',
-        '_clients',
-        '_logger',
-    )
-
     _TOR_CHECK_RESULT = {}
 
     def __init__(
-        # pylint: disable=too-many-arguments
         self,
         enable_http: bool = True,
         verify: bool = True,
@@ -81,6 +63,7 @@ def __init__(
         retry_on_http_error: bool = False,
         max_redirects: int = 30,
         logger_name: str = None,  # pyright: ignore[reportArgumentType]
+        impersonate: "BrowserTypeLiteral | None" = None,
     ):
 
         self.enable_http = enable_http
@@ -95,9 +78,10 @@ def __init__(
         self.retries = retries
         self.retry_on_http_error = retry_on_http_error
         self.max_redirects = max_redirects
+        self.impersonate: "BrowserTypeLiteral | None" = impersonate
         self._local_addresses_cycle = self.get_ipaddress_cycle()
         self._proxies_cycle = self.get_proxy_cycles()
-        self._clients = {}
+        self._clients: dict[t.Any, httpx.AsyncClient] = {}
         self._logger = logger.getChild(logger_name) if logger_name else logger
         self.check_parameters()
 
@@ -119,7 +103,7 @@ def iter_ipaddresses(self) -> Generator[str]:
             local_addresses = [local_addresses]
         yield from local_addresses
 
-    def get_ipaddress_cycle(self):
+    def get_ipaddress_cycle(self) -> Generator[str | None]:
         while True:
             count = 0
             for address in self.iter_ipaddresses():
@@ -190,23 +174,24 @@ async def check_tor_proxy(client: httpx.AsyncClient, proxies) -> bool:
     async def get_client(self, verify: bool | None = None, max_redirects: int | None = None) -> httpx.AsyncClient:
         verify = self.verify if verify is None else verify
         max_redirects = self.max_redirects if max_redirects is None else max_redirects
-        local_address = next(self._local_addresses_cycle)
+        local_address: str | None = next(self._local_addresses_cycle)
         proxies = next(self._proxies_cycle)  # is a tuple so it can be part of the key
-        key = (verify, max_redirects, local_address, proxies)
+        key = (verify, max_redirects, local_address, proxies, self.impersonate)
         hook_log_response = self.log_response if sxng_debug else None
         if key not in self._clients or self._clients[key].is_closed:
             client = new_client(
-                self.enable_http,
-                verify,
-                self.enable_http2,
-                self.max_connections,
-                self.max_keepalive_connections,
-                self.keepalive_expiry,
-                dict(proxies),
-                local_address,
-                0,
-                max_redirects,
-                hook_log_response,
+                impersonate=self.impersonate,
+                enable_http=self.enable_http,
+                verify=verify,
+                enable_http2=self.enable_http2,
+                max_connections=self.max_connections,
+                max_keepalive_connections=self.max_keepalive_connections,
+                keepalive_expiry=self.keepalive_expiry,
+                proxies=dict(proxies),
+                local_address=local_address,
+                retries=0,
+                max_redirects=max_redirects,
+                hook_log_response=hook_log_response,
             )
             if self.using_tor_proxy and not await self.check_tor_proxy(client, proxies):
                 await client.aclose()
@@ -274,10 +259,21 @@ async def call_client(self, stream: bool, method: str, url: str, **kwargs: t.Any
         was_disconnected = False
         do_raise_for_httperror = Network.extract_do_raise_for_httperror(kwargs)
         kwargs_clients = Network.extract_kwargs_clients(kwargs)
+
+        cookies = kwargs.pop("cookies", None)
+        kwargs["headers"] = {k.lower(): v for k, v in kwargs.get("headers", {}).items()}
+
+        if self.impersonate:
+            # In impersonate mode, it must be prevented that the User-Agent
+            # header from the browser is overwritten by the application; we use
+            # the default headers from:
+            # https://curl-cffi.readthedocs.io/en/latest/api.html#curl_cffi.Curl.impersonate:
+            kwargs["headers"].pop("user-agent", None)
+
         while retries >= 0:  # pragma: no cover
             client = await self.get_client(**kwargs_clients)
-            cookies = kwargs.pop("cookies", None)
             client.cookies = httpx.Cookies(cookies)
+
             try:
                 if stream:
                     return client.stream(method, url, **kwargs)
