From 756020cecb4169da8a4765c50ae6cfd290d3d4e8 Mon Sep 17 00:00:00 2001
From: Zhijie He <hezhijie0327@hotmail.com>
Date: Thu, 12 Feb 2026 13:58:03 +0800
Subject: [PATCH] fix: fix glm-* model default max_tokens issue

---
 .../src/providers/minimax/index.ts            | 11 ++--
 .../src/providers/zhipu/index.ts              |  7 ++-
 .../src/utils/getModelMaxOutput.test.ts       | 59 +++++++++++++++++++
 .../src/utils/getModelMaxOutput.ts            | 15 +++++
 4 files changed, 85 insertions(+), 7 deletions(-)
 create mode 100644 packages/model-runtime/src/utils/getModelMaxOutput.test.ts
 create mode 100644 packages/model-runtime/src/utils/getModelMaxOutput.ts

diff --git a/packages/model-runtime/src/providers/minimax/index.ts b/packages/model-runtime/src/providers/minimax/index.ts
index 288aedf..8aec99b 100644
--- a/packages/model-runtime/src/providers/minimax/index.ts
+++ b/packages/model-runtime/src/providers/minimax/index.ts
@@ -2,13 +2,9 @@ import { minimax as minimaxChatModels, ModelProvider } from 'model-bank';
 
 import { createOpenAICompatibleRuntime } from '../../core/openaiCompatibleFactory';
 import { resolveParameters } from '../../core/parameterResolver';
+import { getModelMaxOutput } from '../../utils/getModelMaxOutput';
 import { createMiniMaxImage } from './createImage';
 
-export const getMinimaxMaxOutputs = (modelId: string): number | undefined => {
-  const model = minimaxChatModels.find((model) => model.id === modelId);
-  return model ? model.maxOutput : undefined;
-};
-
 export const LobeMinimaxAI = createOpenAICompatibleRuntime({
   baseURL: 'https://api.minimaxi.com/v1',
   chatCompletion: {
@@ -46,7 +42,10 @@ export const LobeMinimaxAI = createOpenAICompatibleRuntime({
       // Resolve parameters with constraints
       const resolvedParams = resolveParameters(
         {
-          max_tokens: max_tokens !== undefined ? max_tokens : getMinimaxMaxOutputs(payload.model),
+          max_tokens:
+            max_tokens !== undefined
+              ? max_tokens
+              : getModelMaxOutput(payload.model, minimaxChatModels),
           temperature,
           top_p,
         },
diff --git a/packages/model-runtime/src/providers/zhipu/index.ts b/packages/model-runtime/src/providers/zhipu/index.ts
index 348254d..7eb71ef 100644
--- a/packages/model-runtime/src/providers/zhipu/index.ts
+++ b/packages/model-runtime/src/providers/zhipu/index.ts
@@ -1,4 +1,4 @@
-import { ModelProvider } from 'model-bank';
+import { ModelProvider,zhipu as zhipuChatModels } from 'model-bank';
 
 import {
   createOpenAICompatibleRuntime,
@@ -7,6 +7,7 @@ import {
 import { resolveParameters } from '../../core/parameterResolver';
 import { OpenAIStream } from '../../core/streams/openai';
 import { convertIterableToStream } from '../../core/streams/protocol';
+import { getModelMaxOutput } from '../../utils/getModelMaxOutput';
 import { MODEL_LIST_CONFIGS, processModelList } from '../../utils/modelParse';
 
 export interface ZhipuModelCard {
@@ -56,6 +57,10 @@ export const params = {
             : model === 'glm-zero-preview'
               ? { max: 15_300 }
               : undefined,
+          max_tokens:
+            max_tokens !== undefined
+              ? max_tokens
+              : getModelMaxOutput(payload.model, zhipuChatModels),
           normalizeTemperature: true,
           // glm-4-alltools has stricter temperature and top_p constraints
           ...(model === 'glm-4-alltools' && {
diff --git a/packages/model-runtime/src/utils/getModelMaxOutput.test.ts b/packages/model-runtime/src/utils/getModelMaxOutput.test.ts
new file mode 100644
index 0000000..f0e2226
--- /dev/null
+++ b/packages/model-runtime/src/utils/getModelMaxOutput.test.ts
@@ -0,0 +1,59 @@
+import  { type AIChatModelCard } from 'model-bank';
+import { describe, expect, it } from 'vitest';
+
+import { getModelMaxOutput } from './getModelMaxOutput';
+
+describe('getModelMaxOutput', () => {
+  it('should return maxOutput when model is found', () => {
+    const modelBank: AIChatModelCard[] = [
+      {
+        id: 'model-1',
+        maxOutput: 4096,
+        displayName: 'Model 1',
+        type: 'chat',
+        contextWindowTokens: 8192,
+      },
+      {
+        id: 'model-2',
+        maxOutput: 8192,
+        displayName: 'Model 2',
+        type: 'chat',
+        contextWindowTokens: 16384,
+      },
+    ];
+
+    expect(getModelMaxOutput('model-1', modelBank)).toBe(4096);
+    expect(getModelMaxOutput('model-2', modelBank)).toBe(8192);
+  });
+
+  it('should return undefined when model is not found', () => {
+    const modelBank: AIChatModelCard[] = [
+      {
+        id: 'model-1',
+        maxOutput: 4096,
+        displayName: 'Model 1',
+        type: 'chat',
+        contextWindowTokens: 8192,
+      },
+    ];
+
+    expect(getModelMaxOutput('unknown-model', modelBank)).toBeUndefined();
+  });
+
+  it('should return undefined when maxOutput is not defined', () => {
+    const modelBank: AIChatModelCard[] = [
+      {
+        id: 'model-no-maxoutput',
+        displayName: 'Model No MaxOutput',
+        type: 'chat',
+        contextWindowTokens: 8192,
+      },
+    ];
+
+    expect(getModelMaxOutput('model-no-maxoutput', modelBank)).toBeUndefined();
+  });
+
+  it('should handle empty model bank', () => {
+    expect(getModelMaxOutput('any-model', [])).toBeUndefined();
+  });
+});
diff --git a/packages/model-runtime/src/utils/getModelMaxOutput.ts b/packages/model-runtime/src/utils/getModelMaxOutput.ts
new file mode 100644
index 0000000..3a4f284
--- /dev/null
+++ b/packages/model-runtime/src/utils/getModelMaxOutput.ts
@@ -0,0 +1,15 @@
+import  { type AIChatModelCard } from 'model-bank';
+
+/**
+ * Retrieves the maximum output token limit for a given model from a model bank array
+ * @param modelId - The ID of the model to look up
+ * @param modelBank - Array of model cards to search in
+ * @returns The maxOutput value of the model if found, undefined otherwise
+ */
+export const getModelMaxOutput = (
+  modelId: string,
+  modelBank: AIChatModelCard[],
+): number | undefined => {
+  const model = modelBank.find((model) => model.id === modelId);
+  return model?.maxOutput;
+};
-- 
2.52.0

