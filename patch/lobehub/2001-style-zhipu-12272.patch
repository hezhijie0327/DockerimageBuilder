diff --git a/packages/model-bank/src/aiModels/zhipu.ts b/packages/model-bank/src/aiModels/zhipu.ts
index b539723ab74..3fb03674514 100644
--- a/packages/model-bank/src/aiModels/zhipu.ts
+++ b/packages/model-bank/src/aiModels/zhipu.ts
@@ -1,9 +1,70 @@
-import type { AIChatModelCard, AIImageModelCard } from '../types/aiModel';
+import { type AIChatModelCard, type AIImageModelCard } from '../types/aiModel';
 
 // price: https://bigmodel.cn/pricing
 // ref: https://docs.bigmodel.cn/cn/guide/start/model-overview
 
 const zhipuChatModels: AIChatModelCard[] = [
+  {
+    abilities: {
+      functionCall: true,
+      reasoning: true,
+      search: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'GLM-5 is Zhipu’s next-generation flagship foundation model, purpose-built for Agentic Engineering. It delivers reliable productivity in complex systems engineering and long-horizon agentic tasks. In coding and agent capabilities, GLM-5 achieves state-of-the-art performance among open-source models. In real-world programming scenarios, its user experience approaches that of Claude Opus 4.5. It excels at complex systems engineering and long-horizon agent tasks, making it an ideal foundation model for general-purpose agent assistants.',
+    displayName: 'GLM-5',
+    enabled: true,
+    id: 'glm-5',
+    maxOutput: 131_072,
+    pricing: {
+      currency: 'CNY',
+      units: [
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 1,
+              '[0.032, infinity]': 1.5,
+            },
+            pricingParams: ['textInput', 'textOutput'],
+          },
+          name: 'textInput_cacheRead',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 4,
+              '[0.032, infinity]': 6,
+            },
+            pricingParams: ['textInput', 'textOutput'],
+          },
+          name: 'textInput',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 18,
+              '[0.032, infinity]': 22,
+            },
+            pricingParams: ['textInput', 'textOutput'],
+          },
+          name: 'textOutput',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+      ],
+    },
+    releasedAt: '2026-02-12',
+    settings: {
+      extendParams: ['enableReasoning'],
+      searchImpl: 'params',
+    },
+    type: 'chat',
+  },
   {
     abilities: {
       functionCall: true,
@@ -61,6 +122,7 @@ const zhipuChatModels: AIChatModelCard[] = [
         },
       ],
     },
+    releasedAt: '2025-12-22',
     settings: {
       extendParams: ['enableReasoning'],
       searchImpl: 'params',
@@ -88,6 +150,7 @@ const zhipuChatModels: AIChatModelCard[] = [
         { name: 'textOutput', rate: 0, strategy: 'fixed', unit: 'millionTokens' },
       ],
     },
+    releasedAt: '2026-01-19',
     settings: {
       extendParams: ['enableReasoning'],
       searchImpl: 'params',
@@ -115,6 +178,160 @@ const zhipuChatModels: AIChatModelCard[] = [
         { name: 'textOutput', rate: 3, strategy: 'fixed', unit: 'millionTokens' },
       ],
     },
+    releasedAt: '2026-01-19',
+    settings: {
+      extendParams: ['enableReasoning'],
+      searchImpl: 'params',
+    },
+    type: 'chat',
+  },
+  {
+    abilities: {
+      functionCall: true,
+      reasoning: true,
+      search: true,
+      vision: true,
+    },
+    contextWindowTokens: 131_072,
+    description:
+      'The GLM-4.6V series represents a major iteration of the GLM family in the multimodal direction, comprising GLM-4.6V (flagship), GLM-4.6V-FlashX (lightweight and high-speed), and GLM-4.6V-Flash (fully free). It extends the training-time context window to 128k tokens, achieves state-of-the-art visual understanding accuracy at comparable parameter scales, and, for the first time, natively integrates Function Call (tool invocation) capabilities into the visual model architecture. This unifies the pipeline from “visual perception” to “executable actions,” providing a consistent technical foundation for multimodal agents in real-world production scenarios.',
+    displayName: 'GLM-4.6V',
+    enabled: true,
+    id: 'glm-4.6v',
+    maxOutput: 32_768,
+    pricing: {
+      currency: 'CNY',
+      units: [
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 0.2,
+              '[0.032, infinity]': 0.4,
+            },
+            pricingParams: ['textInput'],
+          },
+          name: 'textInput_cacheRead',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 1,
+              '[0.032, infinity]': 2,
+            },
+            pricingParams: ['textInput'],
+          },
+          name: 'textInput',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 3,
+              '[0.032, infinity]': 6,
+            },
+            pricingParams: ['textInput'],
+          },
+          name: 'textOutput',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+      ],
+    },
+    releasedAt: '2025-12-08',
+    settings: {
+      extendParams: ['enableReasoning'],
+      searchImpl: 'params',
+    },
+    type: 'chat',
+  },
+  {
+    abilities: {
+      functionCall: true,
+      reasoning: true,
+      search: true,
+      vision: true,
+    },
+    contextWindowTokens: 131_072,
+    description:
+      'The GLM-4.6V series represents a major iteration of the GLM family in the multimodal direction, comprising GLM-4.6V (flagship), GLM-4.6V-FlashX (lightweight and high-speed), and GLM-4.6V-Flash (fully free). It extends the training-time context window to 128k tokens, achieves state-of-the-art visual understanding accuracy at comparable parameter scales, and, for the first time, natively integrates Function Call (tool invocation) capabilities into the visual model architecture. This unifies the pipeline from “visual perception” to “executable actions,” providing a consistent technical foundation for multimodal agents in real-world production scenarios.',
+    displayName: 'GLM-4.6V-FlashX',
+    enabled: true,
+    id: 'glm-4.6v-flashx',
+    maxOutput: 32_768,
+    pricing: {
+      currency: 'CNY',
+      units: [
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 0.03,
+              '[0.032, infinity]': 0.03,
+            },
+            pricingParams: ['textInput'],
+          },
+          name: 'textInput_cacheRead',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 0.15,
+              '[0.032, infinity]': 0.3,
+            },
+            pricingParams: ['textInput'],
+          },
+          name: 'textInput',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+        {
+          lookup: {
+            prices: {
+              '[0, 0.032]': 1.5,
+              '[0.032, infinity]': 3,
+            },
+            pricingParams: ['textInput'],
+          },
+          name: 'textOutput',
+          strategy: 'lookup',
+          unit: 'millionTokens',
+        },
+      ],
+    },
+    releasedAt: '2025-12-08',
+    settings: {
+      extendParams: ['enableReasoning'],
+      searchImpl: 'params',
+    },
+    type: 'chat',
+  },
+  {
+    abilities: {
+      functionCall: true,
+      reasoning: true,
+      search: true,
+      vision: true,
+    },
+    contextWindowTokens: 131_072,
+    description:
+      'The GLM-4.6V series represents a major iteration of the GLM family in the multimodal direction, comprising GLM-4.6V (flagship), GLM-4.6V-FlashX (lightweight and high-speed), and GLM-4.6V-Flash (fully free). It extends the training-time context window to 128k tokens, achieves state-of-the-art visual understanding accuracy at comparable parameter scales, and, for the first time, natively integrates Function Call (tool invocation) capabilities into the visual model architecture. This unifies the pipeline from “visual perception” to “executable actions,” providing a consistent technical foundation for multimodal agents in real-world production scenarios.',
+    displayName: 'GLM-4.6V-Flash',
+    enabled: true,
+    id: 'glm-4.6v-flash',
+    maxOutput: 32_768,
+    pricing: {
+      currency: 'CNY',
+      units: [
+        { name: 'textInput_cacheRead', rate: 0, strategy: 'fixed', unit: 'millionTokens' },
+        { name: 'textInput', rate: 0, strategy: 'fixed', unit: 'millionTokens' },
+        { name: 'textOutput', rate: 0, strategy: 'fixed', unit: 'millionTokens' },
+      ],
+    },
+    releasedAt: '2025-12-08',
     settings: {
       extendParams: ['enableReasoning'],
       searchImpl: 'params',
@@ -177,6 +394,7 @@ const zhipuChatModels: AIChatModelCard[] = [
         },
       ],
     },
+    releasedAt: '2025-09-08',
     settings: {
       extendParams: ['enableReasoning'],
       searchImpl: 'params',
@@ -194,7 +412,6 @@ const zhipuChatModels: AIChatModelCard[] = [
     description:
       'Zhipu’s next-generation MoE vision reasoning model has 106B total parameters with 12B active, achieving SOTA among similarly sized open-source multimodal models across image, video, document understanding, and GUI tasks.',
     displayName: 'GLM-4.5V',
-    enabled: true,
     id: 'glm-4.5v',
     maxOutput: 16_384,
     pricing: {
@@ -900,7 +1117,36 @@ const zhipuChatModels: AIChatModelCard[] = [
 ];
 
 const zhipuImageModels: AIImageModelCard[] = [
-  // https://bigmodel.cn/dev/api/image-model/cogview
+  {
+    description:
+      'GLM-Image is Zhipu’s new flagship image generation model. The model was trained end-to-end on domestically produced chips and adopts an original hybrid architecture that combines autoregressive modeling with a diffusion decoder. This design enables strong global instruction understanding alongside fine-grained local detail rendering, overcoming long-standing challenges in generating knowledge-dense content such as posters, presentations, and educational diagrams. It represents an important exploration toward a new generation of “cognitive generative” technology paradigms, exemplified by Nano Banana Pro.',
+    displayName: 'GLM-Image',
+    enabled: true,
+    id: 'glm-image',
+    parameters: {
+      prompt: {
+        default: '',
+      },
+      size: {
+        default: '1280x1280',
+        enum: [
+          '1280x1280',
+          '1568x1056',
+          '1056x1568',
+          '1472x1088',
+          '1088x1472',
+          '1728x960',
+          '960x1728',
+        ],
+      },
+    },
+    pricing: {
+      currency: 'CNY',
+      units: [{ name: 'imageGeneration', rate: 0.1, strategy: 'fixed', unit: 'image' }],
+    },
+    releasedAt: '2026-01-14',
+    type: 'image',
+  },
   {
     description:
       'CogView-4 is Zhipu’s first open-source text-to-image model that can generate Chinese characters. It improves semantic understanding, image quality, and Chinese/English text rendering, supports arbitrary-length bilingual prompts, and can generate images at any resolution within specified ranges.',
@@ -923,6 +1169,27 @@ const zhipuImageModels: AIImageModelCard[] = [
     releasedAt: '2025-03-04',
     type: 'image',
   },
+  {
+    description:
+      'CogView-3-Flash is a free image generation model launched by Zhipu. It generates images that align with user instructions while achieving higher aesthetic quality scores. CogView-3-Flash is primarily used in fields such as artistic creation, design reference, game development, and virtual reality, helping users rapidly convert text descriptions into images.',
+    displayName: 'CogView-3-Flash',
+    enabled: true,
+    id: 'cogview-3-flash',
+    parameters: {
+      prompt: {
+        default: '',
+      },
+      size: {
+        default: '1024x1024',
+        enum: ['1024x1024', '768x1344', '864x1152', '1344x768', '1152x864', '1440x720', '720x1440'],
+      },
+    },
+    pricing: {
+      currency: 'CNY',
+      units: [{ name: 'imageGeneration', rate: 0, strategy: 'fixed', unit: 'image' }],
+    },
+    type: 'image',
+  },
 ];
 
 export const allModels = [...zhipuChatModels, ...zhipuImageModels];
diff --git a/packages/model-runtime/src/providers/minimax/index.ts b/packages/model-runtime/src/providers/minimax/index.ts
index 288aedf5740..9dd3df7a427 100644
--- a/packages/model-runtime/src/providers/minimax/index.ts
+++ b/packages/model-runtime/src/providers/minimax/index.ts
@@ -2,13 +2,9 @@ import { minimax as minimaxChatModels, ModelProvider } from 'model-bank';
 
 import { createOpenAICompatibleRuntime } from '../../core/openaiCompatibleFactory';
 import { resolveParameters } from '../../core/parameterResolver';
+import { getModelMaxOutputs } from '../../utils/getModelMaxOutputs';
 import { createMiniMaxImage } from './createImage';
 
-export const getMinimaxMaxOutputs = (modelId: string): number | undefined => {
-  const model = minimaxChatModels.find((model) => model.id === modelId);
-  return model ? model.maxOutput : undefined;
-};
-
 export const LobeMinimaxAI = createOpenAICompatibleRuntime({
   baseURL: 'https://api.minimaxi.com/v1',
   chatCompletion: {
@@ -46,7 +42,10 @@ export const LobeMinimaxAI = createOpenAICompatibleRuntime({
       // Resolve parameters with constraints
       const resolvedParams = resolveParameters(
         {
-          max_tokens: max_tokens !== undefined ? max_tokens : getMinimaxMaxOutputs(payload.model),
+          max_tokens:
+            max_tokens !== undefined
+              ? max_tokens
+              : getModelMaxOutputs(payload.model, minimaxChatModels),
           temperature,
           top_p,
         },
diff --git a/packages/model-runtime/src/providers/zhipu/index.test.ts b/packages/model-runtime/src/providers/zhipu/index.test.ts
index 0efdb5e8bff..7ef068cbfc4 100644
--- a/packages/model-runtime/src/providers/zhipu/index.test.ts
+++ b/packages/model-runtime/src/providers/zhipu/index.test.ts
@@ -1,7 +1,7 @@
 // @vitest-environment node
 import { afterEach, beforeEach, describe, expect, it, vi } from 'vitest';
 
-import type { LobeOpenAICompatibleRuntime } from '../../core/BaseAI';
+import { type LobeOpenAICompatibleRuntime } from '../../core/BaseAI';
 import { testProvider } from '../../providerTestUtils';
 import { LobeZhipuAI, params } from './index';
 
@@ -384,39 +384,6 @@ describe('LobeZhipuAI - custom features', () => {
       });
     });
 
-    describe('Stream parameter', () => {
-      it('should always set stream to true', async () => {
-        await instance.chat({
-          messages: [{ content: 'Hello', role: 'user' }],
-          model: 'glm-4',
-          temperature: 0.5,
-        });
-
-        expect(instance['client'].chat.completions.create).toHaveBeenCalledWith(
-          expect.objectContaining({
-            stream: true,
-          }),
-          expect.anything(),
-        );
-      });
-
-      it('should override stream parameter to true', async () => {
-        await instance.chat({
-          messages: [{ content: 'Hello', role: 'user' }],
-          model: 'glm-4',
-          stream: false,
-          temperature: 0.5,
-        });
-
-        expect(instance['client'].chat.completions.create).toHaveBeenCalledWith(
-          expect.objectContaining({
-            stream: true,
-          }),
-          expect.anything(),
-        );
-      });
-    });
-
     describe('Preserve other payload properties', () => {
       it('should preserve all other properties', async () => {
         await instance.chat({
diff --git a/packages/model-runtime/src/providers/zhipu/index.ts b/packages/model-runtime/src/providers/zhipu/index.ts
index 5561139e27e..191cf8e876c 100644
--- a/packages/model-runtime/src/providers/zhipu/index.ts
+++ b/packages/model-runtime/src/providers/zhipu/index.ts
@@ -1,10 +1,13 @@
-import { ModelProvider } from 'model-bank';
+import { ModelProvider,zhipu as zhipuChatModels } from 'model-bank';
 
-import type { OpenAICompatibleFactoryOptions } from '../../core/openaiCompatibleFactory';
-import { createOpenAICompatibleRuntime } from '../../core/openaiCompatibleFactory';
+import {
+  createOpenAICompatibleRuntime,
+  type OpenAICompatibleFactoryOptions,
+} from '../../core/openaiCompatibleFactory';
 import { resolveParameters } from '../../core/parameterResolver';
 import { OpenAIStream } from '../../core/streams/openai';
 import { convertIterableToStream } from '../../core/streams/protocol';
+import { getModelMaxOutputs } from '../../utils/getModelMaxOutputs';
 import { MODEL_LIST_CONFIGS, processModelList } from '../../utils/modelParse';
 
 export interface ZhipuModelCard {
@@ -17,8 +20,17 @@ export const params = {
   baseURL: 'https://open.bigmodel.cn/api/paas/v4',
   chatCompletion: {
     handlePayload: (payload) => {
-      const { enabledSearch, max_tokens, model, temperature, thinking, tools, top_p, ...rest } =
-        payload;
+      const {
+        enabledSearch,
+        max_tokens,
+        model,
+        stream,
+        temperature,
+        thinking,
+        tools,
+        top_p,
+        ...rest
+      } = payload;
 
       const zhipuTools = enabledSearch
         ? [
@@ -37,7 +49,14 @@ export const params = {
 
       // Resolve parameters based on model-specific constraints
       const resolvedParams = resolveParameters(
-        { max_tokens, temperature, top_p },
+        {
+          max_tokens:
+            max_tokens !== undefined
+              ? max_tokens
+              : getModelMaxOutputs(payload.model, zhipuChatModels),
+          temperature,
+          top_p,
+        },
         {
           // max_tokens constraints
           maxTokensRange: model.includes('glm-4v')
@@ -58,8 +77,9 @@ export const params = {
         ...rest,
         ...resolvedParams,
         model,
-        stream: true,
+        stream,
         thinking: thinking ? { type: thinking.type } : undefined,
+        tool_stream: stream && /^glm-(?:4\.(?:6|7)|5)$/.test(model) ? true : undefined,
         tools: zhipuTools,
       } as any;
     },
diff --git a/packages/model-runtime/src/utils/getModelMaxOutputs.test.ts b/packages/model-runtime/src/utils/getModelMaxOutputs.test.ts
new file mode 100644
index 00000000000..7c4bf581c77
--- /dev/null
+++ b/packages/model-runtime/src/utils/getModelMaxOutputs.test.ts
@@ -0,0 +1,44 @@
+import  { type AiFullModelCard } from 'model-bank';
+import { describe, expect, it } from 'vitest';
+
+import { getModelMaxOutputs } from './getModelMaxOutputs';
+
+describe('getModelMaxOutputs', () => {
+  const mockModels: AiFullModelCard[] = [
+    {
+      id: 'model-1',
+      providerId: 'provider-1',
+      type: 'chat',
+      maxOutput: 1000,
+    } as AiFullModelCard,
+    {
+      id: 'model-2',
+      providerId: 'provider-1',
+      type: 'chat',
+      maxOutput: 2000,
+    } as AiFullModelCard,
+    {
+      id: 'model-3',
+      providerId: 'provider-2',
+      type: 'chat',
+      maxOutput: undefined,
+    } as AiFullModelCard,
+  ] as AiFullModelCard[];
+
+  it('should return maxOutput for existing model', () => {
+    expect(getModelMaxOutputs('model-1', mockModels)).toBe(1000);
+    expect(getModelMaxOutputs('model-2', mockModels)).toBe(2000);
+  });
+
+  it('should return undefined for model with undefined maxOutput', () => {
+    expect(getModelMaxOutputs('model-3', mockModels)).toBeUndefined();
+  });
+
+  it('should return undefined for non-existing model', () => {
+    expect(getModelMaxOutputs('non-existing', mockModels)).toBeUndefined();
+  });
+
+  it('should return undefined for empty models array', () => {
+    expect(getModelMaxOutputs('model-1', [])).toBeUndefined();
+  });
+});
diff --git a/packages/model-runtime/src/utils/getModelMaxOutputs.ts b/packages/model-runtime/src/utils/getModelMaxOutputs.ts
new file mode 100644
index 00000000000..a7c04029f5b
--- /dev/null
+++ b/packages/model-runtime/src/utils/getModelMaxOutputs.ts
@@ -0,0 +1,15 @@
+import  { type AiFullModelCard } from 'model-bank';
+
+/**
+ * Get the max outputs for a specific model from the provider's model list
+ * @param modelId The ID of the model
+ * @param models The provider's model list to search in
+ * @returns The max output value or undefined
+ */
+export const getModelMaxOutputs = (
+  modelId: string,
+  models: AiFullModelCard[],
+): number | undefined => {
+  const model = models.find((model) => model.id === modelId);
+  return model ? model.maxOutput : undefined;
+};
