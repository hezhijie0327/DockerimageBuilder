diff --git a/src/app/(main)/settings/llm/ProviderList/providers.tsx b/src/app/(main)/settings/llm/ProviderList/providers.tsx
index 8fb6fc359b64..bc204f1dbead 100644
--- a/src/app/(main)/settings/llm/ProviderList/providers.tsx
+++ b/src/app/(main)/settings/llm/ProviderList/providers.tsx
@@ -17,6 +17,7 @@ import {
   OpenRouterProviderCard,
   PerplexityProviderCard,
   QwenProviderCard,
+  SenseCoreProviderCard,
   SiliconCloudProviderCard,
   SparkProviderCard,
   StepfunProviderCard,
@@ -65,6 +66,7 @@ export const useProviderList = (): ProviderItem[] => {
       SparkProviderCard,
       ZhiPuProviderCard,
       ZeroOneProviderCard,
+      SenseCoreProviderCard,
       StepfunProviderCard,
       MoonshotProviderCard,
       BaichuanProviderCard,
diff --git a/src/app/api/chat/agentRuntime.ts b/src/app/api/chat/agentRuntime.ts
index f215a6ea0004..fe2986448bfb 100644
--- a/src/app/api/chat/agentRuntime.ts
+++ b/src/app/api/chat/agentRuntime.ts
@@ -253,6 +253,22 @@ const getLlmOptionsFromPayload = (provider: string, payload: JWTPayload) => {
 
       return { apiKey };
     }
+    case ModelProvider.SenseCore: {
+      const { SENSECORE_ACCESS_KEY_ID, SENSECORE_ACCESS_KEY_SECRET } = getLLMConfig();
+
+      const sensecoreAccessKeyID = payload?.sensecoreAccessKeyID || SENSECORE_ACCESS_KEY_ID;
+      const sensecoreAccessKeySecret = payload?.sensecoreAccessKeySecret || SENSECORE_ACCESS_KEY_SECRET;
+
+      encodeJwtTokenSenseCore(sensecoreAccessKeyID, sensecoreAccessKeySecret)
+        .then(apiKey => {
+          console.log('Generated API Key:', apiKey);
+
+          return { apiKey };
+        })
+        .catch(error => {
+          console.error('Error generating API Key:', error);
+        });
+    }
   }
 };
 
diff --git a/src/config/llm.ts b/src/config/llm.ts
index f967c108c156..f6f1daa781f2 100644
--- a/src/config/llm.ts
+++ b/src/config/llm.ts
@@ -121,6 +121,10 @@ export const getLLMConfig = () => {
       ENABLED_HUNYUAN: z.boolean(),
       HUNYUAN_API_KEY: z.string().optional(),
       HUNYUAN_MODEL_LIST: z.string().optional(),
+
+      ENABLED_SENSECORE: z.boolean(),
+      SENSECORE_API_KEY: z.string().optional(),
+      SENSECORE_MODEL_LIST: z.string().optional(),
     },
     runtimeEnv: {
       API_KEY_SELECT_MODE: process.env.API_KEY_SELECT_MODE,
@@ -239,6 +243,10 @@ export const getLLMConfig = () => {
       ENABLED_HUNYUAN: !!process.env.HUNYUAN_API_KEY,
       HUNYUAN_API_KEY: process.env.HUNYUAN_API_KEY,
       HUNYUAN_MODEL_LIST: process.env.HUNYUAN_MODEL_LIST,
+
+      ENABLED_SENSECORE: !!process.env.SENSECORE_API_KEY,
+      SENSECORE_API_KEY: process.env.SENSECORE_API_KEY,
+      SENSECORE_MODEL_LIST: process.env.SENSECORE_MODEL_LIST,
     },
   });
 };
diff --git a/src/config/modelProviders/index.ts b/src/config/modelProviders/index.ts
index 2dbd8c842b75..227a22308444 100644
--- a/src/config/modelProviders/index.ts
+++ b/src/config/modelProviders/index.ts
@@ -21,6 +21,7 @@ import OpenAIProvider from './openai';
 import OpenRouterProvider from './openrouter';
 import PerplexityProvider from './perplexity';
 import QwenProvider from './qwen';
+import SenseCoreProvider from './sensecore';
 import SiliconCloudProvider from './siliconcloud';
 import SparkProvider from './spark';
 import StepfunProvider from './stepfun';
@@ -59,6 +60,7 @@ export const LOBE_DEFAULT_MODEL_LIST: ChatModelCard[] = [
   SparkProvider.chatModels,
   Ai21Provider.chatModels,
   HunyuanProvider.chatModels,
+  SenseCoreProvider.chatModels,
 ].flat();
 
 export const DEFAULT_MODEL_PROVIDER_LIST = [
@@ -91,6 +93,7 @@ export const DEFAULT_MODEL_PROVIDER_LIST = [
   Ai360Provider,
   TaichuProvider,
   SiliconCloudProvider,
+  SenseCoreProvider,
 ];
 
 export const filterEnabledModels = (provider: ModelProviderCard) => {
@@ -123,6 +126,7 @@ export { default as OpenAIProviderCard } from './openai';
 export { default as OpenRouterProviderCard } from './openrouter';
 export { default as PerplexityProviderCard } from './perplexity';
 export { default as QwenProviderCard } from './qwen';
+export { default as SenseCoreProviderCard } from './sensecore';
 export { default as SiliconCloudProviderCard } from './siliconcloud';
 export { default as SparkProviderCard } from './spark';
 export { default as StepfunProviderCard } from './stepfun';
diff --git a/src/config/modelProviders/sensecore.ts b/src/config/modelProviders/sensecore.ts
new file mode 100644
index 000000000000..5fb1919fb5d7
--- /dev/null
+++ b/src/config/modelProviders/sensecore.ts
@@ -0,0 +1,124 @@
+import { ModelProviderCard } from '@/types/llm';
+
+// ref https://console.sensecore.cn/help/docs/model-as-a-service/nova/model/llm/GeneralLLM
+// ref https://console.sensecore.cn/help/docs/model-as-a-service/nova/pricing
+const SenseCore: ModelProviderCard = {
+  chatModels: [
+    {
+      description: '版本：V5.5。日日新系列目前的主力大模型，国内首个流式多模态交互大模型，显著提升数理逻辑和指令跟随能力，综合性能较「SenseChat-5」提升30%，交互效果和多项核心指标实现对标GPT-4o。',
+      displayName: 'SenseChat 5.5',
+      enabled: true,
+      functionCall: true,
+      id: 'SenseChat-5',
+      pricing: {
+        currency: 'CNY',
+        input: 40,
+        output: 100,
+      },
+      tokens: 131_072,
+    },
+    {
+      description: 'SenseChat-5.0，支持128k上下文长度，在各项能力表现较为均衡，尤其在生成创作、角色扮演、安全能力、工具使用上表现较好。',
+      displayName: 'SenseChat 5.0 128K',
+      enabled: true,
+      id: 'SenseChat-128K',
+      pricing: {
+        currency: 'CNY',
+        input: 60,
+        output: 60,
+      },
+      tokens: 131_072,
+    },
+    {
+      description: '基于SenseChat-5.0的轻量版本，模型运行速度更快，支持32k最大上下文长度。',
+      displayName: 'SenseChat 5.0 Turbo',
+      enabled: true,
+      id: 'SenseChat-Turbo',
+      pricing: {
+        currency: 'CNY',
+        input: 2,
+        output: 5,
+      },
+      tokens: 32_768,
+    },
+    {
+      description: '版本：5.0。图文感知能力达到全球领先水平，在多模态大模型权威综合基准测试MMBench中综合得分排名首位，在多个知名多模态榜单MathVista、AI2D、ChartQA、TextVQA、DocVQA、MMMU 取得领先成绩。',
+      displayName: 'SenseChat 5.0 Vision',
+      enabled: true,
+      id: 'SenseChat-Vision',
+      pricing: {
+        currency: 'CNY',
+        input: 100,
+        output: 100,
+      },
+      tokens: 4096,
+      vision: true,
+    },
+    {
+      description: '基于SenseChat-5.0专门为适应粤语语种地区的对话习惯、俚语及本地知识而设计，显著提升在粤语本土化对话理解方面的能力，支持32k长文本输入。',
+      displayName: 'SenseChat 5.0 Cantonese',
+      id: 'SenseChat-5-Cantonese',
+      pricing: {
+        currency: 'CNY',
+        input: 27,
+        output: 27,
+      },
+      tokens: 32_768,
+    },
+    {
+      description: 'SenseChat-4.0，支持4k上下文长度，响应快，在各项能力表现较为均衡，尤其在生成创作、角色扮演、安全能力、工具使用上表现较好。',
+      displayName: 'SenseChat 4.0',
+      enabled: true,
+      id: 'SenseChat',
+      pricing: {
+        currency: 'CNY',
+        input: 12,
+        output: 12,
+      },
+      tokens: 4096,
+    },
+    {
+      description: 'SenseChat-4.0，支持32k上下文长度，在各项能力表现较为均衡，尤其在生成创作、角色扮演、安全能力、工具使用上表现较好。',
+      displayName: 'SenseChat 4.0 32K',
+      enabled: true,
+      id: 'SenseChat-32K',
+      pricing: {
+        currency: 'CNY',
+        input: 36,
+        output: 36,
+      },
+      tokens: 32_768,
+    },
+    {
+      description: '商汤支持设置角色创建、知识库构建、多人群聊、角色其密度等能力，支持8K上下文长度。',
+      displayName: 'SenseChat Character',
+      id: 'SenseChat-Character',
+      pricing: {
+        currency: 'CNY',
+        input: 12,
+        output: 12,
+      },
+      tokens: 8192,
+    },
+    {
+      description: '在标准版的基础上，角色对话、人设、及剧情推动能力全面提升；支持32K上下文长度；支持中/英文对话，赋能海外拟人对话场景。',
+      displayName: 'SenseChat Character Pro',
+      id: 'SenseChat-Character-Pro',
+      pricing: {
+        currency: 'CNY',
+        input: 15,
+        output: 15,
+      },
+      tokens: 32_768,
+    },
+  ],
+  checkModel: 'SenseChat-Turbo',
+  disableBrowserRequest: true,
+  id: 'sensecore',
+  modelList: { showModelFetcher: true },
+  modelsUrl: 'https://console.sensecore.cn/help/docs/model-as-a-service/nova/model/llm/GeneralLLM',
+  name: 'SenseCore',
+  url: 'https://console.sensecore.cn/aistudio',
+};
+
+export default SenseCore;
diff --git a/src/const/settings/llm.ts b/src/const/settings/llm.ts
index 77d4eb61441d..6a32c165afbf 100644
--- a/src/const/settings/llm.ts
+++ b/src/const/settings/llm.ts
@@ -19,6 +19,7 @@ import {
   OpenRouterProviderCard,
   PerplexityProviderCard,
   QwenProviderCard,
+  SenseCoreProviderCard,
   SiliconCloudProviderCard,
   SparkProviderCard,
   StepfunProviderCard,
@@ -117,6 +118,10 @@ export const DEFAULT_LLM_CONFIG: UserModelProviderConfig = {
     enabled: false,
     enabledModels: filterEnabledModels(QwenProviderCard),
   },
+  sensecore: {
+    enabled: false,
+    enabledModels: filterEnabledModels(SenseCoreProviderCard),
+  },
   siliconcloud: {
     enabled: false,
     enabledModels: filterEnabledModels(SiliconCloudProviderCard),
diff --git a/src/libs/agent-runtime/AgentRuntime.ts b/src/libs/agent-runtime/AgentRuntime.ts
index 68ea3ade7cd7..ae9765a04c97 100644
--- a/src/libs/agent-runtime/AgentRuntime.ts
+++ b/src/libs/agent-runtime/AgentRuntime.ts
@@ -24,6 +24,7 @@ import { LobeOpenAI } from './openai';
 import { LobeOpenRouterAI } from './openrouter';
 import { LobePerplexityAI } from './perplexity';
 import { LobeQwenAI } from './qwen';
+import { LobeSenseCoreAI } from './sensecore';
 import { LobeSiliconCloudAI } from './siliconcloud';
 import { LobeSparkAI } from './spark';
 import { LobeStepfunAI } from './stepfun';
@@ -144,6 +145,7 @@ class AgentRuntime {
       openrouter: Partial<ClientOptions>;
       perplexity: Partial<ClientOptions>;
       qwen: Partial<ClientOptions>;
+      sensecore: Partial<ClientOptions>;
       siliconcloud: Partial<ClientOptions>;
       spark: Partial<ClientOptions>;
       stepfun: Partial<ClientOptions>;
@@ -307,6 +309,11 @@ class AgentRuntime {
         runtimeModel = new LobeHunyuanAI(params.hunyuan);
         break;
       }
+
+      case ModelProvider.SenseCore: {
+        runtimeModel = new LobeSenseCoreAI(params.sensecore);
+        break;
+      }
     }
 
     return new AgentRuntime(runtimeModel);
diff --git a/src/libs/agent-runtime/sensecore/index.test.ts b/src/libs/agent-runtime/sensecore/index.test.ts
new file mode 100644
index 000000000000..665d26385436
--- /dev/null
+++ b/src/libs/agent-runtime/sensecore/index.test.ts
@@ -0,0 +1,255 @@
+// @vitest-environment node
+import OpenAI from 'openai';
+import { Mock, afterEach, beforeEach, describe, expect, it, vi } from 'vitest';
+
+import {
+  ChatStreamCallbacks,
+  LobeOpenAICompatibleRuntime,
+  ModelProvider,
+} from '@/libs/agent-runtime';
+
+import * as debugStreamModule from '../utils/debugStream';
+import { LobeSenseCoreAI } from './index';
+
+const provider = ModelProvider.SenseCore;
+const defaultBaseURL = 'https://api.sensenova.cn/compatible-mode/v1';
+
+const bizErrorType = 'ProviderBizError';
+const invalidErrorType = 'InvalidProviderAPIKey';
+
+// Mock the console.error to avoid polluting test output
+vi.spyOn(console, 'error').mockImplementation(() => {});
+
+let instance: LobeOpenAICompatibleRuntime;
+
+beforeEach(() => {
+  instance = new LobeSenseCoreAI({ apiKey: 'test' });
+
+  // 使用 vi.spyOn 来模拟 chat.completions.create 方法
+  vi.spyOn(instance['client'].chat.completions, 'create').mockResolvedValue(
+    new ReadableStream() as any,
+  );
+});
+
+afterEach(() => {
+  vi.clearAllMocks();
+});
+
+describe('LobeSenseCoreAI', () => {
+  describe('init', () => {
+    it('should correctly initialize with an API key', async () => {
+      const instance = new LobeSenseCoreAI({ apiKey: 'test_api_key' });
+      expect(instance).toBeInstanceOf(LobeSenseCoreAI);
+      expect(instance.baseURL).toEqual(defaultBaseURL);
+    });
+  });
+
+  describe('chat', () => {
+    describe('Error', () => {
+      it('should return OpenAIBizError with an openai error response when OpenAI.APIError is thrown', async () => {
+        // Arrange
+        const apiError = new OpenAI.APIError(
+          400,
+          {
+            status: 400,
+            error: {
+              message: 'Bad Request',
+            },
+          },
+          'Error message',
+          {},
+        );
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(apiError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            error: {
+              error: { message: 'Bad Request' },
+              status: 400,
+            },
+            errorType: bizErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should throw AgentRuntimeError with NoOpenAIAPIKey if no apiKey is provided', async () => {
+        try {
+          new LobeSenseCoreAI({});
+        } catch (e) {
+          expect(e).toEqual({ errorType: invalidErrorType });
+        }
+      });
+
+      it('should return OpenAIBizError with the cause when OpenAI.APIError is thrown with cause', async () => {
+        // Arrange
+        const errorInfo = {
+          stack: 'abc',
+          cause: {
+            message: 'api is undefined',
+          },
+        };
+        const apiError = new OpenAI.APIError(400, errorInfo, 'module error', {});
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(apiError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            error: {
+              cause: { message: 'api is undefined' },
+              stack: 'abc',
+            },
+            errorType: bizErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should return OpenAIBizError with an cause response with desensitize Url', async () => {
+        // Arrange
+        const errorInfo = {
+          stack: 'abc',
+          cause: { message: 'api is undefined' },
+        };
+        const apiError = new OpenAI.APIError(400, errorInfo, 'module error', {});
+
+        instance = new LobeSenseCoreAI({
+          apiKey: 'test',
+
+          baseURL: 'https://api.abc.com/v1',
+        });
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(apiError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: 'https://api.***.com/v1',
+            error: {
+              cause: { message: 'api is undefined' },
+              stack: 'abc',
+            },
+            errorType: bizErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should throw an InvalidSenseCoreAPIKey error type on 401 status code', async () => {
+        // Mock the API call to simulate a 401 error
+        const error = new Error('Unauthorized') as any;
+        error.status = 401;
+        vi.mocked(instance['client'].chat.completions.create).mockRejectedValue(error);
+
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          // Expect the chat method to throw an error with InvalidSenseCoreAPIKey
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            error: new Error('Unauthorized'),
+            errorType: invalidErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should return AgentRuntimeError for non-OpenAI errors', async () => {
+        // Arrange
+        const genericError = new Error('Generic Error');
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(genericError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            errorType: 'AgentRuntimeError',
+            provider,
+            error: {
+              name: genericError.name,
+              cause: genericError.cause,
+              message: genericError.message,
+              stack: genericError.stack,
+            },
+          });
+        }
+      });
+    });
+
+    describe('DEBUG', () => {
+      it('should call debugStream and return StreamingTextResponse when DEBUG_SENSECORE_CHAT_COMPLETION is 1', async () => {
+        // Arrange
+        const mockProdStream = new ReadableStream() as any; // 模拟的 prod 流
+        const mockDebugStream = new ReadableStream({
+          start(controller) {
+            controller.enqueue('Debug stream content');
+            controller.close();
+          },
+        }) as any;
+        mockDebugStream.toReadableStream = () => mockDebugStream; // 添加 toReadableStream 方法
+
+        // 模拟 chat.completions.create 返回值，包括模拟的 tee 方法
+        (instance['client'].chat.completions.create as Mock).mockResolvedValue({
+          tee: () => [mockProdStream, { toReadableStream: () => mockDebugStream }],
+        });
+
+        // 保存原始环境变量值
+        const originalDebugValue = process.env.DEBUG_SENSECORE_CHAT_COMPLETION;
+
+        // 模拟环境变量
+        process.env.DEBUG_SENSECORE_CHAT_COMPLETION = '1';
+        vi.spyOn(debugStreamModule, 'debugStream').mockImplementation(() => Promise.resolve());
+
+        // 执行测试
+        // 运行你的测试函数，确保它会在条件满足时调用 debugStream
+        // 假设的测试函数调用，你可能需要根据实际情况调整
+        await instance.chat({
+          messages: [{ content: 'Hello', role: 'user' }],
+          model: 'SenseChat-Turbo',
+          stream: true,
+          temperature: 0,
+        });
+
+        // 验证 debugStream 被调用
+        expect(debugStreamModule.debugStream).toHaveBeenCalled();
+
+        // 恢复原始环境变量值
+        process.env.DEBUG_SENSECORE_CHAT_COMPLETION = originalDebugValue;
+      });
+    });
+  });
+});
diff --git a/src/libs/agent-runtime/sensecore/index.ts b/src/libs/agent-runtime/sensecore/index.ts
new file mode 100644
index 000000000000..462123af403c
--- /dev/null
+++ b/src/libs/agent-runtime/sensecore/index.ts
@@ -0,0 +1,24 @@
+import OpenAI from 'openai';
+
+import { ChatStreamPayload, ModelProvider } from '../types';
+import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
+
+export const LobeSenseCoreAI = LobeOpenAICompatibleFactory({
+  baseURL: 'https://api.sensenova.cn/compatible-mode/v1',
+  chatCompletion: {
+    handlePayload: (payload: ChatStreamPayload) => {
+      const { frequency_penalty, temperature, top_p, ...rest } = payload;
+
+      return {
+        ...rest,
+        frequency_penalty: (frequency_penalty !== undefined && frequency_penalty > 0 && frequency_penalty <= 2) ? frequency_penalty : undefined,
+        temperature: (temperature !== undefined && temperature > 0 && temperature <= 2) ? temperature : undefined,
+        top_p: (top_p !== undefined && top_p > 0 && top_p < 1) ? top_p : undefined,
+      } as OpenAI.ChatCompletionCreateParamsStreaming;
+    }
+  },
+  debug: {
+    chatCompletion: () => process.env.DEBUG_SENSECORE_CHAT_COMPLETION === '1',
+  },
+  provider: ModelProvider.SenseCore,
+});
diff --git a/src/libs/agent-runtime/types/type.ts b/src/libs/agent-runtime/types/type.ts
index ae94893af796..e195b34ea12f 100644
--- a/src/libs/agent-runtime/types/type.ts
+++ b/src/libs/agent-runtime/types/type.ts
@@ -43,6 +43,7 @@ export enum ModelProvider {
   OpenRouter = 'openrouter',
   Perplexity = 'perplexity',
   Qwen = 'qwen',
+  SenseCore = 'sensecore',
   SiliconCloud = 'siliconcloud',
   Spark = 'spark',
   Stepfun = 'stepfun',
diff --git a/src/server/globalConfig/index.ts b/src/server/globalConfig/index.ts
index c53f7be3ce1b..6a8420444d22 100644
--- a/src/server/globalConfig/index.ts
+++ b/src/server/globalConfig/index.ts
@@ -15,6 +15,7 @@ import {
   OpenAIProviderCard,
   OpenRouterProviderCard,
   QwenProviderCard,
+  SenseCoreProviderCard,
   SiliconCloudProviderCard,
   TogetherAIProviderCard,
   ZeroOneProviderCard,
@@ -71,6 +72,9 @@ export const getServerGlobalConfig = () => {
     ENABLED_AI21,
     ENABLED_AI360,
 
+    ENABLED_SENSECORE,
+    SENSECORE_MODEL_LIST,
+
     ENABLED_SILICONCLOUD,
     SILICONCLOUD_MODEL_LIST,
 
@@ -218,6 +222,14 @@ export const getServerGlobalConfig = () => {
           modelString: QWEN_MODEL_LIST,
         }),
       },
+      sensecore: {
+        enabled: ENABLED_SENSECORE,
+        enabledModels: extractEnabledModels(SENSECORE_MODEL_LIST),
+        serverModelCards: transformToChatModelCards({
+          defaultChatModels: SenseCoreProviderCard.chatModels,
+          modelString: SENSECORE_MODEL_LIST,
+        }),
+      },
       siliconcloud: {
         enabled: ENABLED_SILICONCLOUD,
         enabledModels: extractEnabledModels(SILICONCLOUD_MODEL_LIST),
diff --git a/src/types/user/settings/keyVaults.ts b/src/types/user/settings/keyVaults.ts
index 3a0c6c4f0e4b..90f0a45117cd 100644
--- a/src/types/user/settings/keyVaults.ts
+++ b/src/types/user/settings/keyVaults.ts
@@ -40,6 +40,7 @@ export interface UserKeyVaults {
   password?: string;
   perplexity?: OpenAICompatibleKeyVault;
   qwen?: OpenAICompatibleKeyVault;
+  sensecore?: OpenAICompatibleKeyVault;
   siliconcloud?: OpenAICompatibleKeyVault;
   spark?: OpenAICompatibleKeyVault;
   stepfun?: OpenAICompatibleKeyVault;
