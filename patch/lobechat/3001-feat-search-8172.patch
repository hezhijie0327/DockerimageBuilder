diff --git a/docs/self-hosting/advanced/online-search.mdx b/docs/self-hosting/advanced/online-search.mdx
index 0ae3e4239ba62..a40ae2c368ad9 100644
--- a/docs/self-hosting/advanced/online-search.mdx
+++ b/docs/self-hosting/advanced/online-search.mdx
@@ -14,16 +14,134 @@ tags:
 
 # Configuring Online Search Functionality
 
-LobeChat supports configuring online search functionality for AI, allowing it to access the latest web information and provide more accurate and timely responses. The online search feature is based on the [SearXNG](https://github.com/searxng/searxng) search engine, which is a privacy-respecting metasearch engine that aggregates results from multiple search engines.
+LobeChat supports configuring **web search functionality** for AI, enabling it to retrieve real-time information from the internet to provide more accurate and up-to-date responses. Web search supports multiple search engine providers, including [SearXNG](https://github.com/searxng/searxng), [Search1API](https://www.search1api.com), [Google](https://programmablesearchengine.google.com), and [Brave](https://brave.com/search/api), among others.
 
-<Callout type={'info'}>
-  SearXNG is an open-source metasearch engine that can be self-hosted or accessed via public
-  instances. By configuring SearXNG, LobeChat enables AI to retrieve the latest internet
-  information, allowing it to answer time-sensitive questions and provide up-to-date news.
+<Callout type="info">
+  Web search allows AI to access time-sensitive content, such as the latest news, technology trends, or product information. You can deploy the open-source SearXNG yourself, or choose to integrate mainstream search services like Search1API, Google, Brave, etc., combining them freely based on your use case.
 </Callout>
 
+By setting the search service environment variable `SEARCH_PROVIDERS` and the corresponding API Keys, LobeChat will query multiple sources and return the results. You can also configure crawler service environment variables such as `CRAWLER_IMPLS` (e.g., `browserless`, `firecrawl`, `tavily`, etc.) to extract webpage content, enhancing the capability of search + reading.
+
 # Core Environment Variables
 
+## `CRAWLER_IMPLS`
+
+Configure available web crawlers for structured extraction of webpage content.
+
+```env
+CRAWLER_IMPLS="native,search1api"
+```
+
+Supported crawler types are listed below:
+
+| Value         | Description                                                                                                         | Environment Variable       |
+| ------------- | ------------------------------------------------------------------------------------------------------------------- | -------------------------- |
+| `browserless` | Headless browser crawler based on [Browserless](https://www.browserless.io/), suitable for rendering complex pages. | `BROWSERLESS_TOKEN`        |
+| `exa`         | Crawler capabilities provided by [Exa](https://exa.ai/), API required.                                              | `EXA_API_KEY`              |
+| `firecrawl`   | [Firecrawl](https://firecrawl.dev/) headless browser API, ideal for modern websites.                                | `FIRECRAWL_API_KEY`        |
+| `jina`        | Crawler service from [Jina AI](https://jina.ai/), supports fast content summarization.                              | `JINA_READER_API_KEY`      |
+| `native`      | Built-in general-purpose crawler for standard web structures.                                                       |                            |
+| `search1api`  | Page crawling capabilities from [Search1API](https://www.search1api.com), great for structured content extraction.  | `SEARCH1API_CRAWL_API_KEY` |
+| `tavily`      | Web scraping and summarization API from [Tavily](https://www.tavily.com/).                                          | `TAVILY_API_KEY`           |
+
+> ğŸ’¡ Setting multiple crawlers increases success rate; the system will try different ones based on priority.
+
+---
+
+## `SEARCH_PROVIDERS`
+
+Configure which search engine providers to use for web search.
+
+```env
+SEARCH_PROVIDERS="searxng"
+```
+
+Supported search engines include:
+
+| Value        | Description                                                                              | Environment Variable                        |
+| ------------ | ---------------------------------------------------------------------------------------- | ------------------------------------------- |
+| `anspire`    | Search service provided by [Anspire](https://anspire.ai/).                               | `ANSPIRE_API_KEY`                           |
+| `bocha`      | Search service from [Bocha](https://open.bochaai.com/).                                  | `BOCHA_API_KEY`                             |
+| `brave`      | [Brave](https://search.brave.com/help/api), a privacy-friendly search source.            | `BRAVE_API_KEY`                             |
+| `exa`        | [Exa](https://exa.ai/), a search API designed for AI.                                    | `EXA_API_KEY`                               |
+| `firecrawl`  | Search capabilities via [Firecrawl](https://firecrawl.dev/).                             | `FIRECRAWL_API_KEY`                         |
+| `google`     | Uses [Google Programmable Search Engine](https://programmablesearchengine.google.com/).  | `GOOGLE_PSE_API_KEY` `GOOGLE_PSE_ENGINE_ID` |
+| `jina`       | Semantic search provided by [Jina AI](https://jina.ai/).                                 | `JINA_READER_API_KEY`                       |
+| `kagi`       | Premium search API by [Kagi](https://kagi.com/), requires a subscription key.            | `KAGI_API_KEY`                              |
+| `search1api` | Aggregated search capabilities from [Search1API](https://www.search1api.com).            | `SEARCH1API_CRAWL_API_KEY`                  |
+| `searxng`    | Use a self-hosted or public [SearXNG](https://searx.space/) instance.                    | `SEARXNG_URL`                               |
+| `tavily`     | [Tavily](https://www.tavily.com/), offers fast web summaries and answers.                | `TAVILY_API_KEY`                            |
+
+> âš ï¸ Some search providers require you to apply for an API Key and configure it in your `.env` file.
+
+---
+
+## `BROWSERLESS_URL`
+
+Specifies the API endpoint for [Browserless](https://www.browserless.io/), used for web crawling tasks. Browserless is a browser automation platform based on Headless Chrome, ideal for rendering dynamic pages.
+
+```env
+BROWSERLESS_URL=https://chrome.browserless.io
+```
+
+> ğŸ“Œ Usually used together with `CRAWLER_IMPLS=browserless`.
+
+---
+
+## `GOOGLE_PSE_ENGINE_ID`
+
+Configure the Search Engine ID for Google Programmable Search Engine (Google PSE), used to restrict the search scope. Must be used alongside `GOOGLE_PSE_API_KEY`.
+
+```env
+GOOGLE_PSE_ENGINE_ID=your-google-cx-id
+```
+
+> ğŸ”‘ How to get it: Visit [programmablesearchengine.google.com](https://programmablesearchengine.google.com/), create a search engine, and obtain the `cx` parameter.
+
+---
+
+## `FIRECRAWL_URL`
+
+Sets the access URL for the [Firecrawl](https://firecrawl.dev/) API, used for web content scraping. Default value:
+
+```env
+FIRECRAWL_URL=https://api.firecrawl.dev/v1
+```
+
+> âš™ï¸ Usually does not need to be changed unless youâ€™re using a self-hosted version or a proxy service.
+
+---
+
+## `TAVILY_SEARCH_DEPTH`
+
+Configure the result depth for [Tavily](https://www.tavily.com/) searches.
+
+```env
+TAVILY_SEARCH_DEPTH=basic
+```
+
+Supported values:
+
+* `basic`: Fast search, returns brief results;
+* `advanced`: Deep search, returns more context and web page details.
+
+---
+
+## `TAVILY_EXTRACT_DEPTH`
+
+Configure how deeply Tavily extracts content from web pages.
+
+```env
+TAVILY_EXTRACT_DEPTH=basic
+```
+
+Supported values:
+
+* `basic`: Extracts basic info like title and content summary;
+* `advanced`: Extracts structured data, lists, charts, and more from web pages.
+
+---
+
 ## `SEARXNG_URL`
 
 The URL of the SearXNG instance, which is a necessary configuration to enable the online search functionality. For example:
diff --git a/docs/self-hosting/advanced/online-search.zh-CN.mdx b/docs/self-hosting/advanced/online-search.zh-CN.mdx
index bcdf065e94045..7914b05d94e3a 100644
--- a/docs/self-hosting/advanced/online-search.zh-CN.mdx
+++ b/docs/self-hosting/advanced/online-search.zh-CN.mdx
@@ -10,15 +10,134 @@ tags:
 
 # é…ç½®è”ç½‘æœç´¢åŠŸèƒ½
 
-LobeChat æ”¯æŒä¸º AI é…ç½®è”ç½‘æœç´¢åŠŸèƒ½ï¼Œè¿™ä½¿å¾— AI èƒ½å¤Ÿè·å–æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®ã€æ›´åŠæ—¶çš„å›ç­”ã€‚è”ç½‘æœç´¢åŠŸèƒ½åŸºäº [SearXNG](https://github.com/searxng/searxng) æœç´¢å¼•æ“ï¼Œå®ƒæ˜¯ä¸€ä¸ªå°Šé‡éšç§çš„å…ƒæœç´¢å¼•æ“ï¼Œå¯ä»¥èšåˆå¤šä¸ªæœç´¢å¼•æ“çš„ç»“æœã€‚
+LobeChat æ”¯æŒä¸º AI é…ç½®**è”ç½‘æœç´¢åŠŸèƒ½**ï¼Œä½¿å…¶èƒ½å¤Ÿå®æ—¶è·å–äº’è”ç½‘ä¿¡æ¯ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®ã€æœ€æ–°çš„å›ç­”ã€‚è”ç½‘æœç´¢æ”¯æŒå¤šä¸ªæœç´¢å¼•æ“æä¾›å•†ï¼ŒåŒ…æ‹¬ [SearXNG](https://github.com/searxng/searxng)ã€[Search1API](https://www.search1api.com)ã€[Google](https://programmablesearchengine.google.com)ã€[Brave](https://brave.com/search/api) ç­‰ã€‚
 
-<Callout type={'info'}>
-  SearXNG æ˜¯ä¸€ä¸ªå¼€æºçš„å…ƒæœç´¢å¼•æ“ï¼Œå¯ä»¥è‡ªè¡Œéƒ¨ç½²ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¬å…±å®ä¾‹ã€‚é€šè¿‡é…ç½® SearXNGï¼ŒLobeChat
-  å¯ä»¥è®© AI è·å–æœ€æ–°çš„äº’è”ç½‘ä¿¡æ¯ï¼Œä»è€Œå›ç­”æ—¶æ•ˆæ€§é—®é¢˜ã€æä¾›æœ€æ–°èµ„è®¯ã€‚
+<Callout type="info">
+  è”ç½‘æœç´¢å¯ä»¥è®© AI è·å–æ—¶æ•ˆæ€§å†…å®¹ï¼Œå¦‚æœ€æ–°æ–°é—»ã€æŠ€æœ¯åŠ¨æ€æˆ–äº§å“ä¿¡æ¯ã€‚ä½ å¯ä»¥ä½¿ç”¨å¼€æºçš„ SearXNG è‡ªè¡Œéƒ¨ç½²ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©é›†æˆä¸»æµæœç´¢å¼•æ“æœåŠ¡ï¼Œå¦‚ Search1APIã€Googleã€Brave ç­‰ï¼Œæ ¹æ®ä½ çš„ä½¿ç”¨åœºæ™¯è‡ªç”±ç»„åˆã€‚
 </Callout>
 
+é€šè¿‡è®¾ç½®æœç´¢æœåŠ¡ç¯å¢ƒå˜é‡ `SEARCH_PROVIDERS` å’Œå¯¹åº”çš„ API Keyï¼ŒLobeChat å°†åœ¨å¤šä¸ªæœç´¢æºä¸­æŸ¥è¯¢å¹¶è¿”å›ç»“æœã€‚ä½ è¿˜å¯ä»¥æ­é…é…ç½®çˆ¬è™«æœåŠ¡ç¯å¢ƒå˜é‡ `CRAWLER_IMPLS`ï¼ˆå¦‚ `browserless`ã€`firecrawl`ã€`tavily` ç­‰ï¼‰ä»¥æå–ç½‘é¡µå†…å®¹ï¼Œå®ç°æœç´¢+é˜…è¯»çš„å¢å¼ºèƒ½åŠ›ã€‚
+
 # æ ¸å¿ƒç¯å¢ƒå˜é‡
 
+## `CRAWLER_IMPLS`
+
+é…ç½®å¯ç”¨çš„ç½‘é¡µçˆ¬è™«ï¼Œç”¨äºå¯¹ç½‘é¡µè¿›è¡Œç»“æ„åŒ–å†…å®¹æå–ã€‚
+
+```env
+CRAWLER_IMPLS="native,search1api"
+```
+
+æ”¯æŒçš„çˆ¬è™«ç±»å‹å¦‚ä¸‹ï¼š
+
+| å€¼            | è¯´æ˜                                                                                   | ç¯å¢ƒå˜é‡                   |
+| ------------- | -------------------------------------------------------------------------------------- | -------------------------- |
+| `browserless` | åŸºäº [Browserless](https://www.browserless.io/) çš„æ— å¤´æµè§ˆå™¨çˆ¬è™«ï¼Œé€‚åˆæ¸²æŸ“å¤æ‚é¡µé¢ã€‚   | `BROWSERLESS_TOKEN`        |
+| `exa`         | ä½¿ç”¨ [Exa](https://exa.ai/) æä¾›çš„çˆ¬è™«èƒ½åŠ›ï¼Œéœ€ç”³è¯· APIã€‚                               | `EXA_API_KEY`              |
+| `firecrawl`   | [Firecrawl](https://firecrawl.dev/) æ— å¤´æµè§ˆå™¨ APIï¼Œé€‚åˆç°ä»£ç½‘ç«™æŠ“å–ã€‚                 | `FIRECRAWL_API_KEY`        |
+| `jina`        | ä½¿ç”¨ [Jina AI](https://jina.ai/) çš„çˆ¬è™«æœåŠ¡ï¼Œæ”¯æŒå¿«é€Ÿæå–æ‘˜è¦ä¿¡æ¯ã€‚                    | `JINA_READER_API_KEY`      |
+| `native`      | å†…ç½®é€šç”¨çˆ¬è™«ï¼Œé€‚ç”¨äºæ ‡å‡†ç½‘é¡µç»“æ„ã€‚                                                     |                            |
+| `search1api`  | åˆ©ç”¨ [Search1API](https://www.search1api.com) æä¾›çš„é¡µé¢æŠ“å–èƒ½åŠ›ï¼Œé€‚åˆç»“æ„åŒ–å†…å®¹æå–ã€‚ | `SEARCH1API_CRAWL_API_KEY` |
+| `tavily`      | ä½¿ç”¨ [Tavily](https://www.tavily.com/) çš„ç½‘é¡µæŠ“å–ä¸æ‘˜è¦ APIã€‚                          | `TAVILY_API_KEY`           |
+
+> ğŸ’¡ è®¾ç½®å¤šä¸ªçˆ¬è™«å¯æå‡æˆåŠŸç‡ï¼Œç³»ç»Ÿå°†æ ¹æ®ä¼˜å…ˆçº§å°è¯•ä¸åŒçˆ¬è™«ã€‚
+
+---
+
+## `SEARCH_PROVIDERS`
+
+é…ç½®è”ç½‘æœç´¢ä½¿ç”¨çš„æœç´¢å¼•æ“æä¾›å•†ã€‚
+
+```env
+SEARCH_PROVIDERS="searxng"
+```
+
+æ”¯æŒçš„æœç´¢å¼•æ“å¦‚ä¸‹ï¼š
+
+| å€¼           | è¯´æ˜                                                                                     | ç¯å¢ƒå˜é‡                                    |
+| ------------ | ---------------------------------------------------------------------------------------- | ------------------------------------------- |
+| `anspire`    | åŸºäº [Anspireï¼ˆå®‰æ€æ´¾ï¼‰](https://anspire.ai/) æä¾›çš„æœç´¢æœåŠ¡ã€‚                           | `ANSPIRE_API_KEY`                           |
+| `bocha`      | åŸºäº [Bochaï¼ˆåšæŸ¥ï¼‰](https://open.bochaai.com/) æä¾›çš„æœç´¢æœåŠ¡ã€‚                         | `BOCHA_API_KEY`                             |
+| `brave`      | [Brave](https://search.brave.com/help/api)ï¼Œéšç§å‹å¥½çš„æœç´¢æºã€‚                           | `BRAVE_API_KEY`                             |
+| `exa`        | [Exa](https://exa.ai/)ï¼Œé¢å‘ AI çš„æœç´¢ APIã€‚                                             | `EXA_API_KEY`                               |
+| `firecrawl`  | æ”¯æŒ [Firecrawl](https://firecrawl.dev/) æä¾›çš„æœç´¢æœåŠ¡ã€‚                                | `FIRECRAWL_API_KEY`                         |
+| `google`     | ä½¿ç”¨ [Google Programmable Search Engine](https://programmablesearchengine.google.com/)ã€‚ | `GOOGLE_PSE_API_KEY` `GOOGLE_PSE_ENGINE_ID` |
+| `jina`       | ä½¿ç”¨ [Jina AI](https://jina.ai/) æä¾›çš„è¯­ä¹‰æœç´¢æœåŠ¡ã€‚                                    | `JINA_READER_API_KEY`                       |
+| `kagi`       | [Kagi](https://kagi.com/) æä¾›çš„é«˜çº§æœç´¢ APIï¼Œéœ€è®¢é˜… Keyã€‚                               | `KAGI_API_KEY`                              |
+| `search1api` | ä½¿ç”¨ [Search1API](https://www.search1api.com) èšåˆæœç´¢èƒ½åŠ›ã€‚                             | `SEARCH1API_CRAWL_API_KEY`                  |
+| `searxng`    | ä½¿ç”¨è‡ªæ‰˜ç®¡æˆ–å…¬å…± [SearXNG](https://searx.space/) å®ä¾‹ã€‚                                  | `SEARXNG_URL`                               |
+| `tavily`     | [Tavily](https://www.tavily.com/)ï¼Œå¿«é€Ÿç½‘é¡µæ‘˜è¦ä¸ç­”æ¡ˆè¿”å›ã€‚                              | `TAVILY_API_KEY`                            |
+
+> âš ï¸ æŸäº›æœç´¢æä¾›å•†éœ€è¦å•ç‹¬ç”³è¯· API Keyï¼Œå¹¶åœ¨ `.env` ä¸­é…ç½®ç›¸å…³å‡­è¯ã€‚
+
+---
+
+## `BROWSERLESS_URL`
+
+æŒ‡å®š [Browserless](https://www.browserless.io/) æœåŠ¡çš„ API åœ°å€ï¼Œç”¨äºæ‰§è¡Œç½‘é¡µçˆ¬å–ä»»åŠ¡ã€‚Browserless æ˜¯ä¸€ä¸ªåŸºäºæ— å¤´æµè§ˆå™¨ï¼ˆHeadless Chromeï¼‰çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å¹³å°ï¼Œé€‚åˆå¤„ç†éœ€è¦æ¸²æŸ“çš„åŠ¨æ€é¡µé¢ã€‚
+
+```env
+BROWSERLESS_URL=https://chrome.browserless.io
+```
+
+> ğŸ“Œ é€šå¸¸éœ€è¦æ­é… `CRAWLER_IMPLS=browserless` å¯ç”¨ã€‚
+
+---
+
+## `GOOGLE_PSE_ENGINE_ID`
+
+é…ç½® Google Programmable Search Engineï¼ˆGoogle PSEï¼‰çš„æœç´¢å¼•æ“ IDï¼Œç”¨äºé™å®šæœç´¢èŒƒå›´ã€‚éœ€é…åˆ `GOOGLE_PSE_API_KEY` ä¸€èµ·ä½¿ç”¨ã€‚
+
+```env
+GOOGLE_PSE_ENGINE_ID=your-google-cx-id
+```
+
+> ğŸ”‘ è·å–æ–¹å¼ï¼šè®¿é—® [programmablesearchengine.google.com](https://programmablesearchengine.google.com/)ï¼Œåˆ›å»ºæœç´¢å¼•æ“åè·å– `cx` å‚æ•°å€¼ã€‚
+
+---
+
+## `FIRECRAWL_URL`
+
+è®¾ç½® [Firecrawl](https://firecrawl.dev/) API çš„è®¿é—®åœ°å€ã€‚ç”¨äºç½‘é¡µå†…å®¹æŠ“å–ï¼Œé»˜è®¤å€¼å¦‚ä¸‹ï¼š
+
+```env
+FIRECRAWL_URL=https://api.firecrawl.dev/v1
+```
+
+> âš™ï¸ ä¸€èˆ¬æ— éœ€ä¿®æ”¹ï¼Œé™¤éä½ ä½¿ç”¨çš„æ˜¯è‡ªæ‰˜ç®¡ç‰ˆæœ¬æˆ–ä»£ç†æœåŠ¡ã€‚
+
+---
+
+## `TAVILY_SEARCH_DEPTH`
+
+é…ç½® [Tavily](https://www.tavily.com/) æœç´¢çš„ç»“æœæ·±åº¦ã€‚
+
+```env
+TAVILY_SEARCH_DEPTH=basic
+```
+
+æ”¯æŒçš„å€¼ï¼š
+
+* `basic`: å¿«é€Ÿæœç´¢ï¼Œè¿”å›ç®€è¦ç»“æœï¼›
+* `advanced`: æ·±åº¦æœç´¢ï¼Œè¿”å›æ›´å¤šä¸Šä¸‹æ–‡å’Œç½‘é¡µä¿¡æ¯ã€‚
+
+---
+
+## `TAVILY_EXTRACT_DEPTH`
+
+é…ç½® Tavily åœ¨æŠ“å–ç½‘é¡µå†…å®¹æ—¶çš„æå–æ·±åº¦ã€‚
+
+```env
+TAVILY_EXTRACT_DEPTH=basic
+```
+
+æ”¯æŒçš„å€¼ï¼š
+
+* `basic`: æå–æ ‡é¢˜ã€æ­£æ–‡æ‘˜è¦ç­‰åŸºç¡€ä¿¡æ¯ï¼›
+* `advanced`: æå–ç½‘é¡µçš„ç»“æ„åŒ–ä¿¡æ¯ã€åˆ—è¡¨ã€å›¾è¡¨ç­‰æ›´å¤šå†…å®¹ã€‚
+
+---
+
 ## `SEARXNG_URL`
 
 SearXNG å®ä¾‹çš„ URL åœ°å€ï¼Œè¿™æ˜¯å¯ç”¨è”ç½‘æœç´¢åŠŸèƒ½çš„å¿…è¦é…ç½®ã€‚ä¾‹å¦‚ï¼š
diff --git a/src/server/services/search/impls/anspire/index.ts b/src/server/services/search/impls/anspire/index.ts
new file mode 100644
index 0000000000000..7745ee0973112
--- /dev/null
+++ b/src/server/services/search/impls/anspire/index.ts
@@ -0,0 +1,132 @@
+import { TRPCError } from '@trpc/server';
+import debug from 'debug';
+import urlJoin from 'url-join';
+
+import { SearchParams, UniformSearchResponse, UniformSearchResult } from '@/types/tool/search';
+
+import { SearchServiceImpl } from '../type';
+import { AnspireSearchParameters, AnspireResponse } from './type';
+
+const log = debug('lobe-search:Anspire');
+
+/**
+ * Anspire implementation of the search service
+ * Primarily used for web crawling
+ */
+export class AnspireImpl implements SearchServiceImpl {
+  private get apiKey(): string | undefined {
+    return process.env.ANSPIRE_API_KEY;
+  }
+
+  private get baseUrl(): string {
+    // Assuming the base URL is consistent with the crawl endpoint
+    return 'https://plugin.anspire.cn/api';
+  }
+
+  async query(query: string, params: SearchParams = {}): Promise<UniformSearchResponse> {
+    log('Starting Anspire query with query: "%s", params: %o', query, params);
+    const endpoint = urlJoin(this.baseUrl, '/ntsearch/search');
+
+    const defaultQueryParams: AnspireSearchParameters = {
+      mode: 0,
+      query,
+      top_k: 20,
+    };
+
+    let body: AnspireSearchParameters = {
+      ...defaultQueryParams,
+      ...(params?.searchTimeRange && params.searchTimeRange !== 'anytime'
+        ? (() => {
+            const now = Date.now();
+            const days = { day: 1, month: 30, week: 7, year: 365 }[params.searchTimeRange!];
+
+            if (days === undefined) return {};
+
+            return {
+              FromTime: new Date(now - days * 86_400 * 1000).toISOString().slice(0, 19).replace('T', ' '),
+              ToTime: new Date(now).toISOString().slice(0, 19).replace('T', ' '),
+            };
+          })()
+        : {}),
+    };
+
+    log('Constructed request body: %o', body);
+
+    const searchParams = new URLSearchParams();
+    for (const [key, value] of Object.entries(body)) {
+      searchParams.append(key, String(value));
+    }
+
+    let response: Response;
+    const startAt = Date.now();
+    let costTime = 0;
+    try {
+      log('Sending request to endpoint: %s', endpoint);
+      response = await fetch(`${endpoint}?${searchParams.toString()}`, {
+        headers: {
+          'Accept': '*/*',
+          'Authorization': this.apiKey ? `Bearer ${this.apiKey}` : '',
+          'Connection': 'keep-alive ',
+          'Content-Type': 'application/json',
+        },
+        method: 'GET',
+      });
+      log('Received response with status: %d', response.status);
+      costTime = Date.now() - startAt;
+    } catch (error) {
+      log.extend('error')('Anspire fetch error: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'SERVICE_UNAVAILABLE',
+        message: 'Failed to connect to Anspire.',
+      });
+    }
+
+    if (!response.ok) {
+      const errorBody = await response.text();
+      log.extend('error')(
+        `Anspire request failed with status ${response.status}: %s`,
+        errorBody.length > 200 ? `${errorBody.slice(0, 200)}...` : errorBody,
+      );
+      throw new TRPCError({
+        cause: errorBody,
+        code: 'SERVICE_UNAVAILABLE',
+        message: `Anspire request failed: ${response.statusText}`,
+      });
+    }
+
+    try {
+      const anspireResponse = (await response.json()) as AnspireResponse;
+
+      log('Parsed Anspire response: %o', anspireResponse);
+
+      const mappedResults = (anspireResponse.results || []).map(
+        (result): UniformSearchResult => ({
+          category: 'general', // Default category
+          content: result.content || '', // Prioritize content
+          engines: ['anspire'], // Use 'anspire' as the engine name
+          parsedUrl: result.url ? new URL(result.url).hostname : '', // Basic URL parsing
+          score: result.score || 0, // Default score to 0 if undefined
+          title: result.title || '',
+          url: result.url,
+        }),
+      );
+
+      log('Mapped %d results to SearchResult format', mappedResults.length);
+
+      return {
+        costTime,
+        query: query,
+        resultNumbers: mappedResults.length,
+        results: mappedResults,
+      };
+    } catch (error) {
+      log.extend('error')('Error parsing Anspire response: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'INTERNAL_SERVER_ERROR',
+        message: 'Failed to parse Anspire response.',
+      });
+    }
+  }
+}
diff --git a/src/server/services/search/impls/anspire/type.ts b/src/server/services/search/impls/anspire/type.ts
new file mode 100644
index 0000000000000..6b0a415395dc3
--- /dev/null
+++ b/src/server/services/search/impls/anspire/type.ts
@@ -0,0 +1,21 @@
+export interface AnspireSearchParameters {
+  FromTime?: string;
+  Insite?: string;
+  ToTime?: string;
+  mode?: number;
+  query: string;
+  top_k?: number;
+}
+
+interface AnspireResults {
+  content?: string;
+  score?: number;
+  title: string;
+  url: string;
+}
+
+export interface AnspireResponse {
+  Uuid?: string;
+  query?: string;
+  results?: AnspireResults[];
+}
diff --git a/src/server/services/search/impls/brave/index.ts b/src/server/services/search/impls/brave/index.ts
new file mode 100644
index 0000000000000..bb3fc590b133e
--- /dev/null
+++ b/src/server/services/search/impls/brave/index.ts
@@ -0,0 +1,129 @@
+import { TRPCError } from '@trpc/server';
+import debug from 'debug';
+import urlJoin from 'url-join';
+
+import { SearchParams, UniformSearchResponse, UniformSearchResult } from '@/types/tool/search';
+
+import { SearchServiceImpl } from '../type';
+import { BraveSearchParameters, BraveResponse } from './type';
+
+const log = debug('lobe-search:Brave');
+
+const timeRangeMapping = {
+  day: 'pd',
+  month: 'pm',
+  week: 'pw',
+  year: 'py',
+};
+
+/**
+ * Brave implementation of the search service
+ * Primarily used for web crawling
+ */
+export class BraveImpl implements SearchServiceImpl {
+  private get apiKey(): string | undefined {
+    return process.env.BRAVE_API_KEY;
+  }
+
+  private get baseUrl(): string {
+    // Assuming the base URL is consistent with the crawl endpoint
+    return 'https://api.search.brave.com/res/v1';
+  }
+
+  async query(query: string, params: SearchParams = {}): Promise<UniformSearchResponse> {
+    log('Starting Brave query with query: "%s", params: %o', query, params);
+    const endpoint = urlJoin(this.baseUrl, '/web/search');
+
+    const defaultQueryParams: BraveSearchParameters = {
+      count: 15,
+      q: query,
+      result_filter: 'web',
+    };
+
+    let body: BraveSearchParameters = {
+      ...defaultQueryParams,
+      freshness:
+        params?.searchTimeRange && params.searchTimeRange !== 'anytime'
+          ? timeRangeMapping[params.searchTimeRange as keyof typeof timeRangeMapping] ?? undefined
+          : undefined,
+    };
+
+    log('Constructed request body: %o', body);
+
+    const searchParams = new URLSearchParams();
+    for (const [key, value] of Object.entries(body)) {
+      searchParams.append(key, String(value));
+    }
+
+    let response: Response;
+    const startAt = Date.now();
+    let costTime = 0;
+    try {
+      log('Sending request to endpoint: %s', endpoint);
+      response = await fetch(`${endpoint}?${searchParams.toString()}`, {
+        headers: {
+          'Accept': 'application/json',
+          'Accept-Encoding': 'gzip',
+          'X-Subscription-Token': this.apiKey ? this.apiKey : '',
+        },
+        method: 'GET',
+      });
+      log('Received response with status: %d', response.status);
+      costTime = Date.now() - startAt;
+    } catch (error) {
+      log.extend('error')('Brave fetch error: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'SERVICE_UNAVAILABLE',
+        message: 'Failed to connect to Brave.',
+      });
+    }
+
+    if (!response.ok) {
+      const errorBody = await response.text();
+      log.extend('error')(
+        `Brave request failed with status ${response.status}: %s`,
+        errorBody.length > 200 ? `${errorBody.slice(0, 200)}...` : errorBody,
+      );
+      throw new TRPCError({
+        cause: errorBody,
+        code: 'SERVICE_UNAVAILABLE',
+        message: `Brave request failed: ${response.statusText}`,
+      });
+    }
+
+    try {
+      const braveResponse = (await response.json()) as BraveResponse;
+
+      log('Parsed Brave response: %o', braveResponse);
+
+      const mappedResults = (braveResponse.web.results || []).map(
+        (result): UniformSearchResult => ({
+          category: 'general', // Default category
+          content: result.description || '', // Prioritize content
+          engines: ['brave'], // Use 'brave' as the engine name
+          parsedUrl: result.url ? new URL(result.url).hostname : '', // Basic URL parsing
+          score: 1, // Default score to 1
+          title: result.title || '',
+          url: result.url,
+        }),
+      );
+
+      log('Mapped %d results to SearchResult format', mappedResults.length);
+
+      return {
+        costTime,
+        query: query,
+        resultNumbers: mappedResults.length,
+        results: mappedResults,
+      };
+    } catch (error) {
+      log.extend('error')('Error parsing Brave response: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'INTERNAL_SERVER_ERROR',
+        message: 'Failed to parse Brave response.',
+      });
+    }
+  }
+}
diff --git a/src/server/services/search/impls/brave/type.ts b/src/server/services/search/impls/brave/type.ts
new file mode 100644
index 0000000000000..74a8251d1faf2
--- /dev/null
+++ b/src/server/services/search/impls/brave/type.ts
@@ -0,0 +1,58 @@
+export interface BraveSearchParameters {
+  count?: number;
+  country?: string;
+  enable_rich_callback?: boolean;
+  extra_snippets?: boolean;
+  freshness?: string;
+  goggles?: string[];
+  goggles_id?: string;
+  offset?: number;
+  q: string;
+  result_filter?: string;
+  safesearch?: string;
+  search_lang?: string;
+  spellcheck?: boolean;
+  summary?: boolean;
+  text_decorations?: boolean;
+  ui_lang?: string;
+  units?: string;
+}
+
+interface BraveResults {
+  age?: string;
+  description: string;
+  family_friendly?: boolean;
+  is_live?: boolean;
+  is_source_both?: boolean;
+  is_source_local?: boolean;
+  language?: string;
+  meta_url?: any;
+  page_age?: string;
+  profile?: any;
+  subtype?: string;
+  thumbnail?: any;
+  title: string;
+  type: string;
+  url: string;
+  video?: any;
+}
+
+interface BraveVideos {
+  mutated_by_goggles?: boolean;
+  results: BraveResults[];
+  type: string;
+}
+
+interface BraveWeb {
+  family_friendly?: boolean;
+  results: BraveResults[];
+  type: string;
+}
+
+export interface BraveResponse {
+  mixed: any;
+  query?: any;
+  type: string;
+  videos?: BraveVideos;
+  web: BraveWeb;
+}
diff --git a/src/server/services/search/impls/google/index.ts b/src/server/services/search/impls/google/index.ts
new file mode 100644
index 0000000000000..8ba9cb8638bfe
--- /dev/null
+++ b/src/server/services/search/impls/google/index.ts
@@ -0,0 +1,129 @@
+import { TRPCError } from '@trpc/server';
+import debug from 'debug';
+import urlJoin from 'url-join';
+
+import { SearchParams, UniformSearchResponse, UniformSearchResult } from '@/types/tool/search';
+
+import { SearchServiceImpl } from '../type';
+import { GoogleSearchParameters, GoogleResponse } from './type';
+
+const log = debug('lobe-search:Google');
+
+const timeRangeMapping = {
+  day: 'd1',
+  month: 'm1',
+  week: 'w1',
+  year: 'y1',
+};
+
+/**
+ * Google implementation of the search service
+ * Primarily used for web crawling
+ */
+export class GoogleImpl implements SearchServiceImpl {
+  private get apiKey(): string | undefined {
+    return process.env.GOOGLE_PSE_API_KEY;
+  }
+
+  private get engineId(): string | undefined {
+    return process.env.GOOGLE_PSE_ENGINE_ID;
+  }
+
+  private get baseUrl(): string {
+    // Assuming the base URL is consistent with the crawl endpoint
+    return 'https://www.googleapis.com';
+  }
+
+  async query(query: string, params: SearchParams = {}): Promise<UniformSearchResponse> {
+    log('Starting Google query with query: "%s", params: %o', query, params);
+    const endpoint = urlJoin(this.baseUrl, '/customsearch/v1');
+
+    const defaultQueryParams: GoogleSearchParameters = {
+      cx: this.engineId || '',
+      key: this.apiKey || '',
+      num: 10,
+      q: query,
+    };
+
+    let body: GoogleSearchParameters = {
+      ...defaultQueryParams,
+      dateRestrict:
+        params?.searchTimeRange && params.searchTimeRange !== 'anytime'
+          ? timeRangeMapping[params.searchTimeRange as keyof typeof timeRangeMapping] ?? undefined
+          : undefined,
+    };
+
+    log('Constructed request body: %o', body);
+
+    const searchParams = new URLSearchParams();
+    for (const [key, value] of Object.entries(body)) {
+      searchParams.append(key, String(value));
+    }
+
+    let response: Response;
+    const startAt = Date.now();
+    let costTime = 0;
+    try {
+      log('Sending request to endpoint: %s', endpoint);
+      response = await fetch(`${endpoint}?${searchParams.toString()}`, {
+        method: 'GET',
+      });
+      log('Received response with status: %d', response.status);
+      costTime = Date.now() - startAt;
+    } catch (error) {
+      log.extend('error')('Google fetch error: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'SERVICE_UNAVAILABLE',
+        message: 'Failed to connect to Google.',
+      });
+    }
+
+    if (!response.ok) {
+      const errorBody = await response.text();
+      log.extend('error')(
+        `Google request failed with status ${response.status}: %s`,
+        errorBody.length > 200 ? `${errorBody.slice(0, 200)}...` : errorBody,
+      );
+      throw new TRPCError({
+        cause: errorBody,
+        code: 'SERVICE_UNAVAILABLE',
+        message: `Google request failed: ${response.statusText}`,
+      });
+    }
+
+    try {
+      const googleResponse = (await response.json()) as GoogleResponse;
+
+      log('Parsed Google response: %o', googleResponse);
+
+      const mappedResults = (googleResponse.items || []).map(
+        (result): UniformSearchResult => ({
+          category: 'general', // Default category
+          content: result.snippet || '', // Prioritize content
+          engines: ['google'], // Use 'google' as the engine name
+          parsedUrl: result.link ? new URL(result.link).hostname : '', // Basic URL parsing
+          score: 1, // Default score to 1
+          title: result.title || '',
+          url: result.link,
+        }),
+      );
+
+      log('Mapped %d results to SearchResult format', mappedResults.length);
+
+      return {
+        costTime,
+        query: query,
+        resultNumbers: mappedResults.length,
+        results: mappedResults,
+      };
+    } catch (error) {
+      log.extend('error')('Error parsing Google response: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'INTERNAL_SERVER_ERROR',
+        message: 'Failed to parse Google response.',
+      });
+    }
+  }
+}
diff --git a/src/server/services/search/impls/google/type.ts b/src/server/services/search/impls/google/type.ts
new file mode 100644
index 0000000000000..4600d604b8e24
--- /dev/null
+++ b/src/server/services/search/impls/google/type.ts
@@ -0,0 +1,53 @@
+export interface GoogleSearchParameters {
+  c2coff?: number;
+  cx: string;
+  dateRestrict?: string;
+  exactTerms?: string;
+  excludeTerms?: string;
+  fileType?: string;
+  filter?: string;
+  gl?: string;
+  highRange?: string;
+  hl?: string;
+  hq?: string;
+  imgColorType?: string;
+  imgDominantColor?: string;
+  imgSize?: string;
+  imgType?: string;
+  key: string;
+  linkSite?: string;
+  lowRange?: string;
+  lr?: string;
+  num?: number;
+  orTerms?: string;
+  q: string;
+  rights?: string;
+  safe?: string;
+  searchType?: string;
+  siteSearch?: string;
+  siteSearchFilter?: string;
+  sort?: string;
+  start?: string;
+}
+
+interface GoogleItems {
+  displayLink?: string;
+  formattedUrl?: string;
+  htmlFormattedUrl?: string;
+  htmlSnippet?: string;
+  htmlTitle?: string;
+  kind?: string;
+  link: string;
+  pagemap?: any;
+  snippet: string;
+  title: string;
+}
+
+export interface GoogleResponse {
+  context?: any;
+  items: GoogleItems[];
+  kind?: string;
+  queries?: any;
+  searchInformation?: any;
+  url?: any;
+}
diff --git a/src/server/services/search/impls/index.ts b/src/server/services/search/impls/index.ts
index a71a38507d519..334083ca40aa6 100644
--- a/src/server/services/search/impls/index.ts
+++ b/src/server/services/search/impls/index.ts
@@ -1,7 +1,11 @@
+import { AnspireImpl } from './anspire';
 import { BochaImpl } from './bocha';
+import { BraveImpl } from './brave';
 import { ExaImpl } from './exa';
 import { FirecrawlImpl } from './firecrawl';
+import { GoogleImpl } from './google';
 import { JinaImpl } from './jina';
+import { KagiImpl } from './kagi';
 import { Search1APIImpl } from './search1api';
 import { SearXNGImpl } from './searxng';
 import { TavilyImpl } from './tavily';
@@ -12,10 +16,14 @@ import { SearchServiceImpl } from './type';
  * Available search service implementations
  */
 export enum SearchImplType {
+  Anspire = 'anspire',
   Bocha = 'bocha',
+  Brave = 'brave',
   Exa = 'exa',
   Firecrawl = 'firecrawl',
+  Google = 'google',
   Jina = 'jina',
+  Kagi = 'kagi',
   SearXNG = 'searxng',
   Search1API = 'search1api',
   Tavily = 'tavily',
@@ -28,10 +36,18 @@ export const createSearchServiceImpl = (
   type: SearchImplType = SearchImplType.SearXNG,
 ): SearchServiceImpl => {
   switch (type) {
+    case SearchImplType.Anspire: {
+      return new AnspireImpl();
+    }
+
     case SearchImplType.Bocha: {
       return new BochaImpl();
     }
 
+    case SearchImplType.Brave: {
+      return new BraveImpl();
+    }
+
     case SearchImplType.Exa: {
       return new ExaImpl();
     }
@@ -40,10 +56,18 @@ export const createSearchServiceImpl = (
       return new FirecrawlImpl();
     }
 
+    case SearchImplType.Google: {
+      return new GoogleImpl();
+    }
+
     case SearchImplType.Jina: {
       return new JinaImpl();
     }
 
+    case SearchImplType.Kagi: {
+      return new KagiImpl();
+    }
+
     case SearchImplType.SearXNG: {
       return new SearXNGImpl();
     }
diff --git a/src/server/services/search/impls/kagi/index.ts b/src/server/services/search/impls/kagi/index.ts
new file mode 100644
index 0000000000000..d41221cef409c
--- /dev/null
+++ b/src/server/services/search/impls/kagi/index.ts
@@ -0,0 +1,111 @@
+import { TRPCError } from '@trpc/server';
+import debug from 'debug';
+import urlJoin from 'url-join';
+
+import { SearchParams, UniformSearchResponse, UniformSearchResult } from '@/types/tool/search';
+
+import { SearchServiceImpl } from '../type';
+import { KagiSearchParameters, KagiResponse } from './type';
+
+const log = debug('lobe-search:Kagi');
+
+/**
+ * Kagi implementation of the search service
+ * Primarily used for web crawling
+ */
+export class KagiImpl implements SearchServiceImpl {
+  private get apiKey(): string | undefined {
+    return process.env.KAGI_API_KEY;
+  }
+
+  private get baseUrl(): string {
+    // Assuming the base URL is consistent with the crawl endpoint
+    return 'https://kagi.com/api/v0';
+  }
+
+  async query(query: string, params: SearchParams = {}): Promise<UniformSearchResponse> {
+    log('Starting Kagi query with query: "%s", params: %o', query, params);
+    const endpoint = urlJoin(this.baseUrl, '/search');
+
+    const body: KagiSearchParameters = {
+      limit: 15,
+      q: query,
+    };
+
+    log('Constructed request body: %o', body);
+
+    const searchParams = new URLSearchParams();
+    for (const [key, value] of Object.entries(body)) {
+      searchParams.append(key, String(value));
+    }
+
+    let response: Response;
+    const startAt = Date.now();
+    let costTime = 0;
+    try {
+      log('Sending request to endpoint: %s', endpoint);
+      response = await fetch(`${endpoint}?${searchParams.toString()}`, {
+        headers: {
+          'Authorization': this.apiKey ? `Bot ${this.apiKey}` : '',
+        },
+        method: 'GET',
+      });
+      log('Received response with status: %d', response.status);
+      costTime = Date.now() - startAt;
+    } catch (error) {
+      log.extend('error')('Kagi fetch error: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'SERVICE_UNAVAILABLE',
+        message: 'Failed to connect to Kagi.',
+      });
+    }
+
+    if (!response.ok) {
+      const errorBody = await response.text();
+      log.extend('error')(
+        `Kagi request failed with status ${response.status}: %s`,
+        errorBody.length > 200 ? `${errorBody.slice(0, 200)}...` : errorBody,
+      );
+      throw new TRPCError({
+        cause: errorBody,
+        code: 'SERVICE_UNAVAILABLE',
+        message: `Kagi request failed: ${response.statusText}`,
+      });
+    }
+
+    try {
+      const kagiResponse = (await response.json()) as KagiResponse;
+
+      log('Parsed Kagi response: %o', kagiResponse);
+
+      const mappedResults = (kagiResponse.data || []).map(
+        (result): UniformSearchResult => ({
+          category: 'general', // Default category
+          content: result.snippet || '', // Prioritize content
+          engines: ['kagi'], // Use 'kagi' as the engine name
+          parsedUrl: result.url ? new URL(result.url).hostname : '', // Basic URL parsing
+          score: 1, // Default score to 1
+          title: result.title || '',
+          url: result.url,
+        }),
+      );
+
+      log('Mapped %d results to SearchResult format', mappedResults.length);
+
+      return {
+        costTime,
+        query: query,
+        resultNumbers: mappedResults.length,
+        results: mappedResults,
+      };
+    } catch (error) {
+      log.extend('error')('Error parsing Kagi response: %o', error);
+      throw new TRPCError({
+        cause: error,
+        code: 'INTERNAL_SERVER_ERROR',
+        message: 'Failed to parse Kagi response.',
+      });
+    }
+  }
+}
diff --git a/src/server/services/search/impls/kagi/type.ts b/src/server/services/search/impls/kagi/type.ts
new file mode 100644
index 0000000000000..c6275b509f849
--- /dev/null
+++ b/src/server/services/search/impls/kagi/type.ts
@@ -0,0 +1,24 @@
+export interface KagiSearchParameters {
+  limit?: number;
+  q: string;
+}
+
+interface KagiThumbnail {
+  height?: number | null;
+  url: string;
+  width?: number | null;
+}
+
+interface KagiData {
+  published?: number;
+  snippet?: string;
+  t: number;
+  thumbnail?: KagiThumbnail;
+  title: string;
+  url: string;
+}
+
+export interface KagiResponse {
+  data: KagiData[];
+  meta?: any;
+}
