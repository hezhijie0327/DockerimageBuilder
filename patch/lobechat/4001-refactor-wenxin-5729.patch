diff --git a/Dockerfile b/Dockerfile
index 56f30488d8c37..f4137cdc9b688 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -212,7 +212,7 @@ ENV \
     # Upstage
     UPSTAGE_API_KEY="" UPSTAGE_MODEL_LIST="" \
     # Wenxin
-    WENXIN_ACCESS_KEY="" WENXIN_SECRET_KEY="" WENXIN_MODEL_LIST="" \
+    WENXIN_API_KEY="" WENXIN_MODEL_LIST="" \
     # xAI
     XAI_API_KEY="" XAI_MODEL_LIST="" XAI_PROXY_URL="" \
     # 01.AI
diff --git a/Dockerfile.database b/Dockerfile.database
index f3fd4b4aed99c..73acc79e61819 100644
--- a/Dockerfile.database
+++ b/Dockerfile.database
@@ -247,7 +247,7 @@ ENV \
     # Upstage
     UPSTAGE_API_KEY="" UPSTAGE_MODEL_LIST="" \
     # Wenxin
-    WENXIN_ACCESS_KEY="" WENXIN_SECRET_KEY="" WENXIN_MODEL_LIST="" \
+    WENXIN_API_KEY="" WENXIN_MODEL_LIST="" \
     # xAI
     XAI_API_KEY="" XAI_MODEL_LIST="" XAI_PROXY_URL="" \
     # 01.AI
diff --git a/docs/usage/providers/wenxin.mdx b/docs/usage/providers/wenxin.mdx
index c20b35dcf4541..9e78d32c98d5c 100644
--- a/docs/usage/providers/wenxin.mdx
+++ b/docs/usage/providers/wenxin.mdx
@@ -22,27 +22,30 @@ This article will guide you on how to use Wenxin Qianfan in LobeChat.
 <Steps>
   ### Step 1: Obtain the Wenxin Qianfan API Key
 
-  - Register and log in to the [Baidu Intelligent Cloud Console](https://console.bce.baidu.com/)
-  - Navigate to `Baidu Intelligent Cloud Qianfan ModelBuilder`
-  - Choose `Application Access` from the left-side menu
-  - Create an application
+  - Register and log in to the [Baidu AI Cloud Console](https://console.bce.baidu.com/)
+  - Navigate to `Baidu AI Cloud Qianfan ModelBuilder`
+  - Select `API Key` from the left menu
 
-  <Image alt={'Create Application'} inStep src={'https://github.com/user-attachments/assets/927b1040-e23f-4919-92e2-80a400db8327'} />
+  <Image alt={'API Key'} inStep src={'https://github.com/user-attachments/assets/6234428d-5633-4b2f-be22-1a1772a69a55'} />
 
-  - Enter the `Security Authentication` -> `Access Key` management page from the user account menu
-  - Copy the `Access Key` and `Secret Key`, and store them safely
+  - Click `Create API Key`
+  - In `Service`, select `Qianfan ModelBuilder`
+  - In `Resource`, choose `All Resources`
+  - Click the `Confirm` button
+  - Copy the `API Key` and keep it safe
 
-  <Image alt={'Save Keys'} inStep src={'https://github.com/user-attachments/assets/bb9dadd3-7e9c-45fd-8c56-553ab7287453'} />
+  <Image alt={'Create API Key'} inStep src={'https://github.com/user-attachments/assets/6d068fe0-8100-4b43-b0c3-7934f54e688f'} />
+  <Image alt={'Copy API Key'} inStep src={'https://github.com/user-attachments/assets/629adf4e-e9e1-40dc-b9e5-d7b908878170'} />
 
   ### Step 2: Configure Wenxin Qianfan in LobeChat
 
-  - Go to the `Settings` interface in LobeChat
-  - Locate the settings for `Wenxin Qianfan` under `Language Model`
+  - Go to the `Settings` page of LobeChat
+  - Under `Language Models`, find the `Wenxin Qianfan` settings
 
-  <Image alt={'Enter API Keys'} inStep src={'https://github.com/user-attachments/assets/e3995de7-38d9-489b-80a2-434477018469'} />
+  <Image alt={'Enter API Key'} inStep src={'https://github.com/user-attachments/assets/d7666e2a-0202-4b45-8338-9806ddffa44e'} />
 
-  - Enter the obtained `Access Key` and `Secret Key`
-  - Select a Wenxin Qianfan model for your AI assistant to start interacting
+  - Enter the obtained `API Key`
+  - Select a Wenxin Qianfan model for your AI assistant, and you're ready to start chatting!
 
   <Image alt={'Select Wenxin Qianfan Model and Start Chat'} inStep src={'https://github.com/user-attachments/assets/b6e6a3eb-13c6-46f0-9c7c-69a20deae30f'} />
 
diff --git a/docs/usage/providers/wenxin.zh-CN.mdx b/docs/usage/providers/wenxin.zh-CN.mdx
index a2b44bd047650..184f574d75e53 100644
--- a/docs/usage/providers/wenxin.zh-CN.mdx
+++ b/docs/usage/providers/wenxin.zh-CN.mdx
@@ -22,24 +22,27 @@ tags:
 
   - 注册并登录 [百度智能云控制台](https://console.bce.baidu.com/)
   - 进入 `百度智能云千帆 ModelBuilder`
-  - 在左侧菜单中选择`应用接入`
-  - 创建一个应用
+  - 在左侧菜单中选择 `API Key`
 
-  <Image alt={'创建应用'} inStep src={'https://github.com/user-attachments/assets/927b1040-e23f-4919-92e2-80a400db8327'} />
+  <Image alt={'API Key'} inStep src={'https://github.com/user-attachments/assets/6234428d-5633-4b2f-be22-1a1772a69a55'} />
 
-  - 在用户账号菜单进入 `安全认证` -> `Access Key` 管理页面
-  - 复制 `Access Key` 和 `Secret Key`，并妥善保存
+  - 点击创建 API Key
+  - 在 `服务` 中选择 `千帆ModelBuilder`
+  - 在 `资源` 中选择 `所有资源`
+  - 点击 `确定` 按钮
+  - 复制 `API Key` 并妥善保存
 
-  <Image alt={'保存密钥'} inStep src={'https://github.com/user-attachments/assets/bb9dadd3-7e9c-45fd-8c56-553ab7287453'} />
+  <Image alt={'创建密钥'} inStep src={'https://github.com/user-attachments/assets/6d068fe0-8100-4b43-b0c3-7934f54e688f'} />
+  <Image alt={'复制密钥'} inStep src={'https://github.com/user-attachments/assets/629adf4e-e9e1-40dc-b9e5-d7b908878170'} />
 
   ### 步骤二：在 LobeChat 中配置文心千帆
 
   - 访问 LobeChat 的`设置`界面
   - 在`语言模型`下找到 `文心千帆` 的设置项
 
-  <Image alt={'填入 API 密钥'} inStep src={'https://github.com/user-attachments/assets/e3995de7-38d9-489b-80a2-434477018469'} />
+  <Image alt={'填入 API 密钥'} inStep src={'https://github.com/user-attachments/assets/d7666e2a-0202-4b45-8338-9806ddffa44e'} />
 
-  - 填入获得的 `Access Key` 和 `Secret Key`
+  - 填入获得的 `API Key`
   - 为你的 AI 助手选择一个文心千帆的模型即可开始对话
 
   <Image alt={'选择文心千帆模型并开始对话'} inStep src={'https://github.com/user-attachments/assets/b6e6a3eb-13c6-46f0-9c7c-69a20deae30f'} />
diff --git a/package.json b/package.json
index 968b106cad3e8..988f33bc3e131 100644
--- a/package.json
+++ b/package.json
@@ -111,7 +111,6 @@
     "@aws-sdk/s3-request-presigner": "^3.723.0",
     "@azure/core-rest-pipeline": "1.16.0",
     "@azure/openai": "1.0.0-beta.12",
-    "@baiducloud/qianfan": "^0.1.9",
     "@cfworker/json-schema": "^4.1.0",
     "@clerk/localizations": "^3.9.6",
     "@clerk/nextjs": "^6.10.6",
diff --git a/src/app/(backend)/webapi/chat/wenxin/route.test.ts b/src/app/(backend)/webapi/chat/wenxin/route.test.ts
deleted file mode 100644
index cb25976b70fd4..0000000000000
--- a/src/app/(backend)/webapi/chat/wenxin/route.test.ts
+++ /dev/null
@@ -1,27 +0,0 @@
-// @vitest-environment edge-runtime
-import { describe, expect, it, vi } from 'vitest';
-
-import { POST as UniverseRoute } from '../[provider]/route';
-import { POST, runtime } from './route';
-
-// 模拟 '../[provider]/route'
-vi.mock('../[provider]/route', () => ({
-  POST: vi.fn().mockResolvedValue('mocked response'),
-}));
-
-describe('Configuration tests', () => {
-  it('should have runtime set to "edge"', () => {
-    expect(runtime).toBe('nodejs');
-  });
-});
-
-describe('Wenxin POST function tests', () => {
-  it('should call UniverseRoute with correct parameters', async () => {
-    const mockRequest = new Request('https://example.com', { method: 'POST' });
-    await POST(mockRequest);
-    expect(UniverseRoute).toHaveBeenCalledWith(mockRequest, {
-      createRuntime: expect.anything(),
-      params: Promise.resolve({ provider: 'wenxin' }),
-    });
-  });
-});
diff --git a/src/app/(backend)/webapi/chat/wenxin/route.ts b/src/app/(backend)/webapi/chat/wenxin/route.ts
deleted file mode 100644
index b048f62e5ec4b..0000000000000
--- a/src/app/(backend)/webapi/chat/wenxin/route.ts
+++ /dev/null
@@ -1,30 +0,0 @@
-import { getLLMConfig } from '@/config/llm';
-import { AgentRuntime, ModelProvider } from '@/libs/agent-runtime';
-import LobeWenxinAI from '@/libs/agent-runtime/wenxin';
-
-import { POST as UniverseRoute } from '../[provider]/route';
-
-export const runtime = 'nodejs';
-
-export const maxDuration = 30;
-
-export const POST = async (req: Request) =>
-  UniverseRoute(req, {
-    createRuntime: (payload) => {
-      const { WENXIN_ACCESS_KEY, WENXIN_SECRET_KEY } = getLLMConfig();
-      let accessKey: string | undefined = WENXIN_ACCESS_KEY;
-      let secretKey: string | undefined = WENXIN_SECRET_KEY;
-
-      // if the payload has the api key, use user
-      if (payload.apiKey) {
-        accessKey = payload?.wenxinAccessKey;
-        secretKey = payload?.wenxinSecretKey;
-      }
-
-      const params = { accessKey, secretKey };
-      const instance = new LobeWenxinAI(params);
-
-      return new AgentRuntime(instance);
-    },
-    params: Promise.resolve({ provider: ModelProvider.Wenxin }),
-  });
diff --git a/src/app/(main)/settings/llm/ProviderList/Wenxin/index.tsx b/src/app/(main)/settings/llm/ProviderList/Wenxin/index.tsx
deleted file mode 100644
index ffde4872d07cd..0000000000000
--- a/src/app/(main)/settings/llm/ProviderList/Wenxin/index.tsx
+++ /dev/null
@@ -1,44 +0,0 @@
-'use client';
-
-import { Input } from 'antd';
-import { useTranslation } from 'react-i18next';
-
-import { WenxinProviderCard } from '@/config/modelProviders';
-import { GlobalLLMProviderKey } from '@/types/user/settings';
-
-import { KeyVaultsConfigKey } from '../../const';
-import { ProviderItem } from '../../type';
-
-const providerKey: GlobalLLMProviderKey = 'wenxin';
-
-export const useWenxinProvider = (): ProviderItem => {
-  const { t } = useTranslation('modelProvider');
-
-  return {
-    ...WenxinProviderCard,
-    apiKeyItems: [
-      {
-        children: (
-          <Input.Password
-            autoComplete={'new-password'}
-            placeholder={t(`${providerKey}.accessKey.placeholder`)}
-          />
-        ),
-        desc: t(`${providerKey}.accessKey.desc`),
-        label: t(`${providerKey}.accessKey.title`),
-        name: [KeyVaultsConfigKey, providerKey, 'accessKey'],
-      },
-      {
-        children: (
-          <Input.Password
-            autoComplete={'new-password'}
-            placeholder={t(`${providerKey}.secretKey.placeholder`)}
-          />
-        ),
-        desc: t(`${providerKey}.secretKey.desc`),
-        label: t(`${providerKey}.secretKey.title`),
-        name: [KeyVaultsConfigKey, providerKey, 'secretKey'],
-      },
-    ],
-  };
-};
diff --git a/src/app/(main)/settings/llm/ProviderList/providers.tsx b/src/app/(main)/settings/llm/ProviderList/providers.tsx
index f108683bfa07f..333864ec7fc37 100644
--- a/src/app/(main)/settings/llm/ProviderList/providers.tsx
+++ b/src/app/(main)/settings/llm/ProviderList/providers.tsx
@@ -27,6 +27,7 @@ import {
   TaichuProviderCard,
   TogetherAIProviderCard,
   UpstageProviderCard,
+  WenxinProviderCard,
   XAIProviderCard,
   ZeroOneProviderCard,
   ZhiPuProviderCard,
@@ -40,7 +41,6 @@ import { useGithubProvider } from './Github';
 import { useHuggingFaceProvider } from './HuggingFace';
 import { useOllamaProvider } from './Ollama';
 import { useOpenAIProvider } from './OpenAI';
-import { useWenxinProvider } from './Wenxin';
 
 export const useProviderList = (): ProviderItem[] => {
   const AzureProvider = useAzureProvider();
@@ -50,7 +50,6 @@ export const useProviderList = (): ProviderItem[] => {
   const CloudflareProvider = useCloudflareProvider();
   const GithubProvider = useGithubProvider();
   const HuggingFaceProvider = useHuggingFaceProvider();
-  const WenxinProvider = useWenxinProvider();
 
   return useMemo(
     () => [
@@ -75,7 +74,7 @@ export const useProviderList = (): ProviderItem[] => {
       UpstageProviderCard,
       XAIProviderCard,
       QwenProviderCard,
-      WenxinProvider,
+      WenxinProviderCard,
       HunyuanProviderCard,
       SparkProviderCard,
       ZhiPuProviderCard,
@@ -99,7 +98,6 @@ export const useProviderList = (): ProviderItem[] => {
       BedrockProvider,
       CloudflareProvider,
       GithubProvider,
-      WenxinProvider,
       HuggingFaceProvider,
     ],
   );
diff --git a/src/app/(main)/settings/provider/(detail)/wenxin/page.tsx b/src/app/(main)/settings/provider/(detail)/wenxin/page.tsx
deleted file mode 100644
index 2d820e1a7df39..0000000000000
--- a/src/app/(main)/settings/provider/(detail)/wenxin/page.tsx
+++ /dev/null
@@ -1,61 +0,0 @@
-'use client';
-
-import { useTranslation } from 'react-i18next';
-
-import { FormPassword } from '@/components/FormInput';
-import { WenxinProviderCard } from '@/config/modelProviders';
-import { aiProviderSelectors, useAiInfraStore } from '@/store/aiInfra';
-import { GlobalLLMProviderKey } from '@/types/user/settings';
-
-import { KeyVaultsConfigKey } from '../../const';
-import { SkeletonInput } from '../../features/ProviderConfig';
-import { ProviderItem } from '../../type';
-import ProviderDetail from '../[id]';
-
-const providerKey: GlobalLLMProviderKey = 'wenxin';
-
-const useProviderCard = (): ProviderItem => {
-  const { t } = useTranslation('modelProvider');
-
-  const isLoading = useAiInfraStore(aiProviderSelectors.isAiProviderConfigLoading(providerKey));
-
-  return {
-    ...WenxinProviderCard,
-    apiKeyItems: [
-      {
-        children: isLoading ? (
-          <SkeletonInput />
-        ) : (
-          <FormPassword
-            autoComplete={'new-password'}
-            placeholder={t(`${providerKey}.accessKey.placeholder`)}
-          />
-        ),
-        desc: t(`${providerKey}.accessKey.desc`),
-        label: t(`${providerKey}.accessKey.title`),
-        name: [KeyVaultsConfigKey, 'accessKey'],
-      },
-      {
-        children: isLoading ? (
-          <SkeletonInput />
-        ) : (
-          <FormPassword
-            autoComplete={'new-password'}
-            placeholder={t(`${providerKey}.secretKey.placeholder`)}
-          />
-        ),
-        desc: t(`${providerKey}.secretKey.desc`),
-        label: t(`${providerKey}.secretKey.title`),
-        name: [KeyVaultsConfigKey, 'secretKey'],
-      },
-    ],
-  };
-};
-
-const Page = () => {
-  const card = useProviderCard();
-
-  return <ProviderDetail {...card} />;
-};
-
-export default Page;
diff --git a/src/config/aiModels/wenxin.ts b/src/config/aiModels/wenxin.ts
index f773c6b6f37a8..cd193788f9e53 100644
--- a/src/config/aiModels/wenxin.ts
+++ b/src/config/aiModels/wenxin.ts
@@ -2,12 +2,15 @@ import { AIChatModelCard } from '@/types/aiModel';
 
 const wenxinChatModels: AIChatModelCard[] = [
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 8192,
     description:
       '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
     displayName: 'ERNIE 3.5 8K',
     enabled: true,
-    id: 'ERNIE-3.5-8K',
+    id: 'ernie-3.5-8k',
     pricing: {
       currency: 'CNY',
       input: 0.8,
@@ -16,11 +19,14 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 8192,
     description:
       '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
     displayName: 'ERNIE 3.5 8K Preview',
-    id: 'ERNIE-3.5-8K-Preview',
+    id: 'ernie-3.5-8k-preview',
     pricing: {
       currency: 'CNY',
       input: 0.8,
@@ -29,12 +35,15 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 128_000,
     description:
       '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
     displayName: 'ERNIE 3.5 128K',
     enabled: true,
-    id: 'ERNIE-3.5-128K',
+    id: 'ernie-3.5-128k',
     pricing: {
       currency: 'CNY',
       input: 0.8,
@@ -43,12 +52,15 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 8192,
     description:
       '百度自研的旗舰级超大规模⼤语⾔模型，相较ERNIE 3.5实现了模型能力全面升级，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。',
     displayName: 'ERNIE 4.0 8K',
     enabled: true,
-    id: 'ERNIE-4.0-8K-Latest',
+    id: 'ernie-4.0-8k-latest',
     pricing: {
       currency: 'CNY',
       input: 30,
@@ -57,11 +69,14 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 8192,
     description:
       '百度自研的旗舰级超大规模⼤语⾔模型，相较ERNIE 3.5实现了模型能力全面升级，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。',
     displayName: 'ERNIE 4.0 8K Preview',
-    id: 'ERNIE-4.0-8K-Preview',
+    id: 'ernie-4.0-8k-preview',
     pricing: {
       currency: 'CNY',
       input: 30,
@@ -70,12 +85,15 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 8192,
     description:
       '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
     displayName: 'ERNIE 4.0 Turbo 8K',
     enabled: true,
-    id: 'ERNIE-4.0-Turbo-8K-Latest',
+    id: 'ernie-4.0-turbo-8k-latest',
     pricing: {
       currency: 'CNY',
       input: 20,
@@ -84,12 +102,15 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 128_000,
     description:
       '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
     displayName: 'ERNIE 4.0 Turbo 128K',
     enabled: true,
-    id: 'ERNIE-4.0-Turbo-128K',
+    id: 'ernie-4.0-turbo-128k',
     pricing: {
       currency: 'CNY',
       input: 20,
@@ -98,11 +119,14 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 8192,
     description:
       '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
     displayName: 'ERNIE 4.0 Turbo 8K Preview',
-    id: 'ERNIE-4.0-Turbo-8K-Preview',
+    id: 'ernie-4.0-turbo-8k-preview',
     pricing: {
       currency: 'CNY',
       input: 20,
@@ -111,12 +135,27 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    contextWindowTokens: 8192,
+    description:
+      'ERNIE Lite是百度自研的轻量级大语言模型，兼顾优异的模型效果与推理性能，适合低算力AI加速卡推理使用。',
+    displayName: 'ERNIE Lite 8K',
+    id: 'ernie-lite-8k',
+    pricing: {
+      currency: 'CNY',
+      input: 0,
+      output: 0,
+    },
+    type: 'chat',
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
     contextWindowTokens: 128_000,
     description:
       '百度自研的轻量级大语言模型，兼顾优异的模型效果与推理性能，效果比ERNIE Lite更优，适合低算力AI加速卡推理使用。',
     displayName: 'ERNIE Lite Pro 128K',
-    enabled: true,
-    id: 'ERNIE-Lite-Pro-128K',
+    id: 'ernie-lite-pro-128k',
     pricing: {
       currency: 'CNY',
       input: 0.2,
@@ -125,16 +164,15 @@ const wenxinChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
-    contextWindowTokens: 128_000,
+    contextWindowTokens: 8192,
     description:
-      '百度2024年最新发布的自研高性能大语言模型，通用能力优异，效果比ERNIE Speed更优，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
-    displayName: 'ERNIE Speed Pro 128K',
-    enabled: true,
-    id: 'ERNIE-Speed-Pro-128K',
+      'ERNIE Tiny是百度自研的超高性能大语言模型，部署与精调成本在文心系列模型中最低。',
+    displayName: 'ERNIE Tiny 8K',
+    id: 'ernie-tiny-8k',
     pricing: {
       currency: 'CNY',
-      input: 0.3,
-      output: 0.6,
+      input: 0,
+      output: 0,
     },
     type: 'chat',
   },
@@ -143,7 +181,7 @@ const wenxinChatModels: AIChatModelCard[] = [
     description:
       '百度2024年最新发布的自研高性能大语言模型，通用能力优异，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
     displayName: 'ERNIE Speed 128K',
-    id: 'ERNIE-Speed-128K',
+    id: 'ernie-speed-128k',
     pricing: {
       currency: 'CNY',
       input: 0,
@@ -151,12 +189,25 @@ const wenxinChatModels: AIChatModelCard[] = [
     },
     type: 'chat',
   },
+  {
+    contextWindowTokens: 128_000,
+    description:
+      '百度2024年最新发布的自研高性能大语言模型，通用能力优异，效果比ERNIE Speed更优，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
+    displayName: 'ERNIE Speed Pro 128K',
+    id: 'ernie-speed-pro-128k',
+    pricing: {
+      currency: 'CNY',
+      input: 0.3,
+      output: 0.6,
+    },
+    type: 'chat',
+  },
   {
     contextWindowTokens: 8192,
     description:
       '百度自研的垂直场景大语言模型，适合游戏NPC、客服对话、对话角色扮演等应用场景，人设风格更为鲜明、一致，指令遵循能力更强，推理性能更优。',
     displayName: 'ERNIE Character 8K',
-    id: 'ERNIE-Character-8K',
+    id: 'ernie-char-8k',
     pricing: {
       currency: 'CNY',
       input: 4,
@@ -164,6 +215,61 @@ const wenxinChatModels: AIChatModelCard[] = [
     },
     type: 'chat',
   },
+  {
+    contextWindowTokens: 8192,
+    description:
+      '百度自研的垂直场景大语言模型，适合游戏NPC、客服对话、对话角色扮演等应用场景，人设风格更为鲜明、一致，指令遵循能力更强，推理性能更优。',
+    displayName: 'ERNIE Character Fiction 8K',
+    id: 'ernie-char-fiction-8k',
+    pricing: {
+      currency: 'CNY',
+      input: 4,
+      output: 8,
+    },
+    type: 'chat',
+  },
+  {
+    contextWindowTokens: 8192,
+    description:
+      '百度自研通用大语言模型，在小说续写能力上有明显优势，也可用在短剧、电影等场景。',
+    displayName: 'ERNIE Novel 8K',
+    id: 'ernie-novel-8k',
+    pricing: {
+      currency: 'CNY',
+      input: 40,
+      output: 120,
+    },
+    type: 'chat',
+  },
+  {
+    contextWindowTokens: 65_536,
+    description:
+      'DeepSeek-V3 为杭州深度求索人工智能基础技术研究有限公司自研的 MoE 模型，其多项评测成绩突出，在主流榜单中位列开源模型榜首。V3 相比 V2.5 模型生成速度实现 3 倍提升，为用户带来更加迅速流畅的使用体验。',
+    displayName: 'DeepSeek V3',
+    id: 'deepseek-v3',
+    pricing: {
+      currency: 'CNY',
+      input: 0.8,
+      output: 1.6,
+    },
+    type: 'chat',
+  },
+  {
+    abilities: {
+      reasoning: true,
+    },
+    contextWindowTokens: 65_536,
+    description:
+      'DeepSeek-R1 在后训练阶段大规模使用了强化学习技术，在仅有极少标注数据的情况下，极大提升了模型推理能力。在数学、代码、自然语言推理等任务上，性能比肩 OpenAI o1 正式版。',
+    displayName: 'DeepSeek R1',
+    id: 'deepseek-r1',
+    pricing: {
+      currency: 'CNY',
+      input: 2,
+      output: 8,
+    },
+    type: 'chat',
+  },
 ];
 
 export const allModels = [...wenxinChatModels];
diff --git a/src/config/llm.ts b/src/config/llm.ts
index c5f15c3f0acf6..f6908756c7fc6 100644
--- a/src/config/llm.ts
+++ b/src/config/llm.ts
@@ -64,8 +64,7 @@ export const getLLMConfig = () => {
       AWS_SESSION_TOKEN: z.string().optional(),
 
       ENABLED_WENXIN: z.boolean(),
-      WENXIN_ACCESS_KEY: z.string().optional(),
-      WENXIN_SECRET_KEY: z.string().optional(),
+      WENXIN_API_KEY: z.string().optional(),
 
       ENABLED_OLLAMA: z.boolean(),
 
@@ -186,9 +185,8 @@ export const getLLMConfig = () => {
       AWS_SECRET_ACCESS_KEY: process.env.AWS_SECRET_ACCESS_KEY,
       AWS_SESSION_TOKEN: process.env.AWS_SESSION_TOKEN,
 
-      ENABLED_WENXIN: !!process.env.WENXIN_ACCESS_KEY && !!process.env.WENXIN_SECRET_KEY,
-      WENXIN_ACCESS_KEY: process.env.WENXIN_ACCESS_KEY,
-      WENXIN_SECRET_KEY: process.env.WENXIN_SECRET_KEY,
+      ENABLED_WENXIN: !!process.env.WENXIN_API_KEY,
+      WENXIN_API_KEY: process.env.WENXIN_API_KEY,
 
       ENABLED_OLLAMA: process.env.ENABLED_OLLAMA !== '0',
 
diff --git a/src/config/modelProviders/wenxin.ts b/src/config/modelProviders/wenxin.ts
index 25a71e60b0497..02305b0479e8b 100644
--- a/src/config/modelProviders/wenxin.ts
+++ b/src/config/modelProviders/wenxin.ts
@@ -9,7 +9,8 @@ const BaiduWenxin: ModelProviderCard = {
         '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
       displayName: 'ERNIE 3.5 8K',
       enabled: true,
-      id: 'ERNIE-3.5-8K',
+      functionCall: true,
+      id: 'ernie-3.5-8k',
       pricing: {
         currency: 'CNY',
         input: 0.8,
@@ -21,7 +22,8 @@ const BaiduWenxin: ModelProviderCard = {
       description:
         '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
       displayName: 'ERNIE 3.5 8K Preview',
-      id: 'ERNIE-3.5-8K-Preview',
+      functionCall: true,
+      id: 'ernie-3.5-8k-preview',
       pricing: {
         currency: 'CNY',
         input: 0.8,
@@ -34,7 +36,8 @@ const BaiduWenxin: ModelProviderCard = {
         '百度自研的旗舰级大规模⼤语⾔模型，覆盖海量中英文语料，具有强大的通用能力，可满足绝大部分对话问答、创作生成、插件应用场景要求；支持自动对接百度搜索插件，保障问答信息时效。',
       displayName: 'ERNIE 3.5 128K',
       enabled: true,
-      id: 'ERNIE-3.5-128K',
+      functionCall: true,
+      id: 'ernie-3.5-128k',
       pricing: {
         currency: 'CNY',
         input: 0.8,
@@ -47,7 +50,8 @@ const BaiduWenxin: ModelProviderCard = {
         '百度自研的旗舰级超大规模⼤语⾔模型，相较ERNIE 3.5实现了模型能力全面升级，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。',
       displayName: 'ERNIE 4.0 8K',
       enabled: true,
-      id: 'ERNIE-4.0-8K-Latest',
+      functionCall: true,
+      id: 'ernie-4.0-8k-latest',
       pricing: {
         currency: 'CNY',
         input: 30,
@@ -59,7 +63,8 @@ const BaiduWenxin: ModelProviderCard = {
       description:
         '百度自研的旗舰级超大规模⼤语⾔模型，相较ERNIE 3.5实现了模型能力全面升级，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。',
       displayName: 'ERNIE 4.0 8K Preview',
-      id: 'ERNIE-4.0-8K-Preview',
+      functionCall: true,
+      id: 'ernie-4.0-8k-preview',
       pricing: {
         currency: 'CNY',
         input: 30,
@@ -72,7 +77,8 @@ const BaiduWenxin: ModelProviderCard = {
         '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
       displayName: 'ERNIE 4.0 Turbo 8K',
       enabled: true,
-      id: 'ERNIE-4.0-Turbo-8K-Latest',
+      functionCall: true,
+      id: 'ernie-4.0-turbo-8k-latest',
       pricing: {
         currency: 'CNY',
         input: 20,
@@ -85,7 +91,8 @@ const BaiduWenxin: ModelProviderCard = {
         '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
       displayName: 'ERNIE 4.0 Turbo 128K',
       enabled: true,
-      id: 'ERNIE-4.0-Turbo-128K',
+      functionCall: true,
+      id: 'ernie-4.0-turbo-128k',
       pricing: {
         currency: 'CNY',
         input: 20,
@@ -97,20 +104,33 @@ const BaiduWenxin: ModelProviderCard = {
       description:
         '百度自研的旗舰级超大规模⼤语⾔模型，综合效果表现出色，广泛适用于各领域复杂任务场景；支持自动对接百度搜索插件，保障问答信息时效。相较于ERNIE 4.0在性能表现上更优秀',
       displayName: 'ERNIE 4.0 Turbo 8K Preview',
-      id: 'ERNIE-4.0-Turbo-8K-Preview',
+      functionCall: true,
+      id: 'ernie-4.0-turbo-8k-preview',
       pricing: {
         currency: 'CNY',
         input: 20,
         output: 60,
       },
     },
+    {
+      contextWindowTokens: 8192,
+      description:
+        'ERNIE Lite是百度自研的轻量级大语言模型，兼顾优异的模型效果与推理性能，适合低算力AI加速卡推理使用。',
+      displayName: 'ERNIE Lite 8K',
+      id: 'ernie-lite-8k',
+      pricing: {
+        currency: 'CNY',
+        input: 0,
+        output: 0,
+      },
+    },
     {
       contextWindowTokens: 128_000,
       description:
         '百度自研的轻量级大语言模型，兼顾优异的模型效果与推理性能，效果比ERNIE Lite更优，适合低算力AI加速卡推理使用。',
       displayName: 'ERNIE Lite Pro 128K',
-      enabled: true,
-      id: 'ERNIE-Lite-Pro-128K',
+      functionCall: true,
+      id: 'ernie-lite-pro-128k',
       pricing: {
         currency: 'CNY',
         input: 0.2,
@@ -118,16 +138,15 @@ const BaiduWenxin: ModelProviderCard = {
       },
     },
     {
-      contextWindowTokens: 128_000,
+      contextWindowTokens: 8192,
       description:
-        '百度2024年最新发布的自研高性能大语言模型，通用能力优异，效果比ERNIE Speed更优，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
-      displayName: 'ERNIE Speed Pro 128K',
-      enabled: true,
-      id: 'ERNIE-Speed-Pro-128K',
+        'ERNIE Tiny是百度自研的超高性能大语言模型，部署与精调成本在文心系列模型中最低。',
+      displayName: 'ERNIE Tiny 8K',
+      id: 'ernie-tiny-8k',
       pricing: {
         currency: 'CNY',
-        input: 0.3,
-        output: 0.6,
+        input: 0,
+        output: 0,
       },
     },
     {
@@ -135,36 +154,94 @@ const BaiduWenxin: ModelProviderCard = {
       description:
         '百度2024年最新发布的自研高性能大语言模型，通用能力优异，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
       displayName: 'ERNIE Speed 128K',
-      id: 'ERNIE-Speed-128K',
+      id: 'ernie-speed-128k',
       pricing: {
         currency: 'CNY',
         input: 0,
         output: 0,
       },
     },
+    {
+      contextWindowTokens: 128_000,
+      description:
+        '百度2024年最新发布的自研高性能大语言模型，通用能力优异，效果比ERNIE Speed更优，适合作为基座模型进行精调，更好地处理特定场景问题，同时具备极佳的推理性能。',
+      displayName: 'ERNIE Speed Pro 128K',
+      id: 'ernie-speed-pro-128k',
+      pricing: {
+        currency: 'CNY',
+        input: 0.3,
+        output: 0.6,
+      },
+    },
     {
       contextWindowTokens: 8192,
       description:
         '百度自研的垂直场景大语言模型，适合游戏NPC、客服对话、对话角色扮演等应用场景，人设风格更为鲜明、一致，指令遵循能力更强，推理性能更优。',
       displayName: 'ERNIE Character 8K',
-      id: 'ERNIE-Character-8K',
+      id: 'ernie-char-8k',
       pricing: {
         currency: 'CNY',
         input: 4,
         output: 8,
       },
     },
+    {
+      contextWindowTokens: 8192,
+      description:
+        '百度自研的垂直场景大语言模型，适合游戏NPC、客服对话、对话角色扮演等应用场景，人设风格更为鲜明、一致，指令遵循能力更强，推理性能更优。',
+      displayName: 'ERNIE Character Fiction 8K',
+      id: 'ernie-char-fiction-8k',
+      pricing: {
+        currency: 'CNY',
+        input: 4,
+        output: 8,
+      },
+    },
+    {
+      contextWindowTokens: 8192,
+      description:
+        '百度自研通用大语言模型，在小说续写能力上有明显优势，也可用在短剧、电影等场景。',
+      displayName: 'ERNIE Novel 8K',
+      id: 'ernie-novel-8k',
+      pricing: {
+        currency: 'CNY',
+        input: 40,
+        output: 120,
+      },
+    },
+    {
+      contextWindowTokens: 65_536,
+      description:
+        'DeepSeek-V3 为杭州深度求索人工智能基础技术研究有限公司自研的 MoE 模型，其多项评测成绩突出，在主流榜单中位列开源模型榜首。V3 相比 V2.5 模型生成速度实现 3 倍提升，为用户带来更加迅速流畅的使用体验。',
+      displayName: 'DeepSeek V3',
+      id: 'deepseek-v3',
+      pricing: {
+        currency: 'CNY',
+        input: 0.8,
+        output: 1.6,
+      },
+    },
+    {
+      contextWindowTokens: 65_536,
+      description:
+        'DeepSeek-R1 在后训练阶段大规模使用了强化学习技术，在仅有极少标注数据的情况下，极大提升了模型推理能力。在数学、代码、自然语言推理等任务上，性能比肩 OpenAI o1 正式版。',
+      displayName: 'DeepSeek R1',
+      id: 'deepseek-r1',
+      pricing: {
+        currency: 'CNY',
+        input: 2,
+        output: 8,
+      },
+    },
   ],
-  checkModel: 'ERNIE-Speed-128K',
+  checkModel: 'ernie-speed-128k',
   description:
     '企业级一站式大模型与AI原生应用开发及服务平台，提供最全面易用的生成式人工智能模型开发、应用开发全流程工具链',
-  disableBrowserRequest: true,
   id: 'wenxin',
   modelsUrl: 'https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu#%E5%AF%B9%E8%AF%9Dchat',
   name: 'Wenxin',
   settings: {
-    disableBrowserRequest: true,
-    sdkType: 'wenxin',
+    sdkType: 'openai',
     smoothing: {
       speed: 2,
       text: true,
diff --git a/src/const/auth.ts b/src/const/auth.ts
index 5bec0201d5a2f..9a633da5459ec 100644
--- a/src/const/auth.ts
+++ b/src/const/auth.ts
@@ -39,9 +39,6 @@ export interface JWTPayload {
 
   cloudflareBaseURLOrAccountID?: string;
 
-  wenxinAccessKey?: string;
-  wenxinSecretKey?: string;
-
   /**
    * user id
    * in client db mode it's a uuid
diff --git a/src/features/Conversation/Error/APIKeyForm/Wenxin.tsx b/src/features/Conversation/Error/APIKeyForm/Wenxin.tsx
deleted file mode 100644
index c109cf6afbd27..0000000000000
--- a/src/features/Conversation/Error/APIKeyForm/Wenxin.tsx
+++ /dev/null
@@ -1,49 +0,0 @@
-import { Wenxin } from '@lobehub/icons';
-import { Input } from 'antd';
-import { memo } from 'react';
-import { useTranslation } from 'react-i18next';
-
-import { ModelProvider } from '@/libs/agent-runtime';
-import { useUserStore } from '@/store/user';
-import { keyVaultsConfigSelectors } from '@/store/user/selectors';
-
-import { FormAction } from '../style';
-
-const WenxinForm = memo(() => {
-  const { t } = useTranslation('modelProvider');
-
-  const [accessKey, secretKey, setConfig] = useUserStore((s) => [
-    keyVaultsConfigSelectors.wenxinConfig(s).accessKey,
-    keyVaultsConfigSelectors.wenxinConfig(s).secretKey,
-    s.updateKeyVaultConfig,
-  ]);
-
-  return (
-    <FormAction
-      avatar={<Wenxin.Color size={56} />}
-      description={t('wenxin.unlock.description')}
-      title={t('wenxin.unlock.title')}
-    >
-      <Input.Password
-        autoComplete={'new-password'}
-        onChange={(e) => {
-          setConfig(ModelProvider.Wenxin, { accessKey: e.target.value });
-        }}
-        placeholder={'Access Key'}
-        type={'block'}
-        value={accessKey}
-      />
-      <Input.Password
-        autoComplete={'new-password'}
-        onChange={(e) => {
-          setConfig(ModelProvider.Wenxin, { secretKey: e.target.value });
-        }}
-        placeholder={'Secret Key'}
-        type={'block'}
-        value={secretKey}
-      />
-    </FormAction>
-  );
-});
-
-export default WenxinForm;
diff --git a/src/features/Conversation/Error/APIKeyForm/index.tsx b/src/features/Conversation/Error/APIKeyForm/index.tsx
index 5ba78f4f0ba34..703b079948f35 100644
--- a/src/features/Conversation/Error/APIKeyForm/index.tsx
+++ b/src/features/Conversation/Error/APIKeyForm/index.tsx
@@ -10,7 +10,6 @@ import { GlobalLLMProviderKey } from '@/types/user/settings';
 
 import BedrockForm from './Bedrock';
 import ProviderApiKeyForm from './ProviderApiKeyForm';
-import WenxinForm from './Wenxin';
 
 interface APIKeyFormProps {
   id: string;
@@ -66,8 +65,6 @@ const APIKeyForm = memo<APIKeyFormProps>(({ id, provider }) => {
     <Center gap={16} style={{ maxWidth: 300 }}>
       {provider === ModelProvider.Bedrock ? (
         <BedrockForm />
-      ) : provider === ModelProvider.Wenxin ? (
-        <WenxinForm />
       ) : (
         <ProviderApiKeyForm
           apiKeyPlaceholder={apiKeyPlaceholder}
diff --git a/src/libs/agent-runtime/AgentRuntime.test.ts b/src/libs/agent-runtime/AgentRuntime.test.ts
index ad41d93639c6a..57198128419ad 100644
--- a/src/libs/agent-runtime/AgentRuntime.test.ts
+++ b/src/libs/agent-runtime/AgentRuntime.test.ts
@@ -26,7 +26,6 @@ import {
   ModelProvider,
 } from '@/libs/agent-runtime';
 import { LobeStepfunAI } from '@/libs/agent-runtime/stepfun';
-import LobeWenxinAI from '@/libs/agent-runtime/wenxin';
 import { createTraceOptions } from '@/server/modules/AgentRuntime';
 
 import { AgentChatOptions } from './AgentRuntime';
diff --git a/src/libs/agent-runtime/AgentRuntime.ts b/src/libs/agent-runtime/AgentRuntime.ts
index c2b46ea6d6618..bf562220f7ea9 100644
--- a/src/libs/agent-runtime/AgentRuntime.ts
+++ b/src/libs/agent-runtime/AgentRuntime.ts
@@ -47,6 +47,7 @@ import {
   TextToSpeechPayload,
 } from './types';
 import { LobeUpstageAI } from './upstage';
+import { LobeWenxinAI } from './wenxin';
 import { LobeXAI } from './xai';
 import { LobeZeroOneAI } from './zeroone';
 import { LobeZhipuAI } from './zhipu';
@@ -167,6 +168,7 @@ class AgentRuntime {
       taichu: Partial<ClientOptions>;
       togetherai: Partial<ClientOptions>;
       upstage: Partial<ClientOptions>;
+      wenxin: Partial<ClientOptions>;
       xai: Partial<ClientOptions>;
       zeroone: Partial<ClientOptions>;
       zhipu: Partial<ClientOptions>;
@@ -370,6 +372,11 @@ class AgentRuntime {
         runtimeModel = new LobeDoubaoAI(params.doubao);
         break;
       }
+
+      case ModelProvider.Wenxin: {
+        runtimeModel = new LobeWenxinAI(params.wenxin);
+        break;
+      }
     }
     return new AgentRuntime(runtimeModel);
   }
diff --git a/src/libs/agent-runtime/utils/streams/wenxin.test.ts b/src/libs/agent-runtime/utils/streams/wenxin.test.ts
deleted file mode 100644
index da0072db84419..0000000000000
--- a/src/libs/agent-runtime/utils/streams/wenxin.test.ts
+++ /dev/null
@@ -1,153 +0,0 @@
-import { describe, expect, it, vi } from 'vitest';
-
-import * as uuidModule from '@/utils/uuid';
-
-import { convertIterableToStream } from '../../utils/streams/protocol';
-import { ChatResp } from '../../wenxin/type';
-import { WenxinStream } from './wenxin';
-
-const dataStream = [
-  {
-    id: 'as-vb0m37ti8y',
-    object: 'chat.completion',
-    created: 1709089502,
-    sentence_id: 0,
-    is_end: false,
-    is_truncated: false,
-    result: '当然可以，',
-    need_clear_history: false,
-    finish_reason: 'normal',
-    usage: { prompt_tokens: 5, completion_tokens: 2, total_tokens: 7 },
-  },
-  {
-    id: 'as-vb0m37ti8y',
-    object: 'chat.completion',
-    created: 1709089504,
-    sentence_id: 1,
-    is_end: false,
-    is_truncated: false,
-    result:
-      '以下是一些建议的自驾游路线，它们涵盖了各种不同的风景和文化体验：\n\n1. **西安-敦煌历史文化之旅**：\n\n\n\t* 路线：西安',
-    need_clear_history: false,
-    finish_reason: 'normal',
-    usage: { prompt_tokens: 5, completion_tokens: 2, total_tokens: 7 },
-  },
-  {
-    id: 'as-vb0m37ti8y',
-    object: 'chat.completion',
-    created: 1709089506,
-    sentence_id: 2,
-    is_end: false,
-    is_truncated: false,
-    result: ' - 天水 - 兰州 - 嘉峪关 - 敦煌\n\t* 特点：此路线让您领略到中国西北的丰富历史文化。',
-    need_clear_history: false,
-    finish_reason: 'normal',
-    usage: { prompt_tokens: 5, completion_tokens: 2, total_tokens: 7 },
-  },
-  {
-    id: 'as-vb0m37ti8y',
-    object: 'chat.completion',
-    created: 1709089508,
-    sentence_id: 3,
-    is_end: false,
-    is_truncated: false,
-    result: '您可以参观西安的兵马俑、大雁塔，体验兰州的黄河风情，以及在敦煌欣赏壮丽的莫高窟。',
-    need_clear_history: false,
-    finish_reason: 'normal',
-    usage: { prompt_tokens: 5, completion_tokens: 2, total_tokens: 7 },
-  },
-  {
-    id: 'as-vb0m37ti8y',
-    object: 'chat.completion',
-    created: 1709089511,
-    sentence_id: 4,
-    is_end: false,
-    is_truncated: false,
-    result: '\n2. **海南环岛热带风情游**：\n\n\n\t* 路线：海口 - 三亚 - 陵水 - 万宁 - 文昌 - 海',
-    need_clear_history: false,
-    finish_reason: 'normal',
-    usage: { prompt_tokens: 5, completion_tokens: 2, total_tokens: 7 },
-  },
-  {
-    id: 'as-vb0m37ti8y',
-    object: 'chat.completion',
-    created: 1709089512,
-    sentence_id: 5,
-    is_end: false,
-    is_truncated: false,
-    result:
-      '口\n\t* 特点：海南岛是中国唯一的黎族聚居区，这里有独特的热带风情、美丽的海滩和丰富的水果。',
-    need_clear_history: false,
-    finish_reason: 'normal',
-    usage: { prompt_tokens: 5, completion_tokens: 153, total_tokens: 158 },
-  },
-];
-
-describe('WenxinStream', () => {
-  it('should transform Wenxin stream to protocol stream', async () => {
-    vi.spyOn(uuidModule, 'nanoid').mockReturnValueOnce('1');
-
-    const mockWenxinStream: AsyncIterable<ChatResp> = {
-      // @ts-ignore
-      async *[Symbol.asyncIterator]() {
-        for (const item of dataStream) {
-          yield item;
-        }
-      },
-    };
-
-    const stream = convertIterableToStream(mockWenxinStream);
-
-    const onStartMock = vi.fn();
-    const onTextMock = vi.fn();
-    const onTokenMock = vi.fn();
-    const onCompletionMock = vi.fn();
-
-    const protocolStream = WenxinStream(stream, {
-      onStart: onStartMock,
-      onText: onTextMock,
-      onToken: onTokenMock,
-      onCompletion: onCompletionMock,
-    });
-
-    const decoder = new TextDecoder();
-    const chunks = [];
-
-    // @ts-ignore
-    for await (const chunk of protocolStream) {
-      chunks.push(decoder.decode(chunk, { stream: true }));
-    }
-
-    expect(chunks).toEqual(
-      [
-        'id: as-vb0m37ti8y',
-        'event: text',
-        `data: "当然可以，"\n`,
-        'id: as-vb0m37ti8y',
-        'event: text',
-        `data: "以下是一些建议的自驾游路线，它们涵盖了各种不同的风景和文化体验：\\n\\n1. **西安-敦煌历史文化之旅**：\\n\\n\\n\\t* 路线：西安"\n`,
-        'id: as-vb0m37ti8y',
-        'event: text',
-        `data: " - 天水 - 兰州 - 嘉峪关 - 敦煌\\n\\t* 特点：此路线让您领略到中国西北的丰富历史文化。"\n`,
-        'id: as-vb0m37ti8y',
-        'event: text',
-        `data: "您可以参观西安的兵马俑、大雁塔，体验兰州的黄河风情，以及在敦煌欣赏壮丽的莫高窟。"\n`,
-        'id: as-vb0m37ti8y',
-        'event: text',
-        `data: "\\n2. **海南环岛热带风情游**：\\n\\n\\n\\t* 路线：海口 - 三亚 - 陵水 - 万宁 - 文昌 - 海"\n`,
-        'id: as-vb0m37ti8y',
-        'event: text',
-        `data: "口\\n\\t* 特点：海南岛是中国唯一的黎族聚居区，这里有独特的热带风情、美丽的海滩和丰富的水果。"\n`,
-      ].map((item) => `${item}\n`),
-    );
-
-    expect(onStartMock).toHaveBeenCalledTimes(1);
-    expect(onTextMock).toHaveBeenNthCalledWith(1, '"当然可以，"');
-    expect(onTextMock).toHaveBeenNthCalledWith(
-      2,
-      '"以下是一些建议的自驾游路线，它们涵盖了各种不同的风景和文化体验：\\n\\n1. **西安-敦煌历史文化之旅**：\\n\\n\\n\\t* 路线：西安"',
-    );
-    expect(onTokenMock).toHaveBeenCalledTimes(6);
-    expect(onCompletionMock).toHaveBeenCalledTimes(1);
-  });
-});
diff --git a/src/libs/agent-runtime/utils/streams/wenxin.ts b/src/libs/agent-runtime/utils/streams/wenxin.ts
deleted file mode 100644
index 418ede9e7f265..0000000000000
--- a/src/libs/agent-runtime/utils/streams/wenxin.ts
+++ /dev/null
@@ -1,38 +0,0 @@
-import { ChatStreamCallbacks } from '@/libs/agent-runtime';
-import { nanoid } from '@/utils/uuid';
-
-import { ChatResp } from '../../wenxin/type';
-import {
-  StreamProtocolChunk,
-  StreamStack,
-  createCallbacksTransformer,
-  createSSEProtocolTransformer,
-} from './protocol';
-
-const transformERNIEBotStream = (chunk: ChatResp): StreamProtocolChunk => {
-  const finished = chunk.is_end;
-  if (finished) {
-    return { data: chunk.finish_reason || 'stop', id: chunk.id, type: 'stop' };
-  }
-
-  if (chunk.result) {
-    return { data: chunk.result, id: chunk.id, type: 'text' };
-  }
-
-  return {
-    data: chunk,
-    id: chunk.id,
-    type: 'data',
-  };
-};
-
-export const WenxinStream = (
-  rawStream: ReadableStream<ChatResp>,
-  callbacks?: ChatStreamCallbacks,
-) => {
-  const streamStack: StreamStack = { id: 'chat_' + nanoid() };
-
-  return rawStream
-    .pipeThrough(createSSEProtocolTransformer(transformERNIEBotStream, streamStack))
-    .pipeThrough(createCallbacksTransformer(callbacks));
-};
diff --git a/src/libs/agent-runtime/wenxin/index.ts b/src/libs/agent-runtime/wenxin/index.ts
index f597fcf92d923..c324e9b9bc695 100644
--- a/src/libs/agent-runtime/wenxin/index.ts
+++ b/src/libs/agent-runtime/wenxin/index.ts
@@ -1,107 +1,10 @@
-import { ChatCompletion } from '@baiducloud/qianfan';
-
-// 如果引入了这个类型，那么在跑 type-check 的 tsc 检查中就会抛错，大无语
-// import type QianFanClient from '@baiducloud/qianfan/src/ChatCompletion/index';
-import { safeParseJSON } from '@/utils/safeParseJSON';
-
-import { LobeRuntimeAI } from '../BaseAI';
-import { AgentRuntimeErrorType } from '../error';
-import { ChatCompetitionOptions, ChatStreamPayload } from '../types';
-import { AgentRuntimeError } from '../utils/createError';
-import { debugStream } from '../utils/debugStream';
-import { StreamingResponse } from '../utils/response';
-import { convertIterableToStream } from '../utils/streams';
-import { WenxinStream } from '../utils/streams/wenxin';
-import { ChatResp } from './type';
-
-interface ChatErrorCode {
-  error_code: number;
-  error_msg: string;
-}
-
-export interface LobeWenxinAIParams {
-  accessKey?: string;
-  baseURL?: string;
-  secretKey?: string;
-}
-
-export class LobeWenxinAI implements LobeRuntimeAI {
-  private client: any;
-  baseURL?: string;
-
-  constructor({ accessKey, baseURL, secretKey }: LobeWenxinAIParams = {}) {
-    if (!accessKey || !secretKey)
-      throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey);
-
-    this.client = new ChatCompletion({
-      QIANFAN_ACCESS_KEY: accessKey,
-      QIANFAN_SECRET_KEY: secretKey,
-    });
-    this.baseURL = baseURL;
-  }
-
-  async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
-    try {
-      const result = await this.client.chat(
-        { messages: payload.messages as any, stream: true, user_id: options?.user },
-        payload.model,
-      );
-
-      const wenxinStream = convertIterableToStream(result as AsyncIterable<ChatResp>);
-
-      const [prod, useForDebug] = wenxinStream.tee();
-
-      if (process.env.DEBUG_WENXIN_CHAT_COMPLETION === '1') {
-        debugStream(useForDebug).catch();
-      }
-
-      const stream = WenxinStream(prod, options?.callback);
-
-      // Respond with the stream
-      return StreamingResponse(stream, { headers: options?.headers });
-    } catch (e) {
-      const err = e as Error;
-
-      const error: ChatErrorCode | undefined = safeParseJSON(err.message);
-
-      if (!error) {
-        throw AgentRuntimeError.createError(AgentRuntimeErrorType.AgentRuntimeError, {
-          message: err.message,
-          name: err.name,
-        });
-      }
-
-      // 文心一言错误码
-      // https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh
-      switch (error.error_code) {
-        // Invalid API key or access key
-        case 100:
-        case 13:
-        case 14: {
-          throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey, error);
-        }
-
-        // quota limit
-        case 4:
-        case 17:
-        case 18:
-        case 19:
-        case 336_501:
-        case 336_502:
-        case 336_503:
-        case 336_504:
-        case 336_505:
-        case 336_507: {
-          throw AgentRuntimeError.createError(AgentRuntimeErrorType.QuotaLimitReached, {
-            errorCode: error.error_code,
-            message: `${error.error_msg} | you can visit https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh for more information about the error code`,
-          });
-        }
-      }
-
-      throw AgentRuntimeError.createError(AgentRuntimeErrorType.ProviderBizError, error);
-    }
-  }
-}
-
-export default LobeWenxinAI;
+import { ModelProvider } from '../types';
+import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
+
+export const LobeWenxinAI = LobeOpenAICompatibleFactory({
+  baseURL: 'https://qianfan.baidubce.com/v2',
+  debug: {
+    chatCompletion: () => process.env.DEBUG_WENXIN_CHAT_COMPLETION === '1',
+  },
+  provider: ModelProvider.Wenxin,
+});
diff --git a/src/libs/agent-runtime/wenxin/type.ts b/src/libs/agent-runtime/wenxin/type.ts
deleted file mode 100644
index 63bafbba7e33e..0000000000000
--- a/src/libs/agent-runtime/wenxin/type.ts
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * token 用量基类
- */
-export interface TokenUsage {
-  /**
-   * 回答tokens数
-   */
-  completion_tokens?: number;
-  /**
-   * 问题tokens数
-   */
-  prompt_tokens: number;
-  /**
-   * tokens总数
-   */
-  total_tokens: number;
-}
-
-/**
- * 响应基类
- */
-export interface RespBase {
-  /**
-   * 时间戳
-   */
-  created: number;
-  /**
-   * 本轮对话的id
-   */
-  id: string;
-  /**
-   * 表示当前子句是否是最后一句。只有在流式接口模式下会返回该字段
-   */
-  is_end?: boolean;
-  /**
-   * 1：表示输入内容无安全风险
-   * 0：表示输入内容有安全风险
-   */
-  is_safe?: number;
-  /**
-   * 回包类型。
-   *
-   * chat.completion：多轮对话返回
-   */
-  object: string;
-  /**
-   * 对话返回结果
-   */
-  result: string;
-  /**
-   * 表示当前子句的序号。只有在流式接口模式下会返回该字段
-   */
-  sentence_id?: number;
-  /**
-   * token统计信息，token数 = 汉字数+单词数*1.3 （仅为估算逻辑）
-   */
-  usage: TokenUsage;
-}
-
-export interface ChatResp extends RespBase {
-  /**
-   * 当 need_clear_history 为 true 时，此字段会告知第几轮对话有敏感信息，如果是当前问题，ban_round=-1
-   */
-  ban_round: number;
-  /**
-   * 输出内容标识，说明：
-   * · normal：输出内容完全由大模型生成，未触发截断、替换
-   * · stop：输出结果命中入参stop中指定的字段后被截断
-   * · length：达到了最大的token数，根据EB返回结果is_truncated来截断
-   * · content_filter：输出内容被截断、兜底、替换为**等
-   */
-  finish_reason: string;
-  /**
-   * 当前生成的结果是否被截断
-   */
-  is_truncated?: boolean;
-  /**
-   * 表示用户输入是否存在安全，是否关闭当前会话，清理历史会话信息
-   *
-   * true：是，表示用户输入存在安全风险，建议关闭当前会话，清理历史会话信息
-   * false：否，表示用户输入无安全风险
-   */
-  need_clear_history: boolean;
-}
diff --git a/src/locales/default/modelProvider.ts b/src/locales/default/modelProvider.ts
index f86f94035e9b6..b554a822ce9ac 100644
--- a/src/locales/default/modelProvider.ts
+++ b/src/locales/default/modelProvider.ts
@@ -291,26 +291,6 @@ export default {
     tooltip: '更新服务商基础配置',
     updateSuccess: '更新成功',
   },
-  wenxin: {
-    accessKey: {
-      desc: '填入百度千帆平台的 Access Key',
-      placeholder: 'Qianfan Access Key',
-      title: 'Access Key',
-    },
-    checker: {
-      desc: '测试 AccessKey / SecretAccess 是否填写正确',
-    },
-    secretKey: {
-      desc: '填入百度千帆平台 Secret Key',
-      placeholder: 'Qianfan Secret Key',
-      title: 'Secret Key',
-    },
-    unlock: {
-      customRegion: '自定义服务区域',
-      description: '输入你的 AccessKey / SecretKey 即可开始会话。应用不会记录你的鉴权配置',
-      title: '使用自定义文心一言鉴权信息',
-    },
-  },
   zeroone: {
     title: '01.AI 零一万物',
   },
diff --git a/src/server/modules/AgentRuntime/index.test.ts b/src/server/modules/AgentRuntime/index.test.ts
index dc4601c59a798..1d853315d74da 100644
--- a/src/server/modules/AgentRuntime/index.test.ts
+++ b/src/server/modules/AgentRuntime/index.test.ts
@@ -25,7 +25,6 @@ import {
 } from '@/libs/agent-runtime';
 import { AgentRuntime } from '@/libs/agent-runtime';
 import { LobeStepfunAI } from '@/libs/agent-runtime/stepfun';
-import LobeWenxinAI from '@/libs/agent-runtime/wenxin';
 
 import { initAgentRuntimeWithUserPayload } from './index';
 
@@ -55,9 +54,6 @@ vi.mock('@/config/llm', () => ({
     TOGETHERAI_API_KEY: 'test-togetherai-key',
     QWEN_API_KEY: 'test-qwen-key',
     STEPFUN_API_KEY: 'test-stepfun-key',
-
-    WENXIN_ACCESS_KEY: 'test-wenxin-access-key',
-    WENXIN_SECRET_KEY: 'test-wenxin-secret-key',
   })),
 }));
 
@@ -207,16 +203,6 @@ describe('initAgentRuntimeWithUserPayload method', () => {
       expect(runtime['_runtime']).toBeInstanceOf(LobeStepfunAI);
     });
 
-    it.skip('Wenxin AI provider: with apikey', async () => {
-      const jwtPayload: JWTPayload = {
-        wenxinAccessKey: 'user-wenxin-accessKey',
-        wenxinSecretKey: 'wenxin-secret-key',
-      };
-      const runtime = await initAgentRuntimeWithUserPayload(ModelProvider.Wenxin, jwtPayload);
-      expect(runtime).toBeInstanceOf(AgentRuntime);
-      expect(runtime['_runtime']).toBeInstanceOf(LobeWenxinAI);
-    });
-
     it('Unknown Provider: with apikey and endpoint, should initialize to OpenAi', async () => {
       const jwtPayload: JWTPayload = {
         apiKey: 'user-unknown-key',
@@ -364,13 +350,6 @@ describe('initAgentRuntimeWithUserPayload method', () => {
       expect(runtime['_runtime']).toBeInstanceOf(LobeTogetherAI);
     });
 
-    it.skip('Wenxin AI provider: without apikey', async () => {
-      const jwtPayload = {};
-      const runtime = await initAgentRuntimeWithUserPayload(ModelProvider.Wenxin, jwtPayload);
-      expect(runtime).toBeInstanceOf(AgentRuntime);
-      expect(runtime['_runtime']).toBeInstanceOf(LobeWenxinAI);
-    });
-
     it('OpenAI provider: without apikey with OPENAI_PROXY_URL', async () => {
       process.env.OPENAI_PROXY_URL = 'https://proxy.example.com/v1';
 
diff --git a/src/services/_auth.ts b/src/services/_auth.ts
index 098f42959e66b..7a3fc2fa49c5e 100644
--- a/src/services/_auth.ts
+++ b/src/services/_auth.ts
@@ -9,7 +9,6 @@ import {
   AzureOpenAIKeyVault,
   CloudflareKeyVault,
   OpenAICompatibleKeyVault,
-  WenxinKeyVault,
 } from '@/types/user/settings';
 import { createJWT } from '@/utils/jwt';
 
@@ -18,7 +17,6 @@ export const getProviderAuthPayload = (
   keyVaults: OpenAICompatibleKeyVault &
     AzureOpenAIKeyVault &
     AWSBedrockKeyVault &
-    WenxinKeyVault &
     CloudflareKeyVault,
 ) => {
   switch (provider) {
@@ -47,18 +45,6 @@ export const getProviderAuthPayload = (
       };
     }
 
-    case ModelProvider.Wenxin: {
-      const { secretKey, accessKey } = keyVaults;
-
-      const apiKey = (accessKey || '') + (secretKey || '');
-
-      return {
-        apiKey,
-        wenxinAccessKey: accessKey,
-        wenxinSecretKey: secretKey,
-      };
-    }
-
     case ModelProvider.Azure: {
       return {
         apiKey: keyVaults.apiKey,
diff --git a/src/store/user/slices/modelList/selectors/keyVaults.ts b/src/store/user/slices/modelList/selectors/keyVaults.ts
index 684c95baa28b1..a564cfde4117f 100644
--- a/src/store/user/slices/modelList/selectors/keyVaults.ts
+++ b/src/store/user/slices/modelList/selectors/keyVaults.ts
@@ -14,7 +14,6 @@ export const keyVaultsSettings = (s: UserStore): UserKeyVaults =>
 
 const openAIConfig = (s: UserStore) => keyVaultsSettings(s).openai || {};
 const bedrockConfig = (s: UserStore) => keyVaultsSettings(s).bedrock || {};
-const wenxinConfig = (s: UserStore) => keyVaultsSettings(s).wenxin || {};
 const ollamaConfig = (s: UserStore) => keyVaultsSettings(s).ollama || {};
 const azureConfig = (s: UserStore) => keyVaultsSettings(s).azure || {};
 const cloudflareConfig = (s: UserStore) => keyVaultsSettings(s).cloudflare || {};
@@ -45,5 +44,4 @@ export const keyVaultsConfigSelectors = {
   ollamaConfig,
   openAIConfig,
   password,
-  wenxinConfig,
 };
diff --git a/src/types/aiProvider.ts b/src/types/aiProvider.ts
index a5d2ae9cd0bad..403a711c63d6a 100644
--- a/src/types/aiProvider.ts
+++ b/src/types/aiProvider.ts
@@ -23,7 +23,6 @@ export const AiProviderSDKEnum = {
   Huggingface: 'huggingface',
   Ollama: 'ollama',
   Openai: 'openai',
-  Wenxin: 'wenxin',
 } as const;
 
 export type AiProviderSDKType = (typeof AiProviderSDKEnum)[keyof typeof AiProviderSDKEnum];
diff --git a/src/types/user/settings/keyVaults.ts b/src/types/user/settings/keyVaults.ts
index 97c910c37bdd3..ec4ffc23e1e31 100644
--- a/src/types/user/settings/keyVaults.ts
+++ b/src/types/user/settings/keyVaults.ts
@@ -25,11 +25,6 @@ export interface CloudflareKeyVault {
   baseURLOrAccountID?: string;
 }
 
-export interface WenxinKeyVault {
-  accessKey?: string;
-  secretKey?: string;
-}
-
 export interface UserKeyVaults {
   ai21?: OpenAICompatibleKeyVault;
   ai360?: OpenAICompatibleKeyVault;
@@ -68,7 +63,7 @@ export interface UserKeyVaults {
   taichu?: OpenAICompatibleKeyVault;
   togetherai?: OpenAICompatibleKeyVault;
   upstage?: OpenAICompatibleKeyVault;
-  wenxin?: WenxinKeyVault;
+  wenxin?: OpenAICompatibleKeyVault;
   xai?: OpenAICompatibleKeyVault;
   zeroone?: OpenAICompatibleKeyVault;
   zhipu?: OpenAICompatibleKeyVault;
