diff --git a/src/config/aiModels/anthropic.ts b/src/config/aiModels/anthropic.ts
index f46a031d45d1b..4d3e232f5fb1d 100644
--- a/src/config/aiModels/anthropic.ts
+++ b/src/config/aiModels/anthropic.ts
@@ -5,6 +5,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     abilities: {
       functionCall: true,
       reasoning: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 200_000,
@@ -23,6 +24,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     releasedAt: '2025-05-23',
     settings: {
       extendParams: ['disableContextCaching', 'enableReasoning', 'reasoningBudgetToken'],
+      searchImpl: 'params',
     },
     type: 'chat',
   },
@@ -30,6 +32,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     abilities: {
       functionCall: true,
       reasoning: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 200_000,
@@ -48,6 +51,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     releasedAt: '2025-05-23',
     settings: {
       extendParams: ['disableContextCaching', 'enableReasoning', 'reasoningBudgetToken'],
+      searchImpl: 'params',
     },
     type: 'chat',
   },
@@ -55,6 +59,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     abilities: {
       functionCall: true,
       reasoning: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 200_000,
@@ -73,12 +78,14 @@ const anthropicChatModels: AIChatModelCard[] = [
     releasedAt: '2025-02-24',
     settings: {
       extendParams: ['disableContextCaching', 'enableReasoning', 'reasoningBudgetToken'],
+      searchImpl: 'params',
     },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 200_000,
@@ -96,6 +103,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     releasedAt: '2024-10-22',
     settings: {
       extendParams: ['disableContextCaching'],
+      searchImpl: 'params',
     },
     type: 'chat',
   },
@@ -119,6 +127,7 @@ const anthropicChatModels: AIChatModelCard[] = [
     releasedAt: '2024-06-20',
     settings: {
       extendParams: ['disableContextCaching'],
+      searchImpl: 'params',
     },
     type: 'chat',
   },
diff --git a/src/libs/model-runtime/anthropic/index.test.ts b/src/libs/model-runtime/anthropic/index.test.ts
index 64e0ff3f8395a..0282cfbeaba32 100644
--- a/src/libs/model-runtime/anthropic/index.test.ts
+++ b/src/libs/model-runtime/anthropic/index.test.ts
@@ -302,6 +302,64 @@ describe('LobeAnthropicAI', () => {
           { enabledContextCaching: true },
         );
       });
+
+      it('should build payload with tools and web search enabled', async () => {
+        const tools: ChatCompletionTool[] = [
+          { function: { name: 'tool1', description: 'desc1' }, type: 'function' }
+        ];
+
+        const mockAnthropicTools = [{ name: 'tool1', description: 'desc1' }];
+
+        vi.spyOn(anthropicHelpers, 'buildAnthropicTools').mockReturnValue(mockAnthropicTools as any);
+
+        const payload: ChatStreamPayload = {
+          messages: [{ content: 'Search and get info', role: 'user' }],
+          model: 'claude-3-haiku-20240307',
+          temperature: 0.5,
+          tools,
+          enabledSearch: true,
+        };
+
+        const result = await instance['buildAnthropicPayload'](payload);
+
+        expect(anthropicHelpers.buildAnthropicTools).toHaveBeenCalledWith(tools, {
+          enabledContextCaching: true,
+        });
+
+        // Should include both the converted tools and web search tool
+        expect(result.tools).toEqual([
+          ...mockAnthropicTools,
+          {
+            name: 'web_search',
+            type: 'web_search_20250305',
+          },
+        ]);
+      });
+
+      it('should build payload with web search enabled but no other tools', async () => {
+        vi.spyOn(anthropicHelpers, 'buildAnthropicTools').mockReturnValue(undefined);
+
+        const payload: ChatStreamPayload = {
+          messages: [{ content: 'Search for information', role: 'user' }],
+          model: 'claude-3-haiku-20240307',
+          temperature: 0.5,
+          enabledSearch: true,
+        };
+
+        const result = await instance['buildAnthropicPayload'](payload);
+
+        expect(anthropicHelpers.buildAnthropicTools).toHaveBeenCalledWith(undefined, {
+          enabledContextCaching: true,
+        });
+
+        // Should only include web search tool
+        expect(result.tools).toEqual([
+          {
+            name: 'web_search',
+            type: 'web_search_20250305',
+          },
+        ]);
+      });
     });
 
     describe('Error', () => {
diff --git a/src/libs/model-runtime/anthropic/index.ts b/src/libs/model-runtime/anthropic/index.ts
index 56b10a4b8cd1c..95567b2eeb7f0 100644
--- a/src/libs/model-runtime/anthropic/index.ts
+++ b/src/libs/model-runtime/anthropic/index.ts
@@ -23,6 +23,8 @@ export interface AnthropicModelCard {
   id: string;
 }
 
+type anthropicTools = Anthropic.Tool | Anthropic.WebSearchTool20250305;
+
 const modelsWithSmallContextWindow = new Set(['claude-3-opus-20240229', 'claude-3-haiku-20240307']);
 
 const DEFAULT_BASE_URL = 'https://api.anthropic.com';
@@ -45,7 +47,14 @@ export class LobeAnthropicAI implements LobeRuntimeAI {
   constructor({ apiKey, baseURL = DEFAULT_BASE_URL, id, ...res }: AnthropicAIParams = {}) {
     if (!apiKey) throw AgentRuntimeError.createError(AgentRuntimeErrorType.InvalidProviderAPIKey);
 
-    this.client = new Anthropic({ apiKey, baseURL, ...res });
+    const betaHeaders = process.env.ANTHROPIC_BETA_HEADERS;
+
+    this.client = new Anthropic({
+      apiKey,
+      baseURL,
+      ...(betaHeaders ? { defaultHeaders: { "anthropic-beta": betaHeaders } } : {}),
+      ...res
+    });
     this.baseURL = this.client.baseURL;
     this.apiKey = apiKey;
     this.id = id || ModelProvider.Anthropic;
@@ -99,6 +108,7 @@ export class LobeAnthropicAI implements LobeRuntimeAI {
       tools,
       thinking,
       enabledContextCaching = true,
+      enabledSearch,
     } = payload;
 
     const { default: anthropicModels } = await import('@/config/aiModels/anthropic');
@@ -127,7 +137,27 @@ export class LobeAnthropicAI implements LobeRuntimeAI {
 
     const postMessages = await buildAnthropicMessages(user_messages, { enabledContextCaching });
 
-    const postTools = buildAnthropicTools(tools, { enabledContextCaching });
+    let postTools: anthropicTools[] | undefined = buildAnthropicTools(tools, { enabledContextCaching });
+
+    if (enabledSearch) {
+      // Limit the number of searches per request
+      const maxUses = process.env.ANTHROPIC_MAX_USES;
+
+      const webSearchTool: Anthropic.WebSearchTool20250305 = {
+        name: 'web_search',
+        type: 'web_search_20250305',
+        ...(maxUses && Number.isInteger(Number(maxUses)) && Number(maxUses) > 0 && { 
+          max_uses: Number(maxUses) 
+        }),
+      };
+
+      // 如果已有工具，则添加到现有工具列表中；否则创建新的工具列表
+      if (postTools && postTools.length > 0) {
+        postTools = [...postTools, webSearchTool];
+      } else {
+        postTools = [webSearchTool];
+      }
+    }
 
     if (!!thinking && thinking.type === 'enabled') {
       const maxTokens = getMaxTokens() || 32_000; // Claude Opus 4 has minimum maxOutput
diff --git a/src/libs/model-runtime/utils/streams/anthropic.ts b/src/libs/model-runtime/utils/streams/anthropic.ts
index 597329e728dc2..756b2a72bccfa 100644
--- a/src/libs/model-runtime/utils/streams/anthropic.ts
+++ b/src/libs/model-runtime/utils/streams/anthropic.ts
@@ -1,7 +1,7 @@
 import Anthropic from '@anthropic-ai/sdk';
 import type { Stream } from '@anthropic-ai/sdk/streaming';
 
-import { ModelTokensUsage } from '@/types/message';
+import { ModelTokensUsage, CitationItem } from '@/types/message';
 
 import { ChatStreamCallbacks } from '../../types';
 import {
@@ -23,6 +23,7 @@ export const transformAnthropicStream = (
   switch (chunk.type) {
     case 'message_start': {
       context.id = chunk.message.id;
+      context.returnedCitationArray = [];
       let totalInputTokens = chunk.message.usage?.input_tokens;
 
       if (
@@ -59,6 +60,7 @@ export const transformAnthropicStream = (
           return { data: chunk.content_block.text, id: context.id, type: 'data' };
         }
 
+        case 'server_tool_use':
         case 'tool_use': {
           const toolChunk = chunk.content_block;
 
@@ -85,6 +87,29 @@ export const transformAnthropicStream = (
 
           return { data: [toolCall], id: context.id, type: 'tool_calls' };
         }
+
+        /*
+        case 'web_search_tool_result': {
+          const citations = chunk.content_block.content;
+
+          return [
+            {
+              data: {
+                citations: (citations as any[]).map(
+                  (item) =>
+                    ({
+                      title: item.title,
+                      url: item.url,
+                    }) as CitationItem,
+                ),
+              },
+              id: context.id,
+              type: 'grounding',
+            },
+          ];
+        }
+        */
+
         case 'thinking': {
           const thinkingChunk = chunk.content_block;
 
@@ -148,6 +173,19 @@ export const transformAnthropicStream = (
           };
         }
 
+        case 'citations_delta': {
+          const citations = (chunk as any).delta.citation;
+
+          if (context.returnedCitationArray) {
+            context.returnedCitationArray.push({
+              title: citations.title,
+              url: citations.url,
+            } as CitationItem)
+          }
+
+          return { data: null, id: context.id, type: 'text' };
+        }
+
         default: {
           break;
         }
@@ -180,7 +218,17 @@ export const transformAnthropicStream = (
     }
 
     case 'message_stop': {
-      return { data: 'message_stop', id: context.id, type: 'stop' };
+      return [
+        ...(context.returnedCitationArray?.length 
+          ? [{ 
+              data: { citations: context.returnedCitationArray }, 
+              id: context.id, 
+              type: 'grounding' 
+            }] 
+          : []
+        ),
+        { data: 'message_stop', id: context.id, type: 'stop' }
+      ] as any;
     }
 
     default: {
diff --git a/src/libs/model-runtime/utils/streams/protocol.ts b/src/libs/model-runtime/utils/streams/protocol.ts
index c6775b08f241c..219068b22c926 100644
--- a/src/libs/model-runtime/utils/streams/protocol.ts
+++ b/src/libs/model-runtime/utils/streams/protocol.ts
@@ -1,4 +1,4 @@
-import { ModelSpeed, ModelTokensUsage } from '@/types/message';
+import { CitationItem, ModelSpeed, ModelTokensUsage } from '@/types/message';
 import { safeParseJSON } from '@/utils/safeParseJSON';
 
 import { AgentRuntimeErrorType } from '../../error';
@@ -16,6 +16,13 @@ export interface StreamContext {
    * Same as Hunyuan and Wenxin
    */
   returnedCitation?: boolean;
+  /**
+   * Claude's citations are inline and interleaved with text output.
+   * Each text segment may carry references to sources (e.g., web search results) 
+   * relevant to that specific portion of the generated content.
+   * This array accumulates all citation items received during the streaming response.
+   */
+  returnedCitationArray?: CitationItem[];
   thinking?: {
     id: string;
     name: string;
