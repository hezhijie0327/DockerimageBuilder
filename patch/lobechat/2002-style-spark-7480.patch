diff --git a/src/config/aiModels/spark.ts b/src/config/aiModels/spark.ts
index 99f1990d858e4..97c7c3d78ed45 100644
--- a/src/config/aiModels/spark.ts
+++ b/src/config/aiModels/spark.ts
@@ -3,8 +3,21 @@ import { AIChatModelCard } from '@/types/aiModel';
 const sparkChatModels: AIChatModelCard[] = [
   {
     abilities: {
+      reasoning: true,
       search: true,
     },
+    contextWindowTokens: 32_768,
+    description:
+      'Spark X1 æ¨¡å‹å°†è¿›ä¸€æ­¥å‡çº§ï¼Œåœ¨åŸæ¥æ•°å­¦ä»»åŠ¡å›½å†…é¢†å…ˆåŸºç¡€ä¸Šï¼Œæ¨ç†ã€æ–‡æœ¬ç”Ÿæˆã€è¯­è¨€ç†è§£ç­‰é€šç”¨ä»»åŠ¡å®ç°æ•ˆæœå¯¹æ ‡ OpenAI o1 å’Œ DeepSeek R1ã€‚',
+    displayName: 'Spark X1',
+    id: 'x1',
+    maxOutput: 32_768,
+    settings: {
+      searchImpl: 'internal',
+    },
+    type: 'chat',
+  },
+  {
     contextWindowTokens: 8192,
     description:
       'Spark Lite æ˜¯ä¸€æ¬¾è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡æä½çš„å»¶è¿Ÿä¸é«˜æ•ˆçš„å¤„ç†èƒ½åŠ›ï¼Œå®Œå…¨å…è´¹å¼€æ”¾ï¼Œæ”¯æŒå®æ—¶åœ¨çº¿æœç´¢åŠŸèƒ½ã€‚å…¶å¿«é€Ÿå“åº”çš„ç‰¹æ€§ä½¿å…¶åœ¨ä½ç®—åŠ›è®¾å¤‡ä¸Šçš„æ¨ç†åº”ç”¨å’Œæ¨¡å‹å¾®è°ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºç”¨æˆ·å¸¦æ¥å‡ºè‰²çš„æˆæœ¬æ•ˆç›Šå’Œæ™ºèƒ½ä½“éªŒï¼Œå°¤å…¶åœ¨çŸ¥è¯†é—®ç­”ã€å†…å®¹ç”ŸæˆåŠæœç´¢åœºæ™¯ä¸‹è¡¨ç°ä¸ä¿—ã€‚',
@@ -12,9 +25,6 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'lite',
     maxOutput: 4096,
-    settings: {
-      searchImpl: 'internal',
-    },
     type: 'chat',
   },
   {
@@ -34,19 +44,12 @@ const sparkChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
-    abilities: {
-      search: true,
-    },
     contextWindowTokens: 131_072,
     description:
       'Spark Pro 128K é…ç½®äº†ç‰¹å¤§ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾128Kçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç‰¹åˆ«é€‚åˆéœ€é€šç¯‡åˆ†æå’Œé•¿æœŸé€»è¾‘å…³è”å¤„ç†çš„é•¿æ–‡å†…å®¹ï¼Œå¯åœ¨å¤æ‚æ–‡æœ¬æ²Ÿé€šä¸­æä¾›æµç•…ä¸€è‡´çš„é€»è¾‘ä¸å¤šæ ·çš„å¼•ç”¨æ”¯æŒã€‚',
     displayName: 'Spark Pro 128K',
-    enabled: true,
     id: 'pro-128k',
     maxOutput: 4096,
-    settings: {
-      searchImpl: 'internal',
-    },
     type: 'chat',
   },
   {
@@ -62,7 +65,7 @@ const sparkChatModels: AIChatModelCard[] = [
     id: 'generalv3.5',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
@@ -75,7 +78,6 @@ const sparkChatModels: AIChatModelCard[] = [
     description:
       'Spark Max 32K é…ç½®äº†å¤§ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œæ›´å¼ºçš„ä¸Šä¸‹æ–‡ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒ32K tokensçš„æ–‡æœ¬è¾“å…¥ï¼Œé€‚ç”¨äºé•¿æ–‡æ¡£é˜…è¯»ã€ç§æœ‰çŸ¥è¯†é—®ç­”ç­‰åœºæ™¯',
     displayName: 'Spark Max 32K',
-    enabled: true,
     id: 'max-32k',
     maxOutput: 8192,
     settings: {
@@ -96,7 +98,7 @@ const sparkChatModels: AIChatModelCard[] = [
     id: '4.0Ultra',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
diff --git a/src/config/modelProviders/spark.ts b/src/config/modelProviders/spark.ts
index dd1d705dafaeb..ffdeac6cd3be4 100644
--- a/src/config/modelProviders/spark.ts
+++ b/src/config/modelProviders/spark.ts
@@ -70,6 +70,9 @@ const Spark: ModelProviderCard = {
   name: 'Spark',
   settings: {
     modelEditable: false,
+    proxyUrl: {
+      placeholder: 'https://spark-api-open.xf-yun.com/v1',
+    },
     sdkType: 'openai',
     showModelFetcher: false,
     smoothing: {
diff --git a/src/libs/agent-runtime/spark/index.ts b/src/libs/agent-runtime/spark/index.ts
index 95d3f3e81d45a..3dfce3e844db1 100644
--- a/src/libs/agent-runtime/spark/index.ts
+++ b/src/libs/agent-runtime/spark/index.ts
@@ -1,4 +1,4 @@
-import { ModelProvider } from '../types';
+import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams';
@@ -6,6 +6,26 @@ import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams'
 export const LobeSparkAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://spark-api-open.xf-yun.com/v1',
   chatCompletion: {
+    handlePayload: (payload: ChatStreamPayload) => {
+      const { enabledSearch, tools, ...rest } = payload;
+
+      const sparkTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          type: "web_search",
+          web_search: {
+            enable: true,
+            search_mode: process.env.SPARK_SEARCH_MODE || "normal", // normal or deep
+            show_ref_label: true,
+          },
+        }
+      ] : tools;
+
+      return {
+        ...rest,
+        tools: sparkTools,
+      } as any;
+    },
     handleStream: SparkAIStream,
     handleTransformResponseToStream: transformSparkResponseToStream,
     noUserId: true,
diff --git a/src/libs/agent-runtime/utils/streams/spark.ts b/src/libs/agent-runtime/utils/streams/spark.ts
index ee74f424df317..c20dec9bb6f6e 100644
--- a/src/libs/agent-runtime/utils/streams/spark.ts
+++ b/src/libs/agent-runtime/utils/streams/spark.ts
@@ -11,6 +11,8 @@ import {
   generateToolCallId,
 } from './protocol';
 
+import { convertUsage } from '../usageConverter';
+
 export function transformSparkResponseToStream(data: OpenAI.ChatCompletion) {
   return new ReadableStream({
     start(controller) {
@@ -106,7 +108,27 @@ export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamP
     return { data: item.finish_reason, id: chunk.id, type: 'stop' };
   }
 
+  if (
+    item.delta &&
+    'reasoning_content' in item.delta &&
+    typeof item.delta.reasoning_content === 'string' &&
+    item.delta.reasoning_content !== ''
+  ) {
+    return { data: item.delta.reasoning_content, id: chunk.id, type: 'reasoning' };
+  }
+
   if (typeof item.delta?.content === 'string') {
+    /*
+    å¤„ç† v1 endpoint usageï¼Œæ··åˆåœ¨æœ€åä¸€ä¸ª content å†…å®¹ä¸­
+    {"code":0,"message":"Success","sid":"cha000d05ef@dx196553ae415b80a432","id":"cha000d05ef@dx196553ae415b80a432","created":1745186655,"choices":[{"delta":{"role":"assistant","content":"ğŸ˜Š"},"index":0}],"usage":{"prompt_tokens":1,"completion_tokens":418,"total_tokens":419}}
+    */
+    if (chunk.usage) {
+      return [
+        { data: item.delta.content, id: chunk.id, type: 'text' },
+        { data: convertUsage(chunk.usage), id: chunk.id, type: 'usage' },
+      ] as any;
+    }
+
     return { data: item.delta.content, id: chunk.id, type: 'text' };
   }
 
@@ -114,6 +136,11 @@ export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamP
     return { data: item.delta, id: chunk.id, type: 'data' };
   }
 
+  // å¤„ç† v2 endpoint usage
+  if (chunk.usage) {
+    return { data: convertUsage(chunk.usage), id: chunk.id, type: 'usage' };
+  }
+
   return {
     data: { delta: item.delta, id: chunk.id, index: item.index },
     id: chunk.id,
