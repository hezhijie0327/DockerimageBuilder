diff --git a/src/config/aiModels/spark.ts b/src/config/aiModels/spark.ts
index 99f1990d858e4..52bcdeb0f4f0f 100644
--- a/src/config/aiModels/spark.ts
+++ b/src/config/aiModels/spark.ts
@@ -3,8 +3,21 @@ import { AIChatModelCard } from '@/types/aiModel';
 const sparkChatModels: AIChatModelCard[] = [
   {
     abilities: {
+      reasoning: true,
       search: true,
     },
+    contextWindowTokens: 32_768,
+    description:
+      'Spark X1 æ¨¡å‹å°†è¿›ä¸€æ­¥å‡çº§ï¼Œåœ¨åŸæ¥æ•°å­¦ä»»åŠ¡å›½å†…é¢†å…ˆåŸºç¡€ä¸Šï¼Œæ¨ç†ã€æ–‡æœ¬ç”Ÿæˆã€è¯­è¨€ç†è§£ç­‰é€šç”¨ä»»åŠ¡å®ç°æ•ˆæœå¯¹æ ‡ OpenAI o1 å’Œ DeepSeek R1ã€‚',
+    displayName: 'Spark X1',
+    id: 'x1',
+    maxOutput: 32_768,
+    settings: {
+      searchImpl: 'params',
+    },
+    type: 'chat',
+  },
+  {
     contextWindowTokens: 8192,
     description:
       'Spark Lite æ˜¯ä¸€æ¬¾è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡æä½çš„å»¶è¿Ÿä¸é«˜æ•ˆçš„å¤„ç†èƒ½åŠ›ï¼Œå®Œå…¨å…è´¹å¼€æ”¾ï¼Œæ”¯æŒå®æ—¶åœ¨çº¿æœç´¢åŠŸèƒ½ã€‚å…¶å¿«é€Ÿå“åº”çš„ç‰¹æ€§ä½¿å…¶åœ¨ä½ç®—åŠ›è®¾å¤‡ä¸Šçš„æ¨ç†åº”ç”¨å’Œæ¨¡å‹å¾®è°ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºç”¨æˆ·å¸¦æ¥å‡ºè‰²çš„æˆæœ¬æ•ˆç›Šå’Œæ™ºèƒ½ä½“éªŒï¼Œå°¤å…¶åœ¨çŸ¥è¯†é—®ç­”ã€å†…å®¹ç”ŸæˆåŠæœç´¢åœºæ™¯ä¸‹è¡¨ç°ä¸ä¿—ã€‚',
@@ -12,9 +25,6 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'lite',
     maxOutput: 4096,
-    settings: {
-      searchImpl: 'internal',
-    },
     type: 'chat',
   },
   {
@@ -29,24 +39,17 @@ const sparkChatModels: AIChatModelCard[] = [
     id: 'generalv3',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
   {
-    abilities: {
-      search: true,
-    },
     contextWindowTokens: 131_072,
     description:
       'Spark Pro 128K é…ç½®äº†ç‰¹å¤§ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾128Kçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç‰¹åˆ«é€‚åˆéœ€é€šç¯‡åˆ†æå’Œé•¿æœŸé€»è¾‘å…³è”å¤„ç†çš„é•¿æ–‡å†…å®¹ï¼Œå¯åœ¨å¤æ‚æ–‡æœ¬æ²Ÿé€šä¸­æä¾›æµç•…ä¸€è‡´çš„é€»è¾‘ä¸å¤šæ ·çš„å¼•ç”¨æ”¯æŒã€‚',
     displayName: 'Spark Pro 128K',
-    enabled: true,
     id: 'pro-128k',
     maxOutput: 4096,
-    settings: {
-      searchImpl: 'internal',
-    },
     type: 'chat',
   },
   {
@@ -62,7 +65,7 @@ const sparkChatModels: AIChatModelCard[] = [
     id: 'generalv3.5',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
@@ -75,7 +78,6 @@ const sparkChatModels: AIChatModelCard[] = [
     description:
       'Spark Max 32K é…ç½®äº†å¤§ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œæ›´å¼ºçš„ä¸Šä¸‹æ–‡ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒ32K tokensçš„æ–‡æœ¬è¾“å…¥ï¼Œé€‚ç”¨äºé•¿æ–‡æ¡£é˜…è¯»ã€ç§æœ‰çŸ¥è¯†é—®ç­”ç­‰åœºæ™¯',
     displayName: 'Spark Max 32K',
-    enabled: true,
     id: 'max-32k',
     maxOutput: 8192,
     settings: {
@@ -96,7 +98,7 @@ const sparkChatModels: AIChatModelCard[] = [
     id: '4.0Ultra',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
diff --git a/src/config/modelProviders/spark.ts b/src/config/modelProviders/spark.ts
index dd1d705dafaeb..7ee7a5a065ad1 100644
--- a/src/config/modelProviders/spark.ts
+++ b/src/config/modelProviders/spark.ts
@@ -69,7 +69,11 @@ const Spark: ModelProviderCard = {
   modelsUrl: 'https://xinghuo.xfyun.cn/spark',
   name: 'Spark',
   settings: {
+    disableBrowserRequest: true,
     modelEditable: false,
+    proxyUrl: {
+      placeholder: 'https://spark-api-open.xf-yun.com/v1',
+    },
     sdkType: 'openai',
     showModelFetcher: false,
     smoothing: {
diff --git a/src/libs/agent-runtime/spark/index.test.ts b/src/libs/agent-runtime/spark/index.test.ts
index f22d440a2eb0d..bd2a23f49b976 100644
--- a/src/libs/agent-runtime/spark/index.test.ts
+++ b/src/libs/agent-runtime/spark/index.test.ts
@@ -13,4 +13,7 @@ testProvider({
   defaultBaseURL,
   chatDebugEnv: 'DEBUG_SPARK_CHAT_COMPLETION',
   chatModel: 'spark',
+  test: {
+    skipAPICall: true,
+  },
 });
diff --git a/src/libs/agent-runtime/spark/index.ts b/src/libs/agent-runtime/spark/index.ts
index 95d3f3e81d45a..c8a27070fb2a2 100644
--- a/src/libs/agent-runtime/spark/index.ts
+++ b/src/libs/agent-runtime/spark/index.ts
@@ -1,4 +1,4 @@
-import { ModelProvider } from '../types';
+import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams';
@@ -6,6 +6,28 @@ import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams'
 export const LobeSparkAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://spark-api-open.xf-yun.com/v1',
   chatCompletion: {
+    handlePayload: (payload: ChatStreamPayload) => {
+      const { enabledSearch, tools, ...rest } = payload;
+
+      const sparkTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          type: "web_search",
+          web_search: {
+            enable: true,
+            search_mode: process.env.SPARK_SEARCH_MODE || "normal", // normal or deep
+            /*
+            show_ref_label: true,
+            */
+          },
+        }
+      ] : tools;
+
+      return {
+        ...rest,
+        tools: sparkTools,
+      } as any;
+    },
     handleStream: SparkAIStream,
     handleTransformResponseToStream: transformSparkResponseToStream,
     noUserId: true,
diff --git a/src/libs/agent-runtime/utils/streams/spark.test.ts b/src/libs/agent-runtime/utils/streams/spark.test.ts
index 933df1dbb959f..8197acf035ed7 100644
--- a/src/libs/agent-runtime/utils/streams/spark.test.ts
+++ b/src/libs/agent-runtime/utils/streams/spark.test.ts
@@ -6,6 +6,72 @@ import { SparkAIStream, transformSparkResponseToStream } from './spark';
 describe('SparkAIStream', () => {
   beforeAll(() => {});
 
+  it('should handle reasoning content in stream', async () => {
+    const data = [
+      {
+        id: 'test-id',
+        object: 'chat.completion.chunk',
+        created: 1734395014,
+        model: 'x1',
+        choices: [
+          {
+            delta: {
+              reasoning_content: 'Hello',
+              role: 'assistant',
+            },
+            index: 0,
+            finish_reason: null,
+          },
+        ],
+      },
+      {
+        id: 'test-id',
+        object: 'chat.completion.chunk',
+        created: 1734395014,
+        model: 'x1',
+        choices: [
+          {
+            delta: {
+              reasoning_content: ' World',
+              role: 'assistant',
+            },
+            index: 0,
+            finish_reason: null,
+          },
+        ],
+      },
+    ]
+
+    const mockSparkStream = new ReadableStream({
+      start(controller) {
+        data.forEach((chunk) => {
+          controller.enqueue(chunk);
+        });
+
+        controller.close();
+      },
+    });
+
+    const protocolStream = SparkAIStream(mockSparkStream);
+
+    const decoder = new TextDecoder();
+    const chunks = [];
+
+    // @ts-ignore
+    for await (const chunk of protocolStream) {
+      chunks.push(decoder.decode(chunk, { stream: true }));
+    }
+
+    expect(chunks).toEqual([
+      'id: test-id\n',
+      'event: reasoning\n',
+      'data: "Hello"\n\n',
+      'id: test-id\n',
+      'event: reasoning\n',
+      'data: " World"\n\n',
+    ]);
+  });
+
   it('should transform non-streaming response to stream', async () => {
     const mockResponse = {
       id: 'cha000ceba6@dx193d200b580b8f3532',
diff --git a/src/libs/agent-runtime/utils/streams/spark.ts b/src/libs/agent-runtime/utils/streams/spark.ts
index ee74f424df317..834cd550e9414 100644
--- a/src/libs/agent-runtime/utils/streams/spark.ts
+++ b/src/libs/agent-runtime/utils/streams/spark.ts
@@ -11,11 +11,15 @@ import {
   generateToolCallId,
 } from './protocol';
 
+import { convertUsage } from '../usageConverter';
+
 export function transformSparkResponseToStream(data: OpenAI.ChatCompletion) {
   return new ReadableStream({
     start(controller) {
+      const choices = data?.choices || [];
+
       const chunk: OpenAI.ChatCompletionChunk = {
-        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => {
+        choices: choices.map((choice: OpenAI.ChatCompletion.Choice) => {
           const toolCallsArray = choice.message.tool_calls
             ? Array.isArray(choice.message.tool_calls)
               ? choice.message.tool_calls
@@ -49,7 +53,7 @@ export function transformSparkResponseToStream(data: OpenAI.ChatCompletion) {
       controller.enqueue(chunk);
 
       controller.enqueue({
-        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
+        choices: choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
           delta: {
             content: null,
             role: choice.message.role,
@@ -106,7 +110,27 @@ export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamP
     return { data: item.finish_reason, id: chunk.id, type: 'stop' };
   }
 
+  if (
+    item.delta &&
+    'reasoning_content' in item.delta &&
+    typeof item.delta.reasoning_content === 'string' &&
+    item.delta.reasoning_content !== ''
+  ) {
+    return { data: item.delta.reasoning_content, id: chunk.id, type: 'reasoning' };
+  }
+
   if (typeof item.delta?.content === 'string') {
+    /*
+    å¤„ç† v1 endpoint usageï¼Œæ··åˆåœ¨æœ€åä¸€ä¸ª content å†…å®¹ä¸­
+    {"code":0,"message":"Success","sid":"cha000d05ef@dx196553ae415b80a432","id":"cha000d05ef@dx196553ae415b80a432","created":1745186655,"choices":[{"delta":{"role":"assistant","content":"ğŸ˜Š"},"index":0}],"usage":{"prompt_tokens":1,"completion_tokens":418,"total_tokens":419}}
+    */
+    if (chunk.usage) {
+      return [
+        { data: item.delta.content, id: chunk.id, type: 'text' },
+        { data: convertUsage(chunk.usage), id: chunk.id, type: 'usage' },
+      ] as any;
+    }
+
     return { data: item.delta.content, id: chunk.id, type: 'text' };
   }
 
@@ -114,6 +138,11 @@ export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamP
     return { data: item.delta, id: chunk.id, type: 'data' };
   }
 
+  // å¤„ç† v2 endpoint usage
+  if (chunk.usage) {
+    return { data: convertUsage(chunk.usage), id: chunk.id, type: 'usage' };
+  }
+
   return {
     data: { delta: item.delta, id: chunk.id, index: item.index },
     id: chunk.id,
