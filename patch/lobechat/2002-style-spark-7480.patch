diff --git a/src/config/aiModels/spark.ts b/src/config/aiModels/spark.ts
index 99f1990d858e4..52bcdeb0f4f0f 100644
--- a/src/config/aiModels/spark.ts
+++ b/src/config/aiModels/spark.ts
@@ -3,8 +3,21 @@ import { AIChatModelCard } from '@/types/aiModel';
 const sparkChatModels: AIChatModelCard[] = [
   {
     abilities: {
+      reasoning: true,
       search: true,
     },
+    contextWindowTokens: 32_768,
+    description:
+      'Spark X1 模型将进一步升级，在原来数学任务国内领先基础上，推理、文本生成、语言理解等通用任务实现效果对标 OpenAI o1 和 DeepSeek R1。',
+    displayName: 'Spark X1',
+    id: 'x1',
+    maxOutput: 32_768,
+    settings: {
+      searchImpl: 'params',
+    },
+    type: 'chat',
+  },
+  {
     contextWindowTokens: 8192,
     description:
       'Spark Lite 是一款轻量级大语言模型，具备极低的延迟与高效的处理能力，完全免费开放，支持实时在线搜索功能。其快速响应的特性使其在低算力设备上的推理应用和模型微调中表现出色，为用户带来出色的成本效益和智能体验，尤其在知识问答、内容生成及搜索场景下表现不俗。',
@@ -12,9 +25,6 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'lite',
     maxOutput: 4096,
-    settings: {
-      searchImpl: 'internal',
-    },
     type: 'chat',
   },
   {
@@ -29,24 +39,17 @@ const sparkChatModels: AIChatModelCard[] = [
     id: 'generalv3',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
   {
-    abilities: {
-      search: true,
-    },
     contextWindowTokens: 131_072,
     description:
       'Spark Pro 128K 配置了特大上下文处理能力，能够处理多达128K的上下文信息，特别适合需通篇分析和长期逻辑关联处理的长文内容，可在复杂文本沟通中提供流畅一致的逻辑与多样的引用支持。',
     displayName: 'Spark Pro 128K',
-    enabled: true,
     id: 'pro-128k',
     maxOutput: 4096,
-    settings: {
-      searchImpl: 'internal',
-    },
     type: 'chat',
   },
   {
@@ -62,7 +65,7 @@ const sparkChatModels: AIChatModelCard[] = [
     id: 'generalv3.5',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
@@ -75,7 +78,6 @@ const sparkChatModels: AIChatModelCard[] = [
     description:
       'Spark Max 32K 配置了大上下文处理能力，更强的上下文理解和逻辑推理能力，支持32K tokens的文本输入，适用于长文档阅读、私有知识问答等场景',
     displayName: 'Spark Max 32K',
-    enabled: true,
     id: 'max-32k',
     maxOutput: 8192,
     settings: {
@@ -96,7 +98,7 @@ const sparkChatModels: AIChatModelCard[] = [
     id: '4.0Ultra',
     maxOutput: 8192,
     settings: {
-      searchImpl: 'internal',
+      searchImpl: 'params',
     },
     type: 'chat',
   },
diff --git a/src/config/modelProviders/spark.ts b/src/config/modelProviders/spark.ts
index dd1d705dafaeb..7ee7a5a065ad1 100644
--- a/src/config/modelProviders/spark.ts
+++ b/src/config/modelProviders/spark.ts
@@ -69,7 +69,11 @@ const Spark: ModelProviderCard = {
   modelsUrl: 'https://xinghuo.xfyun.cn/spark',
   name: 'Spark',
   settings: {
+    disableBrowserRequest: true,
     modelEditable: false,
+    proxyUrl: {
+      placeholder: 'https://spark-api-open.xf-yun.com/v1',
+    },
     sdkType: 'openai',
     showModelFetcher: false,
     smoothing: {
diff --git a/src/libs/agent-runtime/spark/index.test.ts b/src/libs/agent-runtime/spark/index.test.ts
index f22d440a2eb0d..bd2a23f49b976 100644
--- a/src/libs/agent-runtime/spark/index.test.ts
+++ b/src/libs/agent-runtime/spark/index.test.ts
@@ -13,4 +13,7 @@ testProvider({
   defaultBaseURL,
   chatDebugEnv: 'DEBUG_SPARK_CHAT_COMPLETION',
   chatModel: 'spark',
+  test: {
+    skipAPICall: true,
+  },
 });
diff --git a/src/libs/agent-runtime/spark/index.ts b/src/libs/agent-runtime/spark/index.ts
index 95d3f3e81d45a..c8a27070fb2a2 100644
--- a/src/libs/agent-runtime/spark/index.ts
+++ b/src/libs/agent-runtime/spark/index.ts
@@ -1,4 +1,4 @@
-import { ModelProvider } from '../types';
+import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams';
@@ -6,6 +6,28 @@ import { transformSparkResponseToStream, SparkAIStream } from '../utils/streams'
 export const LobeSparkAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://spark-api-open.xf-yun.com/v1',
   chatCompletion: {
+    handlePayload: (payload: ChatStreamPayload) => {
+      const { enabledSearch, tools, ...rest } = payload;
+
+      const sparkTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          type: "web_search",
+          web_search: {
+            enable: true,
+            search_mode: process.env.SPARK_SEARCH_MODE || "normal", // normal or deep
+            /*
+            show_ref_label: true,
+            */
+          },
+        }
+      ] : tools;
+
+      return {
+        ...rest,
+        tools: sparkTools,
+      } as any;
+    },
     handleStream: SparkAIStream,
     handleTransformResponseToStream: transformSparkResponseToStream,
     noUserId: true,
diff --git a/src/libs/agent-runtime/utils/streams/spark.test.ts b/src/libs/agent-runtime/utils/streams/spark.test.ts
index 933df1dbb959f..8197acf035ed7 100644
--- a/src/libs/agent-runtime/utils/streams/spark.test.ts
+++ b/src/libs/agent-runtime/utils/streams/spark.test.ts
@@ -6,6 +6,72 @@ import { SparkAIStream, transformSparkResponseToStream } from './spark';
 describe('SparkAIStream', () => {
   beforeAll(() => {});
 
+  it('should handle reasoning content in stream', async () => {
+    const data = [
+      {
+        id: 'test-id',
+        object: 'chat.completion.chunk',
+        created: 1734395014,
+        model: 'x1',
+        choices: [
+          {
+            delta: {
+              reasoning_content: 'Hello',
+              role: 'assistant',
+            },
+            index: 0,
+            finish_reason: null,
+          },
+        ],
+      },
+      {
+        id: 'test-id',
+        object: 'chat.completion.chunk',
+        created: 1734395014,
+        model: 'x1',
+        choices: [
+          {
+            delta: {
+              reasoning_content: ' World',
+              role: 'assistant',
+            },
+            index: 0,
+            finish_reason: null,
+          },
+        ],
+      },
+    ]
+
+    const mockSparkStream = new ReadableStream({
+      start(controller) {
+        data.forEach((chunk) => {
+          controller.enqueue(chunk);
+        });
+
+        controller.close();
+      },
+    });
+
+    const protocolStream = SparkAIStream(mockSparkStream);
+
+    const decoder = new TextDecoder();
+    const chunks = [];
+
+    // @ts-ignore
+    for await (const chunk of protocolStream) {
+      chunks.push(decoder.decode(chunk, { stream: true }));
+    }
+
+    expect(chunks).toEqual([
+      'id: test-id\n',
+      'event: reasoning\n',
+      'data: "Hello"\n\n',
+      'id: test-id\n',
+      'event: reasoning\n',
+      'data: " World"\n\n',
+    ]);
+  });
+
   it('should transform non-streaming response to stream', async () => {
     const mockResponse = {
       id: 'cha000ceba6@dx193d200b580b8f3532',
diff --git a/src/libs/agent-runtime/utils/streams/spark.ts b/src/libs/agent-runtime/utils/streams/spark.ts
index ee74f424df317..834cd550e9414 100644
--- a/src/libs/agent-runtime/utils/streams/spark.ts
+++ b/src/libs/agent-runtime/utils/streams/spark.ts
@@ -11,11 +11,15 @@ import {
   generateToolCallId,
 } from './protocol';
 
+import { convertUsage } from '../usageConverter';
+
 export function transformSparkResponseToStream(data: OpenAI.ChatCompletion) {
   return new ReadableStream({
     start(controller) {
+      const choices = data?.choices || [];
+
       const chunk: OpenAI.ChatCompletionChunk = {
-        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => {
+        choices: choices.map((choice: OpenAI.ChatCompletion.Choice) => {
           const toolCallsArray = choice.message.tool_calls
             ? Array.isArray(choice.message.tool_calls)
               ? choice.message.tool_calls
@@ -49,7 +53,7 @@ export function transformSparkResponseToStream(data: OpenAI.ChatCompletion) {
       controller.enqueue(chunk);
 
       controller.enqueue({
-        choices: data.choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
+        choices: choices.map((choice: OpenAI.ChatCompletion.Choice) => ({
           delta: {
             content: null,
             role: choice.message.role,
@@ -106,7 +110,27 @@ export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamP
     return { data: item.finish_reason, id: chunk.id, type: 'stop' };
   }
 
+  if (
+    item.delta &&
+    'reasoning_content' in item.delta &&
+    typeof item.delta.reasoning_content === 'string' &&
+    item.delta.reasoning_content !== ''
+  ) {
+    return { data: item.delta.reasoning_content, id: chunk.id, type: 'reasoning' };
+  }
+
   if (typeof item.delta?.content === 'string') {
+    /*
+    处理 v1 endpoint usage，混合在最后一个 content 内容中
+    {"code":0,"message":"Success","sid":"cha000d05ef@dx196553ae415b80a432","id":"cha000d05ef@dx196553ae415b80a432","created":1745186655,"choices":[{"delta":{"role":"assistant","content":"😊"},"index":0}],"usage":{"prompt_tokens":1,"completion_tokens":418,"total_tokens":419}}
+    */
+    if (chunk.usage) {
+      return [
+        { data: item.delta.content, id: chunk.id, type: 'text' },
+        { data: convertUsage(chunk.usage), id: chunk.id, type: 'usage' },
+      ] as any;
+    }
+
     return { data: item.delta.content, id: chunk.id, type: 'text' };
   }
 
@@ -114,6 +138,11 @@ export const transformSparkStream = (chunk: OpenAI.ChatCompletionChunk): StreamP
     return { data: item.delta, id: chunk.id, type: 'data' };
   }
 
+  // 处理 v2 endpoint usage
+  if (chunk.usage) {
+    return { data: convertUsage(chunk.usage), id: chunk.id, type: 'usage' };
+  }
+
   return {
     data: { delta: item.delta, id: chunk.id, index: item.index },
     id: chunk.id,
