diff --git a/src/libs/model-runtime/perplexity/index.test.ts b/src/libs/model-runtime/perplexity/index.test.ts
index 660c63b0f25c6..a546b426a460f 100644
--- a/src/libs/model-runtime/perplexity/index.test.ts
+++ b/src/libs/model-runtime/perplexity/index.test.ts
@@ -231,8 +231,8 @@ describe('LobePerplexityAI', () => {
       expect(noSpeedStream).toEqual(
         [
           'id: 506d64fb-e7f2-4d94-b80f-158369e9446d',
-          'event: text',
-          'data: "<think>"\n',
+          'event: reasoning',
+          'data: ""\n',
           'id: 506d64fb-e7f2-4d94-b80f-158369e9446d',
           'event: grounding',
           'data: {"citations":[{"title":"https://www.weather.com.cn/weather/101210101.shtml","url":"https://www.weather.com.cn/weather/101210101.shtml"},{"title":"https://tianqi.moji.com/weather/china/zhejiang/hangzhou","url":"https://tianqi.moji.com/weather/china/zhejiang/hangzhou"},{"title":"https://weather.cma.cn/web/weather/58457.html","url":"https://weather.cma.cn/web/weather/58457.html"},{"title":"https://tianqi.so.com/weather/101210101","url":"https://tianqi.so.com/weather/101210101"},{"title":"https://www.accuweather.com/zh/cn/hangzhou/106832/weather-forecast/106832","url":"https://www.accuweather.com/zh/cn/hangzhou/106832/weather-forecast/106832"},{"title":"https://www.hzqx.com","url":"https://www.hzqx.com"},{"title":"https://www.hzqx.com/pc/hztq/","url":"https://www.hzqx.com/pc/hztq/"}]}\n',
@@ -240,7 +240,7 @@ describe('LobePerplexityAI', () => {
           'event: text',
           'data: "杭州今"\n',
           'id: 506d64fb-e7f2-4d94-b80f-158369e9446d',
-          'event: text',
+          'event: reasoning',
           'data: "天和未来几天的"\n',
           'id: 506d64fb-e7f2-4d94-b80f-158369e9446d',
           'event: usage',
diff --git a/src/libs/model-runtime/utils/streams/ollama.test.ts b/src/libs/model-runtime/utils/streams/ollama.test.ts
index f841b599dafed..7a879d9d76ff1 100644
--- a/src/libs/model-runtime/utils/streams/ollama.test.ts
+++ b/src/libs/model-runtime/utils/streams/ollama.test.ts
@@ -7,6 +7,65 @@ import { OllamaStream } from './ollama';
 
 describe('OllamaStream', () => {
   describe('should transform Ollama stream to protocol stream', () => {
+    it('reasoning', async () => {
+      vi.spyOn(uuidModule, 'nanoid').mockReturnValueOnce('2');
+
+      const messages = [
+        '<think>',
+        '这是一个思考过程',
+        '，需要仔细分析问题。',
+        '</think>',
+        '根据分析，我的答案是：',
+        '这是最终答案。',
+      ];
+
+      const mockOllamaStream = new ReadableStream<ChatResponse>({
+        start(controller) {
+          messages.forEach((content) => {
+            controller.enqueue({ message: { content }, done: false } as ChatResponse);
+          });
+          controller.enqueue({ message: { content: '' }, done: true } as ChatResponse);
+          controller.close();
+        },
+      });
+
+      const protocolStream = OllamaStream(mockOllamaStream);
+    
+      const decoder = new TextDecoder();
+      const chunks = [];
+
+      // @ts-ignore
+      for await (const chunk of protocolStream) {
+        chunks.push(decoder.decode(chunk, { stream: true }));
+      }
+
+      expect(chunks).toEqual(
+        [
+          'id: chat_2',
+          'event: reasoning',
+          `data: ""\n`,
+          'id: chat_2',
+          'event: reasoning',
+          `data: "这是一个思考过程"\n`,
+          'id: chat_2',
+          'event: reasoning',
+          `data: "，需要仔细分析问题。"\n`,
+          'id: chat_2',
+          'event: text',
+          `data: ""\n`,
+          'id: chat_2',
+          'event: text',
+          `data: "根据分析，我的答案是："\n`,
+          'id: chat_2',
+          'event: text',
+          `data: "这是最终答案。"\n`,
+          'id: chat_2',
+          'event: stop',
+          `data: "finished"\n`,
+        ].map((line) => `${line}\n`)
+      );
+    });
+
     it('text', async () => {
       vi.spyOn(uuidModule, 'nanoid').mockReturnValueOnce('1');
 
diff --git a/src/libs/model-runtime/utils/streams/ollama.ts b/src/libs/model-runtime/utils/streams/ollama.ts
index 6e5575c7e5423..a68c54a73ce2c 100644
--- a/src/libs/model-runtime/utils/streams/ollama.ts
+++ b/src/libs/model-runtime/utils/streams/ollama.ts
@@ -32,7 +32,22 @@ const transformOllamaStream = (chunk: ChatResponse, stack: StreamContext): Strea
       type: 'tool_calls',
     };
   }
-  return { data: chunk.message.content, id: stack.id, type: 'text' };
+
+  // 处理 <think> & </think> 思考链，清除 <think> 标签和周围空白
+  const thinkingContent = chunk.message.content.replace(/<\/?think>/g, '');
+  // 判断是否有 <think> 或 </think> 标签，更新状态
+  if (chunk.message.content.includes('<think>')) {
+    stack.thinkingInContent = true;
+  } else if (chunk.message.content.includes('</think>')) {
+    stack.thinkingInContent = false;
+  }
+
+  // 返回类型根据当前思考模式确定
+  return {
+    data: thinkingContent,
+    id: stack.id,
+    type: stack?.thinkingInContent ? 'reasoning' : 'text',
+  };
 };
 
 export const OllamaStream = (
diff --git a/src/libs/model-runtime/utils/streams/openai.test.ts b/src/libs/model-runtime/utils/streams/openai.test.ts
index 32f03e3855dd5..3158f6a58179e 100644
--- a/src/libs/model-runtime/utils/streams/openai.test.ts
+++ b/src/libs/model-runtime/utils/streams/openai.test.ts
@@ -904,6 +904,169 @@ describe('OpenAIStream', () => {
   });
 
   describe('Reasoning', () => {
+    it('should handle <think></think> tags in streaming content', async () => {
+      const data = [
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '<think>' },
+              logprobs: null,
+              finish_reason: null,
+            },
+          ],
+        },
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '这是一个思考过程' },
+              logprobs: null,
+              finish_reason: null,
+            },
+          ],
+        },
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '，需要仔细分析问题。' },
+              logprobs: null,
+              finish_reason: null,
+            },
+          ],
+        },
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '</think>' },
+              logprobs: null,
+              finish_reason: null,
+            },
+          ],
+        },
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '根据分析，我的答案是：' },
+              logprobs: null,
+              finish_reason: null,
+            },
+          ],
+        },
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '这是最终答案。' },
+              logprobs: null,
+              finish_reason: null,
+            },
+          ],
+        },
+        {
+          id: '1',
+          object: 'chat.completion.chunk',
+          created: 1737563070,
+          model: 'deepseek-reasoner',
+          system_fingerprint: 'fp_1c5d8833bc',
+          choices: [
+            {
+              index: 0,
+              delta: { content: '' },
+              logprobs: null,
+              finish_reason: 'stop',
+            },
+          ],
+          usage: {
+            prompt_tokens: 10,
+            completion_tokens: 50,
+            total_tokens: 60,
+            prompt_tokens_details: { cached_tokens: 0 },
+            completion_tokens_details: { reasoning_tokens: 20 },
+            prompt_cache_hit_tokens: 0,
+            prompt_cache_miss_tokens: 10,
+          },
+        },
+      ];
+
+      const mockOpenAIStream = new ReadableStream({
+        start(controller) {
+          data.forEach((chunk) => {
+            controller.enqueue(chunk);
+          });
+          controller.close();
+        },
+      });
+
+      const protocolStream = OpenAIStream(mockOpenAIStream);
+      const decoder = new TextDecoder();
+      const chunks = [];
+
+      // @ts-ignore
+      for await (const chunk of protocolStream) {
+        chunks.push(decoder.decode(chunk, { stream: true }));
+      }
+
+      expect(chunks).toEqual(
+        [
+          'id: 1',
+          'event: reasoning',
+          `data: ""\n`,
+          'id: 1',
+          'event: reasoning',
+          `data: "这是一个思考过程"\n`,
+          'id: 1',
+          'event: reasoning',
+          `data: "，需要仔细分析问题。"\n`,
+          'id: 1',
+          'event: text',
+          `data: ""\n`,
+          'id: 1',
+          'event: text',
+          `data: "根据分析，我的答案是："\n`,
+          'id: 1',
+          'event: text',
+          `data: "这是最终答案。"\n`,
+          'id: 1',
+          'event: usage',
+          `data: {"inputCacheMissTokens":10,"inputTextTokens":10,"outputReasoningTokens":20,"outputTextTokens":30,"totalInputTokens":10,"totalOutputTokens":50,"totalTokens":60}\n`,
+        ].map((i) => `${i}\n`),
+      );
+    });
+
     it('should handle reasoning event in official DeepSeek api', async () => {
       const data = [
         {
diff --git a/src/libs/model-runtime/utils/streams/openai.ts b/src/libs/model-runtime/utils/streams/openai.ts
index fbb5ad1678f8f..ed408251b04ef 100644
--- a/src/libs/model-runtime/utils/streams/openai.ts
+++ b/src/libs/model-runtime/utils/streams/openai.ts
@@ -242,7 +242,21 @@ export const transformOpenAIStream = (
           }
         }
 
-        return { data: content, id: chunk.id, type: 'text' };
+        // 处理 <think> & </think> 思考链，清除 <think> 标签和周围空白
+        const thinkingContent = content.replace(/<\/?think>/g, '');
+        // 判断是否有 <think> 或 </think> 标签，更新状态
+        if (content.includes('<think>')) {
+          streamContext.thinkingInContent = true;
+        } else if (content.includes('</think>')) {
+          streamContext.thinkingInContent = false;
+        }
+
+        // 返回类型根据当前思考模式确定
+        return {
+          data: thinkingContent,
+          id: chunk.id,
+          type: streamContext?.thinkingInContent ? 'reasoning' : 'text',
+        };
       }
     }
 
diff --git a/src/libs/model-runtime/utils/streams/protocol.ts b/src/libs/model-runtime/utils/streams/protocol.ts
index a0e6f0832e03a..fe1df5db3274d 100644
--- a/src/libs/model-runtime/utils/streams/protocol.ts
+++ b/src/libs/model-runtime/utils/streams/protocol.ts
@@ -27,6 +27,18 @@ export interface StreamContext {
     id: string;
     name: string;
   };
+  /**
+   * Indicates whether the current state is within a "thinking" segment of the model output
+   * (e.g., when processing lmstudio responses).
+   *
+   * When parsing output containing <think> and </think> tags:
+   * - Set to `true` upon encountering a <think> tag (entering reasoning mode)
+   * - Set to `false` upon encountering a </think> tag (exiting reasoning mode)
+   *
+   * While `thinkingInContent` is `true`, subsequent content should be stored in `reasoning_content`.
+   * When `false`, content should be stored in the regular `content` field.
+   */
+  thinkingInContent?: boolean;
   tool?: {
     id: string;
     index: number;
