diff --git a/src/app/[variants]/(main)/settings/llm/ProviderList/providers.tsx b/src/app/[variants]/(main)/settings/llm/ProviderList/providers.tsx
index 6d503a1aae6d6..05176e79fcddc 100644
--- a/src/app/[variants]/(main)/settings/llm/ProviderList/providers.tsx
+++ b/src/app/[variants]/(main)/settings/llm/ProviderList/providers.tsx
@@ -5,6 +5,7 @@ import {
   Ai360ProviderCard,
   AnthropicProviderCard,
   BaichuanProviderCard,
+  CohereProviderCard,
   DeepSeekProviderCard,
   FireworksAIProviderCard,
   GiteeAIProviderCard,
@@ -82,6 +83,7 @@ export const useProviderList = (): ProviderItem[] => {
       XAIProviderCard,
       JinaProviderCard,
       SambaNovaProviderCard,
+      CohereProviderCard,
       QwenProviderCard,
       WenxinProviderCard,
       HunyuanProviderCard,
diff --git a/src/config/aiModels/cohere.ts b/src/config/aiModels/cohere.ts
new file mode 100644
index 0000000000000..8699237bb4b80
--- /dev/null
+++ b/src/config/aiModels/cohere.ts
@@ -0,0 +1,237 @@
+import { AIChatModelCard } from '@/types/aiModel';
+
+const cohereChatModels: AIChatModelCard[] = [
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 256_000,
+    description: 'Command A is our most performant model to date, excelling at tool use, agents, retrieval augmented generation (RAG), and multilingual use cases. Command A has a context length of 256K, only requires two GPUs to run, and has 150% higher throughput compared to Command R+ 08-2024.',
+    displayName: 'Command A',
+    enabled: true,
+    id: 'command-a-03-2025',
+    maxOutput: 8000,
+    pricing: {
+      input: 2.5,
+      output: 10
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'command-r-plus is an alias for command-r-plus-04-2024, so if you use command-r-plus in the API, that’s the model you’re pointing to.',
+    displayName: 'Command R+',
+    enabled: true,
+    id: 'command-r-plus',
+    maxOutput: 4000,
+    pricing: {
+      input: 2.5,
+      output: 10
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'Command R+ is an instruction-following conversational model that performs language tasks at a higher quality, more reliably, and with a longer context than previous models. It is best suited for complex RAG workflows and multi-step tool use.',
+    displayName: 'Command R+ 04-2024',
+    id: 'command-r-plus-04-2024',
+    maxOutput: 4000,
+    pricing: {
+      input: 3,
+      output: 15
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'command-r is an alias for command-r-03-2024, so if you use command-r in the API, that’s the model you’re pointing to.',
+    displayName: 'Command R',
+    enabled: true,
+    id: 'command-r',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.15,
+      output: 0.6
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'command-r-08-2024 is an update of the Command R model, delivered in August 2024.',
+    displayName: 'Command R 08-2024',
+    id: 'command-r-08-2024',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.15,
+      output: 0.6
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'Command R is an instruction-following conversational model that performs language tasks at a higher quality, more reliably, and with a longer context than previous models. It can be used for complex workflows like code generation, retrieval augmented generation (RAG), tool use, and agents.',
+    displayName: 'Command R 03-2024',
+    id: 'command-r-03-2024',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.5,
+      output: 1.5
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'command-r7b-12-2024 is a small, fast update delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps.',
+    displayName: 'Command R7B 12-2024',
+    enabled: true,
+    id: 'command-r7b-12-2024',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.0375,
+      output: 0.15
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 4000,
+    description: 'An instruction-following conversational model that performs language tasks with high quality, more reliably and with a longer context than our base generative models.',
+    displayName: 'Command',
+    enabled: true,
+    id: 'command',
+    maxOutput: 4000,
+    pricing: {
+      input: 1,
+      output: 2
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 128_000,
+    description: 'To reduce the time between major releases, we put out nightly versions of command models. For command, that is command-nightly. Be advised that command-nightly is the latest, most experimental, and (possibly) unstable version of its default counterpart. Nightly releases are updated regularly, without warning, and are not recommended for production use.',
+    displayName: 'Command Nightly',
+    id: 'command-nightly',
+    maxOutput: 4000,
+    pricing: {
+      input: 1,
+      output: 2
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 4000,
+    description: 'A smaller, faster version of command. Almost as capable, but a lot faster.',
+    displayName: 'Command Light',
+    enabled: true,
+    id: 'command-light',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.3,
+      output: 0.6
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 4000,
+    description: 'To reduce the time between major releases, we put out nightly versions of command models. For command-light, that is command-light-nightly. Be advised that command-light-nightly is the latest, most experimental, and (possibly) unstable version of its default counterpart. Nightly releases are updated regularly, without warning, and are not recommended for production use.',
+    displayName: 'Command Light Nightly',
+    id: 'command-light-nightly',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.3,
+      output: 0.6
+    },
+    type: 'chat'
+  },
+  {
+    contextWindowTokens: 128_000,
+    description: 'Aya Expanse is a highly performant 32B multilingual model, designed to rival monolingual performance through innovations in instruction tuning with data arbitrage, preference training, and model merging. Serves 23 languages.',
+    displayName: 'Aya Expanse 32B',
+    enabled: true,
+    id: 'c4ai-aya-expanse-32b',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.5,
+      output: 1.5
+    },
+    type: 'chat'
+  },
+  {
+    contextWindowTokens: 8000,
+    description: 'Aya Expanse is a highly performant 8B multilingual model, designed to rival monolingual performance through innovations in instruction tuning with data arbitrage, preference training, and model merging. Serves 23 languages.',
+    displayName: 'Aya Expanse 8B',
+    enabled: true,
+    id: 'c4ai-aya-expanse-8b',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.5,
+      output: 1.5
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      vision: true,
+    },
+    contextWindowTokens: 16_000,
+    description: 'Aya Vision is a state-of-the-art multimodal model excelling at a variety of critical benchmarks for language, text, and image capabilities. Serves 23 languages. This 32 billion parameter variant is focused on state-of-art multilingual performance.',
+    displayName: 'Aya Vision 32B',
+    enabled: true,
+    id: 'c4ai-aya-vision-32b',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.5,
+      output: 1.5
+    },
+    type: 'chat'
+  },
+  {
+    abilities: {
+      vision: true,
+    },
+    contextWindowTokens: 16_000,
+    description: 'Aya Vision is a state-of-the-art multimodal model excelling at a variety of critical benchmarks for language, text, and image capabilities. This 8 billion parameter variant is focused on low latency and best-in-class performance.',
+    displayName: 'Aya Vision 8B',
+    enabled: true,
+    id: 'c4ai-aya-vision-8b',
+    maxOutput: 4000,
+    pricing: {
+      input: 0.5,
+      output: 1.5
+    },
+    type: 'chat'
+  },
+]
+
+export const allModels = [...cohereChatModels];
+
+export default allModels;
diff --git a/src/config/aiModels/index.ts b/src/config/aiModels/index.ts
index 7cd0ed49c669d..da4936415d762 100644
--- a/src/config/aiModels/index.ts
+++ b/src/config/aiModels/index.ts
@@ -8,6 +8,7 @@ import { default as azureai } from './azureai';
 import { default as baichuan } from './baichuan';
 import { default as bedrock } from './bedrock';
 import { default as cloudflare } from './cloudflare';
+import { default as cohere } from './cohere';
 import { default as deepseek } from './deepseek';
 import { default as doubao } from './doubao';
 import { default as fireworksai } from './fireworksai';
@@ -77,6 +78,7 @@ export const LOBE_DEFAULT_MODEL_LIST = buildDefaultModelList({
   baichuan,
   bedrock,
   cloudflare,
+  cohere,
   deepseek,
   doubao,
   fireworksai,
@@ -127,6 +129,7 @@ export { default as azureai } from './azureai';
 export { default as baichuan } from './baichuan';
 export { default as bedrock } from './bedrock';
 export { default as cloudflare } from './cloudflare';
+export { default as cohere } from './cohere';
 export { default as deepseek } from './deepseek';
 export { default as doubao } from './doubao';
 export { default as fireworksai } from './fireworksai';
diff --git a/src/config/llm.ts b/src/config/llm.ts
index c4e1411c454a5..f0637da3d8ba9 100644
--- a/src/config/llm.ts
+++ b/src/config/llm.ts
@@ -150,6 +150,9 @@ export const getLLMConfig = () => {
 
       ENABLED_PPIO: z.boolean(),
       PPIO_API_KEY: z.string().optional(),
+
+      ENABLED_COHERE: z.boolean(),
+      COHERE_API_KEY: z.string().optional(),
     },
     runtimeEnv: {
       API_KEY_SELECT_MODE: process.env.API_KEY_SELECT_MODE,
@@ -298,6 +301,9 @@ export const getLLMConfig = () => {
 
       ENABLED_PPIO: !!process.env.PPIO_API_KEY,
       PPIO_API_KEY: process.env.PPIO_API_KEY,
+
+      ENABLED_COHERE: !!process.env.COHERE_API_KEY,
+      COHERE_API_KEY: process.env.COHERE_API_KEY,
     },
   });
 };
diff --git a/src/config/modelProviders/cohere.ts b/src/config/modelProviders/cohere.ts
new file mode 100644
index 0000000000000..03856b349f339
--- /dev/null
+++ b/src/config/modelProviders/cohere.ts
@@ -0,0 +1,21 @@
+import { ModelProviderCard } from '@/types/llm';
+
+const Cohere: ModelProviderCard = {
+  chatModels: [],
+  checkModel: 'command-r7b-12-2024',
+  description: 'Cohere',
+  //disableBrowserRequest: true,
+  id: 'cohere',
+  modelsUrl: 'https://docs.cohere.com/v2/docs/models',
+  name: 'Cohere',
+  settings: {
+    //disableBrowserRequest: true,
+    proxyUrl: {
+      placeholder: 'https://api.cohere.ai/compatibility/v1',
+    },
+    sdkType: 'openai',
+  },
+  url: 'https://cohere.com',
+};
+
+export default Cohere;
diff --git a/src/config/modelProviders/index.ts b/src/config/modelProviders/index.ts
index 84b8d701264e2..f8271ecb7a21d 100644
--- a/src/config/modelProviders/index.ts
+++ b/src/config/modelProviders/index.ts
@@ -8,6 +8,7 @@ import AzureAIProvider from './azureai';
 import BaichuanProvider from './baichuan';
 import BedrockProvider from './bedrock';
 import CloudflareProvider from './cloudflare';
+import CohereProvider from './cohere';
 import DeepSeekProvider from './deepseek';
 import DoubaoProvider from './doubao';
 import FireworksAIProvider from './fireworksai';
@@ -75,6 +76,7 @@ export const LOBE_DEFAULT_MODEL_LIST: ChatModelCard[] = [
   XAIProvider.chatModels,
   JinaProvider.chatModels,
   SambaNovaProvider.chatModels,
+  CohereProvider.chatModels,
   ZeroOneProvider.chatModels,
   StepfunProvider.chatModels,
   NovitaProvider.chatModels,
@@ -124,6 +126,7 @@ export const DEFAULT_MODEL_PROVIDER_LIST = [
   XAIProvider,
   JinaProvider,
   SambaNovaProvider,
+  CohereProvider,
   QwenProvider,
   WenxinProvider,
   TencentcloudProvider,
@@ -164,6 +167,7 @@ export { default as AzureAIProviderCard } from './azureai';
 export { default as BaichuanProviderCard } from './baichuan';
 export { default as BedrockProviderCard } from './bedrock';
 export { default as CloudflareProviderCard } from './cloudflare';
+export { default as CohereProviderCard } from './cohere';
 export { default as DeepSeekProviderCard } from './deepseek';
 export { default as DoubaoProviderCard } from './doubao';
 export { default as FireworksAIProviderCard } from './fireworksai';
diff --git a/src/libs/agent-runtime/cohere/index.ts b/src/libs/agent-runtime/cohere/index.ts
new file mode 100644
index 0000000000000..49aee416bdc7c
--- /dev/null
+++ b/src/libs/agent-runtime/cohere/index.ts
@@ -0,0 +1,48 @@
+import { ModelProvider } from '../types';
+import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
+
+import type { ChatModelCard } from '@/types/llm';
+
+export interface CohereModelCard {
+  context_length: number;
+  features: string[];
+  name: string;
+  supports_vision: boolean;
+}
+
+export const LobeCohereAI = LobeOpenAICompatibleFactory({
+  baseURL: 'https://api.cohere.ai/compatibility/v1',
+  debug: {
+    chatCompletion: () => process.env.DEBUG_COHERE_CHAT_COMPLETION === '1',
+  },
+  models: async ({ client }) => {
+    const { LOBE_DEFAULT_MODEL_LIST } = await import('@/config/aiModels');
+
+    client.baseURL = 'https://api.cohere.com/v1';
+
+    const modelsPage = await client.models.list() as any;
+    const modelList: CohereModelCard[] = modelsPage.models;
+
+    return modelList
+      .map((model) => {
+        const knownModel = LOBE_DEFAULT_MODEL_LIST.find((m) => model.name.toLowerCase() === m.id.toLowerCase());
+
+        return {
+          contextWindowTokens: model.context_length,
+          displayName: knownModel?.displayName ?? undefined,
+          enabled: knownModel?.enabled || false,
+          functionCall:
+            model.features.includes("tools")
+            || knownModel?.abilities?.functionCall
+            || false,
+          id: model.name,
+          vision:
+            model.supports_vision
+            || knownModel?.abilities?.vision
+            || false,
+        };
+      })
+      .filter(Boolean) as ChatModelCard[];
+  },
+  provider: ModelProvider.Cohere,
+});
diff --git a/src/libs/agent-runtime/runtimeMap.ts b/src/libs/agent-runtime/runtimeMap.ts
index fbe1d080070cb..e5d2b55192bbd 100644
--- a/src/libs/agent-runtime/runtimeMap.ts
+++ b/src/libs/agent-runtime/runtimeMap.ts
@@ -6,6 +6,7 @@ import { LobeAzureAI } from './azureai';
 import { LobeBaichuanAI } from './baichuan';
 import LobeBedrockAI from './bedrock';
 import { LobeCloudflareAI } from './cloudflare';
+import { LobeCohereAI } from './cohere';
 import { LobeDeepSeekAI } from './deepseek';
 import { LobeFireworksAI } from './fireworksai';
 import { LobeGiteeAI } from './giteeai';
@@ -54,6 +55,7 @@ export const providerRuntimeMap = {
   baichuan: LobeBaichuanAI,
   bedrock: LobeBedrockAI,
   cloudflare: LobeCloudflareAI,
+  cohere: LobeCohereAI,
   deepseek: LobeDeepSeekAI,
   doubao: LobeVolcengineAI,
   fireworksai: LobeFireworksAI,
diff --git a/src/libs/agent-runtime/types/type.ts b/src/libs/agent-runtime/types/type.ts
index 80ca857f49f29..c64a002e1677f 100644
--- a/src/libs/agent-runtime/types/type.ts
+++ b/src/libs/agent-runtime/types/type.ts
@@ -30,6 +30,7 @@ export enum ModelProvider {
   Baichuan = 'baichuan',
   Bedrock = 'bedrock',
   Cloudflare = 'cloudflare',
+  Cohere = 'cohere',
   DeepSeek = 'deepseek',
   /**
    * @deprecated
diff --git a/src/types/user/settings/keyVaults.ts b/src/types/user/settings/keyVaults.ts
index bf0abcb21fe7f..2bac7c43c5130 100644
--- a/src/types/user/settings/keyVaults.ts
+++ b/src/types/user/settings/keyVaults.ts
@@ -41,6 +41,7 @@ export interface UserKeyVaults extends SearchEngineKeyVaults {
   baichuan?: OpenAICompatibleKeyVault;
   bedrock?: AWSBedrockKeyVault;
   cloudflare?: CloudflareKeyVault;
+  cohere?: OpenAICompatibleKeyVault;
   deepseek?: OpenAICompatibleKeyVault;
   doubao?: OpenAICompatibleKeyVault;
   fireworksai?: OpenAICompatibleKeyVault;
