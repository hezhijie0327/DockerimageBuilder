diff --git a/src/config/aiModels/wenxin.ts b/src/config/aiModels/wenxin.ts
index cd193788f9e53..431a8e2dc2c77 100644
--- a/src/config/aiModels/wenxin.ts
+++ b/src/config/aiModels/wenxin.ts
@@ -4,6 +4,7 @@ const wenxinChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -16,11 +17,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 0.8,
       output: 2,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -32,11 +37,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 0.8,
       output: 2,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description:
@@ -49,11 +58,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 0.8,
       output: 2,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -66,11 +79,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 30,
       output: 90,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -82,11 +99,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 30,
       output: 90,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -99,11 +120,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 20,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description:
@@ -116,11 +141,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 20,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -132,6 +161,9 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 20,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/libs/agent-runtime/hunyuan/index.ts b/src/libs/agent-runtime/hunyuan/index.ts
index 728e43157dcde..27d3f2c5ab490 100644
--- a/src/libs/agent-runtime/hunyuan/index.ts
+++ b/src/libs/agent-runtime/hunyuan/index.ts
@@ -16,13 +16,13 @@ export const LobeHunyuanAI = LobeOpenAICompatibleFactory({
       return {
         ...rest,
         ...(enabledSearch && {
-          /*
           citation: true,
+          enable_enhancement: true,
+          /*
           enable_multimedia: true,
-          search_info: true
           */
-          enable_enhancement: true,
           enable_speed_search: process.env.HUNYUAN_ENABLE_SPEED_SEARCH === '1',
+          search_info: true,
         }),
       } as any;
     },
diff --git a/src/libs/agent-runtime/utils/streams/openai.ts b/src/libs/agent-runtime/utils/streams/openai.ts
index 3cac3e173f294..e07b359626232 100644
--- a/src/libs/agent-runtime/utils/streams/openai.ts
+++ b/src/libs/agent-runtime/utils/streams/openai.ts
@@ -127,19 +127,35 @@ export const transformOpenAIStream = (
       }
 
       if (typeof content === 'string') {
-        // in Perplexity api, the citation is in every chunk, but we only need to return it once
-
-        if ('citations' in chunk && !!chunk.citations && !streamContext?.returnedPplxCitation) {
-          streamContext.returnedPplxCitation = true;
-
-          const citations = (chunk.citations as any[]).map((item) =>
-            typeof item === 'string' ? ({ title: item, url: item } as CitationItem) : item,
-          );
-
-          return [
-            { data: { citations }, id: chunk.id, type: 'grounding' },
-            { data: content, id: chunk.id, type: 'text' },
-          ];
+        if (!streamContext?.returnedCitation) {
+          const citations =
+            // in Perplexity api, the citation is in every chunk, but we only need to return it once
+            ('citations' in chunk && chunk.citations) ||
+            // in Hunyuan api, the citation is in every chunk
+            ('search_info' in chunk && (chunk.search_info as any)?.search_results) ||
+            // in Wenxin api, the citation is in the first and last chunk
+            ('search_results' in chunk && chunk.search_results);
+
+          if (citations) {
+            streamContext.returnedCitation = true;
+
+            return [
+              {
+                data: {
+                  citations: (citations as any[]).map(
+                    (item) =>
+                      ({
+                        title: typeof item === 'string' ? item : item.title,
+                        url: typeof item === 'string' ? item : item.url,
+                      }) as CitationItem
+                  ),
+                },
+                id: chunk.id,
+                type: 'grounding',
+              },
+              { data: content, id: chunk.id, type: 'text' },
+            ];
+          }
         }
 
         return { data: content, id: chunk.id, type: 'text' };
diff --git a/src/libs/agent-runtime/utils/streams/protocol.ts b/src/libs/agent-runtime/utils/streams/protocol.ts
index fb345e070a805..c362e114e2f19 100644
--- a/src/libs/agent-runtime/utils/streams/protocol.ts
+++ b/src/libs/agent-runtime/utils/streams/protocol.ts
@@ -9,9 +9,10 @@ export interface StreamContext {
   id: string;
   /**
    * As pplx citations is in every chunk, but we only need to return it once
-   * this flag is used to check if the pplx citation is returned,and then not return it again
+   * this flag is used to check if the pplx citation is returned,and then not return it again.
+   * Same as Hunyuan and Wenxin
    */
-  returnedPplxCitation?: boolean;
+  returnedCitation?: boolean;
   thinking?: {
     id: string;
     name: string;
diff --git a/src/libs/agent-runtime/wenxin/index.ts b/src/libs/agent-runtime/wenxin/index.ts
index c324e9b9bc695..85bed26c08c5b 100644
--- a/src/libs/agent-runtime/wenxin/index.ts
+++ b/src/libs/agent-runtime/wenxin/index.ts
@@ -3,6 +3,22 @@ import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 export const LobeWenxinAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://qianfan.baidubce.com/v2',
+  chatCompletion: {
+    handlePayload: (payload) => {
+      const { enabledSearch, ...rest } = payload;
+
+      return {
+        ...rest,
+        ...(enabledSearch && {
+          web_search: {
+            enable: true,
+            enable_citation: true,
+            enable_trace: true,
+          }
+        }),
+      } as any;
+    },
+  },
   debug: {
     chatCompletion: () => process.env.DEBUG_WENXIN_CHAT_COMPLETION === '1',
   },
