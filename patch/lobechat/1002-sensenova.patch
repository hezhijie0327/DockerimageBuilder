diff --git a/package.json b/package.json
index 8b3cb670c2ce..138e8c710ad8 100644
--- a/package.json
+++ b/package.json
@@ -135,6 +135,7 @@
     "@trpc/next": "next",
     "@trpc/react-query": "next",
     "@trpc/server": "next",
+    "@types/crypto-js": "^4.2.0",
     "@vercel/analytics": "^1.3.1",
     "@vercel/edge-config": "^1.2.1",
     "@vercel/speed-insights": "^1.0.12",
diff --git a/src/app/(main)/settings/llm/ProviderList/SenseNova/index.tsx b/src/app/(main)/settings/llm/ProviderList/SenseNova/index.tsx
new file mode 100644
index 000000000000..eb93cb6ea8d6
--- /dev/null
+++ b/src/app/(main)/settings/llm/ProviderList/SenseNova/index.tsx
@@ -0,0 +1,46 @@
+'use client';
+
+import { SenseNova } from '@lobehub/icons';
+import { Input } from 'antd';
+import { useTranslation } from 'react-i18next';
+
+import { SenseNovaProviderCard } from '@/config/modelProviders';
+import { GlobalLLMProviderKey } from '@/types/user/settings';
+
+import { KeyVaultsConfigKey } from '../../const';
+import { ProviderItem } from '../../type';
+
+const providerKey: GlobalLLMProviderKey = 'sensenova';
+
+export const useSenseNovaProvider = (): ProviderItem => {
+  const { t } = useTranslation('modelProvider');
+
+  return {
+    ...SenseNovaProviderCard,
+    apiKeyItems: [
+      {
+        children: (
+          <Input.Password
+            autoComplete={'new-password'}
+            placeholder={t(`${providerKey}.sensenovaAccessKeyID.placeholder`)}
+          />
+        ),
+        desc: t(`${providerKey}.sensenovaAccessKeyID.desc`),
+        label: t(`${providerKey}.sensenovaAccessKeyID.title`),
+        name: [KeyVaultsConfigKey, providerKey, 'sensenovaAccessKeyID'],
+      },
+      {
+        children: (
+          <Input.Password
+            autoComplete={'new-password'}
+            placeholder={t(`${providerKey}.sensenovaAccessKeySecret.placeholder`)}
+          />
+        ),
+        desc: t(`${providerKey}.sensenovaAccessKeySecret.desc`),
+        label: t(`${providerKey}.sensenovaAccessKeySecret.title`),
+        name: [KeyVaultsConfigKey, providerKey, 'sensenovaAccessKeySecret'],
+      },
+    ],
+    title: <SenseNova.Combine size={24} type={'color'} />,
+  };
+};
diff --git a/src/app/(main)/settings/llm/ProviderList/providers.tsx b/src/app/(main)/settings/llm/ProviderList/providers.tsx
index d07e93fe0546..47f40bf2f30d 100644
--- a/src/app/(main)/settings/llm/ProviderList/providers.tsx
+++ b/src/app/(main)/settings/llm/ProviderList/providers.tsx
@@ -34,6 +34,7 @@ import { useGithubProvider } from './Github';
 import { useOllamaProvider } from './Ollama';
 import { useOpenAIProvider } from './OpenAI';
 import { useWenxinProvider } from './Wenxin';
+import { useSenseNovaProvider } from './SenseNova';
 
 export const useProviderList = (): ProviderItem[] => {
   const AzureProvider = useAzureProvider();
@@ -42,6 +43,7 @@ export const useProviderList = (): ProviderItem[] => {
   const BedrockProvider = useBedrockProvider();
   const GithubProvider = useGithubProvider();
   const WenxinProvider = useWenxinProvider();
+  const SenseNovaProvider = useSenseNovaProvider();
 
   return useMemo(
     () => [
@@ -68,6 +70,7 @@ export const useProviderList = (): ProviderItem[] => {
       SparkProviderCard,
       ZhiPuProviderCard,
       ZeroOneProviderCard,
+      SenseNovaProvider,
       StepfunProviderCard,
       MoonshotProviderCard,
       BaichuanProviderCard,
@@ -76,6 +79,6 @@ export const useProviderList = (): ProviderItem[] => {
       TaichuProviderCard,
       SiliconCloudProviderCard,
     ],
-    [AzureProvider, OllamaProvider, OpenAIProvider, BedrockProvider, GithubProvider,WenxinProvider],
+    [AzureProvider, OllamaProvider, OpenAIProvider, BedrockProvider, GithubProvider, WenxinProvider, SenseNovaProvider],
   );
 };
diff --git a/src/app/api/chat/agentRuntime.test.ts b/src/app/api/chat/agentRuntime.test.ts
index 1d1a1b1064e4..848e8b25fbde 100644
--- a/src/app/api/chat/agentRuntime.test.ts
+++ b/src/app/api/chat/agentRuntime.test.ts
@@ -24,6 +24,7 @@ import {
   ModelProvider,
 } from '@/libs/agent-runtime';
 import { AgentRuntime } from '@/libs/agent-runtime';
+import { LobeSenseNovaAI } from '@/libs/agent-runtime/sensenova';
 import { LobeStepfunAI } from '@/libs/agent-runtime/stepfun';
 import LobeWenxinAI from '@/libs/agent-runtime/wenxin';
 
@@ -58,6 +59,9 @@ vi.mock('@/config/llm', () => ({
 
     WENXIN_ACCESS_KEY: 'test-wenxin-access-key',
     WENXIN_SECRET_KEY: 'test-wenxin-secret-key',
+
+    SENSENOVA_ACCESS_KEY_ID: 'test-sensenova-access-key-id',
+    SENSENOVA_ACCESS_KEY_SECRET: 'test-sensenova-access-key-secret',
   })),
 }));
 
@@ -206,6 +210,16 @@ describe('initAgentRuntimeWithUserPayload method', () => {
       expect(runtime['_runtime']).toBeInstanceOf(LobeStepfunAI);
     });
 
+    it.skip('SenseNova AI provider: with apikey', async () => {
+      const jwtPayload: JWTPayload = {
+        sensenovaAccessKeyID: 'user-sensenova-access-key-id',
+        sensenovaAccessKeySecret: 'sensenova-access-key-secret',
+      };
+      const runtime = await initAgentRuntimeWithUserPayload(ModelProvider.SenseNova, jwtPayload);
+      expect(runtime).toBeInstanceOf(AgentRuntime);
+      expect(runtime['_runtime']).toBeInstanceOf(LobeSenseNovaAI);
+    });
+
     it.skip('Wenxin AI provider: with apikey', async () => {
       const jwtPayload: JWTPayload = {
         wenxinAccessKey: 'user-wenxin-accessKey',
@@ -353,6 +367,13 @@ describe('initAgentRuntimeWithUserPayload method', () => {
       expect(runtime['_runtime']).toBeInstanceOf(LobeTogetherAI);
     });
 
+    it.skip('SenseNova AI provider: without apikey', async () => {
+      const jwtPayload = {};
+      const runtime = await initAgentRuntimeWithUserPayload(ModelProvider.SenseNova, jwtPayload);
+      expect(runtime).toBeInstanceOf(AgentRuntime);
+      expect(runtime['_runtime']).toBeInstanceOf(LobeSenseNovaAI);
+    });
+
     it.skip('Wenxin AI provider: without apikey', async () => {
       const jwtPayload = {};
       const runtime = await initAgentRuntimeWithUserPayload(ModelProvider.Wenxin, jwtPayload);
diff --git a/src/app/api/chat/sensenova/route.test.ts b/src/app/api/chat/sensenova/route.test.ts
new file mode 100644
index 000000000000..625ba8a11844
--- /dev/null
+++ b/src/app/api/chat/sensenova/route.test.ts
@@ -0,0 +1,27 @@
+// @vitest-environment edge-runtime
+import { describe, expect, it, vi } from 'vitest';
+
+import { POST as UniverseRoute } from '../[provider]/route';
+import { POST, runtime } from './route';
+
+// 模拟 '../[provider]/route'
+vi.mock('../[provider]/route', () => ({
+  POST: vi.fn().mockResolvedValue('mocked response'),
+}));
+
+describe('Configuration tests', () => {
+  it('should have runtime set to "edge"', () => {
+    expect(runtime).toBe('nodejs');
+  });
+});
+
+describe('SenseNova POST function tests', () => {
+  it('should call UniverseRoute with correct parameters', async () => {
+    const mockRequest = new Request('https://example.com', { method: 'POST' });
+    await POST(mockRequest);
+    expect(UniverseRoute).toHaveBeenCalledWith(mockRequest, {
+      createRuntime: expect.anything(),
+      params: { provider: 'sensenova' },
+    });
+  });
+});
diff --git a/src/app/api/chat/sensenova/route.ts b/src/app/api/chat/sensenova/route.ts
new file mode 100644
index 000000000000..50ea72b2de56
--- /dev/null
+++ b/src/app/api/chat/sensenova/route.ts
@@ -0,0 +1,34 @@
+import { getLLMConfig } from '@/config/llm';
+import { AgentRuntime } from '@/libs/agent-runtime';
+import { LobeSenseNovaAI } from '@/libs/agent-runtime/sensenova';
+
+import { POST as UniverseRoute } from '../[provider]/route';
+
+import { generateJwtTokenSenseNova } from '@/libs/agent-runtime/sensenova/authToken';
+
+export const runtime = 'nodejs';
+
+export const maxDuration = 30;
+
+export const POST = async (req: Request) =>
+  UniverseRoute(req, {
+    createRuntime: (payload) => {
+      const { SENSENOVA_ACCESS_KEY_ID, SENSENOVA_ACCESS_KEY_SECRET } = getLLMConfig();
+      let sensenovaAccessKeyID: string | undefined = SENSENOVA_ACCESS_KEY_ID;
+      let sensenovaAccessKeySecret: string | undefined = SENSENOVA_ACCESS_KEY_SECRET;
+
+      // if the payload has the api key, use user
+      if (payload.apiKey) {
+        sensenovaAccessKeyID = payload?.sensenovaAccessKeyID;
+        sensenovaAccessKeySecret = payload?.sensenovaAccessKeySecret;
+      }
+
+      const apiKey = generateJwtTokenSenseNova(sensenovaAccessKeyID, sensenovaAccessKeySecret, 5, 5);
+
+      const params = { apiKey };
+      const instance = new LobeSenseNovaAI(params);
+
+      return new AgentRuntime(instance);
+    },
+    params: { provider: 'sensenova' },
+  });
diff --git a/src/config/llm.ts b/src/config/llm.ts
index b07ed60f6082..a14fe7c560a8 100644
--- a/src/config/llm.ts
+++ b/src/config/llm.ts
@@ -125,6 +125,11 @@ export const getLLMConfig = () => {
       ENABLED_HUNYUAN: z.boolean(),
       HUNYUAN_API_KEY: z.string().optional(),
       HUNYUAN_MODEL_LIST: z.string().optional(),
+
+      ENABLED_SENSENOVA: z.boolean(),
+      SENSENOVA_ACCESS_KEY_ID: z.string().optional(),
+      SENSENOVA_ACCESS_KEY_SECRET: z.string().optional(),
+      SENSENOVA_MODEL_LIST: z.string().optional(),
     },
     runtimeEnv: {
       API_KEY_SELECT_MODE: process.env.API_KEY_SELECT_MODE,
@@ -247,6 +252,11 @@ export const getLLMConfig = () => {
       ENABLED_HUNYUAN: !!process.env.HUNYUAN_API_KEY,
       HUNYUAN_API_KEY: process.env.HUNYUAN_API_KEY,
       HUNYUAN_MODEL_LIST: process.env.HUNYUAN_MODEL_LIST,
+
+      ENABLED_SENSENOVA: !!process.env.SENSENOVA_ACCESS_KEY_ID && !!process.env.SENSENOVA_ACCESS_KEY_SECRET,
+      SENSENOVA_ACCESS_KEY_ID: process.env.SENSENOVA_ACCESS_KEY_ID,
+      SENSENOVA_ACCESS_KEY_SECRET: process.env.SENSENOVA_ACCESS_KEY_SECRET,
+      SENSENOVA_MODEL_LIST: process.env.SENSENOVA_MODEL_LIST,
     },
   });
 };
diff --git a/src/config/modelProviders/index.ts b/src/config/modelProviders/index.ts
index 3cca8d0f813a..b2516ebaa316 100644
--- a/src/config/modelProviders/index.ts
+++ b/src/config/modelProviders/index.ts
@@ -21,6 +21,7 @@ import OpenAIProvider from './openai';
 import OpenRouterProvider from './openrouter';
 import PerplexityProvider from './perplexity';
 import QwenProvider from './qwen';
+import SenseNovaProvider from './sensenova';
 import SiliconCloudProvider from './siliconcloud';
 import SparkProvider from './spark';
 import StepfunProvider from './stepfun';
@@ -61,6 +62,7 @@ export const LOBE_DEFAULT_MODEL_LIST: ChatModelCard[] = [
   Ai21Provider.chatModels,
   HunyuanProvider.chatModels,
   WenxinProvider.chatModels,
+  SenseNovaProvider.chatModels,
 ].flat();
 
 export const DEFAULT_MODEL_PROVIDER_LIST = [
@@ -87,6 +89,7 @@ export const DEFAULT_MODEL_PROVIDER_LIST = [
   SparkProvider,
   ZhiPuProvider,
   ZeroOneProvider,
+  SenseNovaProvider,
   StepfunProvider,
   MoonshotProvider,
   BaichuanProvider,
@@ -126,6 +129,7 @@ export { default as OpenAIProviderCard } from './openai';
 export { default as OpenRouterProviderCard } from './openrouter';
 export { default as PerplexityProviderCard } from './perplexity';
 export { default as QwenProviderCard } from './qwen';
+export { default as SenseNovaProviderCard } from './sensenova';
 export { default as SiliconCloudProviderCard } from './siliconcloud';
 export { default as SparkProviderCard } from './spark';
 export { default as StepfunProviderCard } from './stepfun';
diff --git a/src/config/modelProviders/sensenova.ts b/src/config/modelProviders/sensenova.ts
new file mode 100644
index 000000000000..fe4965a6897b
--- /dev/null
+++ b/src/config/modelProviders/sensenova.ts
@@ -0,0 +1,124 @@
+import { ModelProviderCard } from '@/types/llm';
+
+// ref https://platform.sensenova.cn/pricing
+// ref https://platform.sensenova.cn/release?path=/release-202409.md
+const SenseNova: ModelProviderCard = {
+  chatModels: [
+    {
+      description: '最新版本模型 (V5.5)，128K上下文长度，在数学推理、英文对话、指令跟随以及长文本理解等领域能力显著提升，比肩GPT-4o',
+      displayName: 'SenseChat 5.5',
+      enabled: true,
+      functionCall: true,
+      id: 'SenseChat-5',
+      pricing: {
+        currency: 'CNY',
+        input: 40,
+        output: 100,
+      },
+      tokens: 131_072,
+    },
+    {
+      description: '最新版本模型 (V5.5)，16K上下文长度，支持多图的输入，全面实现模型基础能力优化，在对象属性识别、空间关系、动作事件识别、场景理解、情感识别、逻辑常识推理和文本理解生成上都实现了较大提升。',
+      displayName: 'SenseChat 5.5 Vision',
+      enabled: true,
+      id: 'SenseChat-Vision',
+      pricing: {
+        currency: 'CNY',
+        input: 100,
+        output: 100,
+      },
+      tokens: 16_384,
+      vision: true,
+    },
+    {
+      description: '适用于快速问答、模型微调场景',
+      displayName: 'SenseChat 5.0 Turbo',
+      enabled: true,
+      id: 'SenseChat-Turbo',
+      pricing: {
+        currency: 'CNY',
+        input: 2,
+        output: 5,
+      },
+      tokens: 32_768,
+    },
+    {
+      description: '32K上下文长度，在粤语的对话理解上超越了GPT-4，在知识、推理、数学及代码编写等多个领域均能与GPT-4 Turbo相媲美',
+      displayName: 'SenseChat 5.0 Cantonese',
+      id: 'SenseChat-5-Cantonese',
+      pricing: {
+        currency: 'CNY',
+        input: 27,
+        output: 27,
+      },
+      tokens: 32_768,
+    },
+    {
+      description: '基础版本模型 (V4)，128K上下文长度，在长文本理解及生成等任务中表现出色',
+      displayName: 'SenseChat 4.0 128K',
+      enabled: true,
+      id: 'SenseChat-128K',
+      pricing: {
+        currency: 'CNY',
+        input: 60,
+        output: 60,
+      },
+      tokens: 131_072,
+    },
+    {
+      description: '基础版本模型 (V4)，32K上下文长度，灵活应用于各类场景',
+      displayName: 'SenseChat 4.0 32K',
+      enabled: true,
+      id: 'SenseChat-32K',
+      pricing: {
+        currency: 'CNY',
+        input: 36,
+        output: 36,
+      },
+      tokens: 32_768,
+    },
+    {
+      description: '基础版本模型 (V4)，4K上下文长度，通用能力强大',
+      displayName: 'SenseChat 4.0 4K',
+      enabled: true,
+      id: 'SenseChat',
+      pricing: {
+        currency: 'CNY',
+        input: 12,
+        output: 12,
+      },
+      tokens: 4096,
+    },
+    {
+      description: '标准版模型，8K上下文长度，高响应速度',
+      displayName: 'SenseChat Character',
+      id: 'SenseChat-Character',
+      pricing: {
+        currency: 'CNY',
+        input: 12,
+        output: 12,
+      },
+      tokens: 8192,
+    },
+    {
+      description: '高级版模型，32K上下文长度，能力全面提升，支持中/英文对话',
+      displayName: 'SenseChat Character Pro',
+      id: 'SenseChat-Character-Pro',
+      pricing: {
+        currency: 'CNY',
+        input: 15,
+        output: 15,
+      },
+      tokens: 32_768,
+    },
+  ],
+  checkModel: 'SenseChat-Turbo',
+  disableBrowserRequest: true,
+  id: 'sensenova',
+  modelList: { showModelFetcher: true },
+  modelsUrl: 'https://platform.sensenova.cn/pricing',
+  name: 'SenseNova',
+  url: 'https://platform.sensenova.cn/home',
+};
+
+export default SenseNova;
diff --git a/src/const/auth.ts b/src/const/auth.ts
index 643ee56ceefe..33c0180fc766 100644
--- a/src/const/auth.ts
+++ b/src/const/auth.ts
@@ -40,6 +40,9 @@ export interface JWTPayload {
   wenxinAccessKey?: string;
   wenxinSecretKey?: string;
 
+  sensenovaAccessKeyID?: string;
+  sensenovaAccessKeySecret?: string;
+
   /**
    * user id
    * in client db mode it's a uuid
diff --git a/src/const/settings/llm.ts b/src/const/settings/llm.ts
index d22d1e55d4b0..630373414cdc 100644
--- a/src/const/settings/llm.ts
+++ b/src/const/settings/llm.ts
@@ -19,6 +19,7 @@ import {
   OpenRouterProviderCard,
   PerplexityProviderCard,
   QwenProviderCard,
+  SenseNovaProviderCard,
   SiliconCloudProviderCard,
   SparkProviderCard,
   StepfunProviderCard,
@@ -118,6 +119,10 @@ export const DEFAULT_LLM_CONFIG: UserModelProviderConfig = {
     enabled: false,
     enabledModels: filterEnabledModels(QwenProviderCard),
   },
+  sensenova: {
+    enabled: false,
+    enabledModels: filterEnabledModels(SenseNovaProviderCard),
+  },
   siliconcloud: {
     enabled: false,
     enabledModels: filterEnabledModels(SiliconCloudProviderCard),
diff --git a/src/features/Conversation/Error/APIKeyForm/SenseNova.tsx b/src/features/Conversation/Error/APIKeyForm/SenseNova.tsx
new file mode 100644
index 000000000000..dbf970b1c6d6
--- /dev/null
+++ b/src/features/Conversation/Error/APIKeyForm/SenseNova.tsx
@@ -0,0 +1,49 @@
+import { SenseNova } from '@lobehub/icons';
+import { Input } from 'antd';
+import { memo } from 'react';
+import { useTranslation } from 'react-i18next';
+
+import { ModelProvider } from '@/libs/agent-runtime';
+import { useUserStore } from '@/store/user';
+import { keyVaultsConfigSelectors } from '@/store/user/selectors';
+
+import { FormAction } from '../style';
+
+const SenseNovaForm = memo(() => {
+  const { t } = useTranslation('modelProvider');
+
+  const [sensenovaAccessKeyID, sensenovaAccessKeySecret, setConfig] = useUserStore((s) => [
+    keyVaultsConfigSelectors.sensenovaConfig(s).sensenovaAccessKeyID,
+    keyVaultsConfigSelectors.sensenovaConfig(s).sensenovaAccessKeySecret,
+    s.updateKeyVaultConfig,
+  ]);
+
+  return (
+    <FormAction
+      avatar={<SenseNova color={SenseNova.colorPrimary} size={56} />}
+      description={t('sensenova.unlock.description')}
+      title={t('sensenova.unlock.title')}
+    >
+      <Input.Password
+        autoComplete={'new-password'}
+        onChange={(e) => {
+          setConfig(ModelProvider.SenseNova, { sensenovaAccessKeyID: e.target.value });
+        }}
+        placeholder={t('sensenova.sensenovaAccessKeyID.placeholder')}
+        type={'block'}
+        value={sensenovaAccessKeyID}
+      />
+      <Input.Password
+        autoComplete={'new-password'}
+        onChange={(e) => {
+          setConfig(ModelProvider.SenseNova, { sensenovaAccessKeySecret: e.target.value });
+        }}
+        placeholder={t('sensenova.sensenovaAccessKeySecret.placeholder')}
+        type={'block'}
+        value={sensenovaAccessKeySecret}
+      />
+    </FormAction>
+  );
+});
+
+export default SenseNovaForm;
diff --git a/src/features/Conversation/Error/APIKeyForm/index.tsx b/src/features/Conversation/Error/APIKeyForm/index.tsx
index 5ba78f4f0ba3..7b53b69d8945 100644
--- a/src/features/Conversation/Error/APIKeyForm/index.tsx
+++ b/src/features/Conversation/Error/APIKeyForm/index.tsx
@@ -10,6 +10,7 @@ import { GlobalLLMProviderKey } from '@/types/user/settings';
 
 import BedrockForm from './Bedrock';
 import ProviderApiKeyForm from './ProviderApiKeyForm';
+import SenseNovaForm from './SenseNova';
 import WenxinForm from './Wenxin';
 
 interface APIKeyFormProps {
@@ -66,6 +67,8 @@ const APIKeyForm = memo<APIKeyFormProps>(({ id, provider }) => {
     <Center gap={16} style={{ maxWidth: 300 }}>
       {provider === ModelProvider.Bedrock ? (
         <BedrockForm />
+      ) : provider === ModelProvider.SenseNova ? (
+        <SenseNovaForm />
       ) : provider === ModelProvider.Wenxin ? (
         <WenxinForm />
       ) : (
diff --git a/src/libs/agent-runtime/AgentRuntime.ts b/src/libs/agent-runtime/AgentRuntime.ts
index 68ea3ade7cd7..2cc181682630 100644
--- a/src/libs/agent-runtime/AgentRuntime.ts
+++ b/src/libs/agent-runtime/AgentRuntime.ts
@@ -24,6 +24,7 @@ import { LobeOpenAI } from './openai';
 import { LobeOpenRouterAI } from './openrouter';
 import { LobePerplexityAI } from './perplexity';
 import { LobeQwenAI } from './qwen';
+import { LobeSenseNovaAI } from './sensenova';
 import { LobeSiliconCloudAI } from './siliconcloud';
 import { LobeSparkAI } from './spark';
 import { LobeStepfunAI } from './stepfun';
@@ -144,6 +145,7 @@ class AgentRuntime {
       openrouter: Partial<ClientOptions>;
       perplexity: Partial<ClientOptions>;
       qwen: Partial<ClientOptions>;
+      sensenova: Partial<ClientOptions>;
       siliconcloud: Partial<ClientOptions>;
       spark: Partial<ClientOptions>;
       stepfun: Partial<ClientOptions>;
@@ -307,6 +309,11 @@ class AgentRuntime {
         runtimeModel = new LobeHunyuanAI(params.hunyuan);
         break;
       }
+
+      case ModelProvider.SenseNova: {
+        runtimeModel = new LobeSenseNovaAI(params.sensenova);
+        break;
+      }
     }
 
     return new AgentRuntime(runtimeModel);
diff --git a/src/libs/agent-runtime/sensenova/authToken.ts b/src/libs/agent-runtime/sensenova/authToken.ts
new file mode 100644
index 000000000000..8a226e88a8f0
--- /dev/null
+++ b/src/libs/agent-runtime/sensenova/authToken.ts
@@ -0,0 +1,34 @@
+import CryptoJS from 'crypto-js';
+
+const base64UrlEncode = (obj: object) => {
+    return CryptoJS.enc.Base64.stringify(CryptoJS.enc.Utf8.parse(JSON.stringify(obj)))
+      .replaceAll('=', '')
+      .replaceAll('+', '-')
+      .replaceAll('/', '_')
+}
+
+// https://console.sensecore.cn/help/docs/model-as-a-service/nova/overview/Authorization
+export const generateJwtTokenSenseNova = (accessKeyID: string = '', accessKeySecret: string = '', expiredAfter: number = 1800, notBefore: number = 5) => {
+      const headers = {
+        alg: 'HS256',
+        typ: 'JWT'
+      }
+
+      const payload = {
+        exp: Math.floor(Date.now() / 1000) + expiredAfter,
+        iss: accessKeyID,
+        nbf: Math.floor(Date.now() / 1000) - notBefore,
+      }
+
+      const data = `${ base64UrlEncode(headers) }.${ base64UrlEncode(payload) }`
+
+      const signature = CryptoJS.HmacSHA256(data, accessKeySecret)
+        .toString(CryptoJS.enc.Base64)
+        .replaceAll('=', '')
+        .replaceAll('+', '-')
+        .replaceAll('/', '_')
+
+      const apiKey = `${ data }.${ signature }`
+
+      return apiKey
+};
diff --git a/src/libs/agent-runtime/sensenova/index.test.ts b/src/libs/agent-runtime/sensenova/index.test.ts
new file mode 100644
index 000000000000..b2e33e48ef67
--- /dev/null
+++ b/src/libs/agent-runtime/sensenova/index.test.ts
@@ -0,0 +1,255 @@
+// @vitest-environment node
+import OpenAI from 'openai';
+import { Mock, afterEach, beforeEach, describe, expect, it, vi } from 'vitest';
+
+import {
+  ChatStreamCallbacks,
+  LobeOpenAICompatibleRuntime,
+  ModelProvider,
+} from '@/libs/agent-runtime';
+
+import * as debugStreamModule from '../utils/debugStream';
+import { LobeSenseNovaAI } from './index';
+
+const provider = ModelProvider.SenseNova;
+const defaultBaseURL = 'https://api.sensenova.cn/compatible-mode/v1';
+
+const bizErrorType = 'ProviderBizError';
+const invalidErrorType = 'InvalidProviderAPIKey';
+
+// Mock the console.error to avoid polluting test output
+vi.spyOn(console, 'error').mockImplementation(() => {});
+
+let instance: LobeOpenAICompatibleRuntime;
+
+beforeEach(() => {
+  instance = new LobeSenseNovaAI({ apiKey: 'test' });
+
+  // 使用 vi.spyOn 来模拟 chat.completions.create 方法
+  vi.spyOn(instance['client'].chat.completions, 'create').mockResolvedValue(
+    new ReadableStream() as any,
+  );
+});
+
+afterEach(() => {
+  vi.clearAllMocks();
+});
+
+describe('LobeSenseNovaAI', () => {
+  describe('init', () => {
+    it('should correctly initialize with an API key', async () => {
+      const instance = new LobeSenseNovaAI({ apiKey: 'test_api_key' });
+      expect(instance).toBeInstanceOf(LobeSenseNovaAI);
+      expect(instance.baseURL).toEqual(defaultBaseURL);
+    });
+  });
+
+  describe('chat', () => {
+    describe('Error', () => {
+      it('should return OpenAIBizError with an openai error response when OpenAI.APIError is thrown', async () => {
+        // Arrange
+        const apiError = new OpenAI.APIError(
+          400,
+          {
+            status: 400,
+            error: {
+              message: 'Bad Request',
+            },
+          },
+          'Error message',
+          {},
+        );
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(apiError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            error: {
+              error: { message: 'Bad Request' },
+              status: 400,
+            },
+            errorType: bizErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should throw AgentRuntimeError with NoOpenAIAPIKey if no apiKey is provided', async () => {
+        try {
+          new LobeSenseNovaAI({});
+        } catch (e) {
+          expect(e).toEqual({ errorType: invalidErrorType });
+        }
+      });
+
+      it('should return OpenAIBizError with the cause when OpenAI.APIError is thrown with cause', async () => {
+        // Arrange
+        const errorInfo = {
+          stack: 'abc',
+          cause: {
+            message: 'api is undefined',
+          },
+        };
+        const apiError = new OpenAI.APIError(400, errorInfo, 'module error', {});
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(apiError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            error: {
+              cause: { message: 'api is undefined' },
+              stack: 'abc',
+            },
+            errorType: bizErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should return OpenAIBizError with an cause response with desensitize Url', async () => {
+        // Arrange
+        const errorInfo = {
+          stack: 'abc',
+          cause: { message: 'api is undefined' },
+        };
+        const apiError = new OpenAI.APIError(400, errorInfo, 'module error', {});
+
+        instance = new LobeSenseNovaAI({
+          apiKey: 'test',
+
+          baseURL: 'https://api.abc.com/v1',
+        });
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(apiError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: 'https://api.***.com/v1',
+            error: {
+              cause: { message: 'api is undefined' },
+              stack: 'abc',
+            },
+            errorType: bizErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should throw an InvalidSenseNovaAPIKey error type on 401 status code', async () => {
+        // Mock the API call to simulate a 401 error
+        const error = new Error('Unauthorized') as any;
+        error.status = 401;
+        vi.mocked(instance['client'].chat.completions.create).mockRejectedValue(error);
+
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          // Expect the chat method to throw an error with InvalidSenseNovaAPIKey
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            error: new Error('Unauthorized'),
+            errorType: invalidErrorType,
+            provider,
+          });
+        }
+      });
+
+      it('should return AgentRuntimeError for non-OpenAI errors', async () => {
+        // Arrange
+        const genericError = new Error('Generic Error');
+
+        vi.spyOn(instance['client'].chat.completions, 'create').mockRejectedValue(genericError);
+
+        // Act
+        try {
+          await instance.chat({
+            messages: [{ content: 'Hello', role: 'user' }],
+            model: 'SenseChat-Turbo',
+            temperature: 0,
+          });
+        } catch (e) {
+          expect(e).toEqual({
+            endpoint: defaultBaseURL,
+            errorType: 'AgentRuntimeError',
+            provider,
+            error: {
+              name: genericError.name,
+              cause: genericError.cause,
+              message: genericError.message,
+              stack: genericError.stack,
+            },
+          });
+        }
+      });
+    });
+
+    describe('DEBUG', () => {
+      it('should call debugStream and return StreamingTextResponse when DEBUG_SENSENOVA_CHAT_COMPLETION is 1', async () => {
+        // Arrange
+        const mockProdStream = new ReadableStream() as any; // 模拟的 prod 流
+        const mockDebugStream = new ReadableStream({
+          start(controller) {
+            controller.enqueue('Debug stream content');
+            controller.close();
+          },
+        }) as any;
+        mockDebugStream.toReadableStream = () => mockDebugStream; // 添加 toReadableStream 方法
+
+        // 模拟 chat.completions.create 返回值，包括模拟的 tee 方法
+        (instance['client'].chat.completions.create as Mock).mockResolvedValue({
+          tee: () => [mockProdStream, { toReadableStream: () => mockDebugStream }],
+        });
+
+        // 保存原始环境变量值
+        const originalDebugValue = process.env.DEBUG_SENSENOVA_CHAT_COMPLETION;
+
+        // 模拟环境变量
+        process.env.DEBUG_SENSENOVA_CHAT_COMPLETION = '1';
+        vi.spyOn(debugStreamModule, 'debugStream').mockImplementation(() => Promise.resolve());
+
+        // 执行测试
+        // 运行你的测试函数，确保它会在条件满足时调用 debugStream
+        // 假设的测试函数调用，你可能需要根据实际情况调整
+        await instance.chat({
+          messages: [{ content: 'Hello', role: 'user' }],
+          model: 'SenseChat-Turbo',
+          stream: true,
+          temperature: 0,
+        });
+
+        // 验证 debugStream 被调用
+        expect(debugStreamModule.debugStream).toHaveBeenCalled();
+
+        // 恢复原始环境变量值
+        process.env.DEBUG_SENSENOVA_CHAT_COMPLETION = originalDebugValue;
+      });
+    });
+  });
+});
diff --git a/src/libs/agent-runtime/sensenova/index.ts b/src/libs/agent-runtime/sensenova/index.ts
new file mode 100644
index 000000000000..a9dd5200654c
--- /dev/null
+++ b/src/libs/agent-runtime/sensenova/index.ts
@@ -0,0 +1,24 @@
+import OpenAI from 'openai';
+
+import { ChatStreamPayload, ModelProvider } from '../types';
+import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
+
+export const LobeSenseNovaAI = LobeOpenAICompatibleFactory({
+  baseURL: 'https://api.sensenova.cn/compatible-mode/v1',
+  chatCompletion: {
+    handlePayload: (payload: ChatStreamPayload) => {
+      const { frequency_penalty, temperature, top_p, ...rest } = payload;
+
+      return {
+        ...rest,
+        frequency_penalty: (frequency_penalty !== undefined && frequency_penalty > 0 && frequency_penalty <= 2) ? frequency_penalty : undefined,
+        temperature: (temperature !== undefined && temperature > 0 && temperature <= 2) ? temperature : undefined,
+        top_p: (top_p !== undefined && top_p > 0 && top_p < 1) ? top_p : undefined,
+      } as OpenAI.ChatCompletionCreateParamsStreaming;
+    }
+  },
+  debug: {
+    chatCompletion: () => process.env.DEBUG_SENSENOVA_CHAT_COMPLETION === '1',
+  },
+  provider: ModelProvider.SenseNova,
+});
diff --git a/src/libs/agent-runtime/types/type.ts b/src/libs/agent-runtime/types/type.ts
index 59a2fc03b8be..7ad9ace9d355 100644
--- a/src/libs/agent-runtime/types/type.ts
+++ b/src/libs/agent-runtime/types/type.ts
@@ -43,6 +43,7 @@ export enum ModelProvider {
   OpenRouter = 'openrouter',
   Perplexity = 'perplexity',
   Qwen = 'qwen',
+  SenseNova = 'sensenova',
   SiliconCloud = 'siliconcloud',
   Spark = 'spark',
   Stepfun = 'stepfun',
diff --git a/src/locales/default/modelProvider.ts b/src/locales/default/modelProvider.ts
index 964413157fbb..71ff45d8149a 100644
--- a/src/locales/default/modelProvider.ts
+++ b/src/locales/default/modelProvider.ts
@@ -115,6 +115,23 @@ export default {
       title: '下载指定的 Ollama 模型',
     },
   },
+  sensenova: {
+    sensenovaAccessKeyID: {
+      desc: '填入 SenseNova Access Key ID',
+      placeholder: 'SenseNova Access Key ID',
+      title: 'Access Key ID',
+    },
+    sensenovaAccessKeySecret: {
+      desc: '填入 SenseNova Access Key Secret',
+      placeholder: 'SenseNova Access Key Secret',
+      title: 'Access Key Secret',
+    },
+    unlock: {
+      description:
+        '输入你的 Access Key ID / Access Key Secret 即可开始会话。应用不会记录你的鉴权配置',
+      title: '使用自定义 SenseNova 鉴权信息',
+    },
+  },
   wenxin: {
     accessKey: {
       desc: '填入百度千帆平台的 Access Key',
diff --git a/src/server/globalConfig/index.ts b/src/server/globalConfig/index.ts
index f26ddd83ae69..89231202bdd3 100644
--- a/src/server/globalConfig/index.ts
+++ b/src/server/globalConfig/index.ts
@@ -15,6 +15,7 @@ import {
   OpenAIProviderCard,
   OpenRouterProviderCard,
   QwenProviderCard,
+  SenseNovaProviderCard,
   SiliconCloudProviderCard,
   TogetherAIProviderCard,
   ZeroOneProviderCard,
@@ -71,6 +72,9 @@ export const getServerGlobalConfig = () => {
     ENABLED_AI21,
     ENABLED_AI360,
 
+    ENABLED_SENSENOVA,
+    SENSENOVA_MODEL_LIST,
+
     ENABLED_SILICONCLOUD,
     SILICONCLOUD_MODEL_LIST,
 
@@ -220,6 +224,14 @@ export const getServerGlobalConfig = () => {
           modelString: QWEN_MODEL_LIST,
         }),
       },
+      sensenova: {
+        enabled: ENABLED_SENSENOVA,
+        enabledModels: extractEnabledModels(SENSENOVA_MODEL_LIST),
+        serverModelCards: transformToChatModelCards({
+          defaultChatModels: SenseNovaProviderCard.chatModels,
+          modelString: SENSENOVA_MODEL_LIST,
+        }),
+      },
       siliconcloud: {
         enabled: ENABLED_SILICONCLOUD,
         enabledModels: extractEnabledModels(SILICONCLOUD_MODEL_LIST),
diff --git a/src/services/_auth.ts b/src/services/_auth.ts
index 7ecfa4f64c13..643df2140229 100644
--- a/src/services/_auth.ts
+++ b/src/services/_auth.ts
@@ -5,6 +5,8 @@ import { keyVaultsConfigSelectors, userProfileSelectors } from '@/store/user/sel
 import { GlobalLLMProviderKey } from '@/types/user/settings';
 import { createJWT } from '@/utils/jwt';
 
+import { generateJwtTokenSenseNova } from '@/libs/agent-runtime/sensenova/authToken';
+
 export const getProviderAuthPayload = (provider: string) => {
   switch (provider) {
     case ModelProvider.Bedrock: {
@@ -25,6 +27,16 @@ export const getProviderAuthPayload = (provider: string) => {
       };
     }
 
+    case ModelProvider.SenseNova: {
+      const { sensenovaAccessKeyID, sensenovaAccessKeySecret } = keyVaultsConfigSelectors.sensenovaConfig(
+        useUserStore.getState(),
+      );
+
+      const apiKey = generateJwtTokenSenseNova(sensenovaAccessKeyID, sensenovaAccessKeySecret, 5, 5);
+
+      return { apiKey };
+    }
+
     case ModelProvider.Wenxin: {
       const { secretKey, accessKey } = keyVaultsConfigSelectors.wenxinConfig(
         useUserStore.getState(),
diff --git a/src/store/user/slices/modelList/selectors/keyVaults.ts b/src/store/user/slices/modelList/selectors/keyVaults.ts
index 8a038f8d4919..6f5047d55481 100644
--- a/src/store/user/slices/modelList/selectors/keyVaults.ts
+++ b/src/store/user/slices/modelList/selectors/keyVaults.ts
@@ -16,6 +16,7 @@ const openAIConfig = (s: UserStore) => keyVaultsSettings(s).openai || {};
 const bedrockConfig = (s: UserStore) => keyVaultsSettings(s).bedrock || {};
 const wenxinConfig = (s: UserStore) => keyVaultsSettings(s).wenxin || {};
 const ollamaConfig = (s: UserStore) => keyVaultsSettings(s).ollama || {};
+const sensenovaConfig = (s: UserStore) => keyVaultsSettings(s).sensenova || {};
 const azureConfig = (s: UserStore) => keyVaultsSettings(s).azure || {};
 const getVaultByProvider = (provider: GlobalLLMProviderKey) => (s: UserStore) =>
   (keyVaultsSettings(s)[provider] || {}) as OpenAICompatibleKeyVault &
@@ -43,5 +44,6 @@ export const keyVaultsConfigSelectors = {
   ollamaConfig,
   openAIConfig,
   password,
+  sensenovaConfig,
   wenxinConfig,
 };
diff --git a/src/types/user/settings/keyVaults.ts b/src/types/user/settings/keyVaults.ts
index 1134a768996a..a6fce0854414 100644
--- a/src/types/user/settings/keyVaults.ts
+++ b/src/types/user/settings/keyVaults.ts
@@ -16,6 +16,11 @@ export interface AWSBedrockKeyVault {
   sessionToken?: string;
 }
 
+export interface SenseNovaKeyVault {
+  sensenovaAccessKeyID?: string;
+  sensenovaAccessKeySecret?: string;
+}
+
 export interface WenxinKeyVault {
   accessKey?: string;
   secretKey?: string;
@@ -45,6 +50,7 @@ export interface UserKeyVaults {
   password?: string;
   perplexity?: OpenAICompatibleKeyVault;
   qwen?: OpenAICompatibleKeyVault;
+  sensenova?: SenseNovaKeyVault;
   siliconcloud?: OpenAICompatibleKeyVault;
   spark?: OpenAICompatibleKeyVault;
   stepfun?: OpenAICompatibleKeyVault;
