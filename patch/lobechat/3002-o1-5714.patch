diff --git a/src/libs/agent-runtime/github/index.ts b/src/libs/agent-runtime/github/index.ts
index 0e55d1392c9d8..e5241cefb8078 100644
--- a/src/libs/agent-runtime/github/index.ts
+++ b/src/libs/agent-runtime/github/index.ts
@@ -2,7 +2,7 @@ import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
 import type { ChatModelCard } from '@/types/llm';
 
 import { AgentRuntimeErrorType } from '../error';
-import { pruneReasoningPayload, reasoningModels } from '../openai';
+import { pruneReasoningPayload } from '../openai';
 import { ModelProvider } from '../types';
 import {
   CHAT_MODELS_BLOCK_LIST,
@@ -37,7 +37,7 @@ export const LobeGithubAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (reasoningModels.has(model)) {
+      if (model.startsWith('o1') || model.startsWith('o3')) {
         return { ...pruneReasoningPayload(payload), stream: false } as any;
       }
 
diff --git a/src/libs/agent-runtime/openai/index.ts b/src/libs/agent-runtime/openai/index.ts
index 8bed6c0702d81..d9679216cf1fd 100644
--- a/src/libs/agent-runtime/openai/index.ts
+++ b/src/libs/agent-runtime/openai/index.ts
@@ -1,29 +1,37 @@
 import { ChatStreamPayload, ModelProvider, OpenAIChatMessage } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
-// TODO: 临时写法，后续要重构成 model card 展示配置
-export const reasoningModels = new Set([
-  'o1-preview',
-  'o1-preview-2024-09-12',
-  'o1-mini',
-  'o1-mini-2024-09-12',
-  'o1',
-  'o1-2024-12-17',
-  'o3-mini',
-  'o3-mini-2025-01-31',
-]);
+export const pruneReasoningPayload = (payload: ChatStreamPayload) => {
+  // TODO: 临时写法，后续要重构成 model card 展示配置
+  const disableStreamModels = new Set([
+    'o1',
+    'o1-2024-12-17'
+  ]);
+  const systemToUserModels = new Set([
+    'o1-preview',
+    'o1-preview-2024-09-12',
+    'o1-mini',
+    'o1-mini-2024-09-12',
+  ]);
 
-export const pruneReasoningPayload = (payload: ChatStreamPayload) => ({
-  ...payload,
-  frequency_penalty: 0,
-  messages: payload.messages.map((message: OpenAIChatMessage) => ({
-    ...message,
-    role: message.role === 'system' ? 'developer' : message.role,
-  })),
-  presence_penalty: 0,
-  temperature: 1,
-  top_p: 1,
-});
+  return {
+    ...payload,
+    frequency_penalty: 0,
+    messages: payload.messages.map((message: OpenAIChatMessage) => ({
+      ...message,
+      role:
+        message.role === 'system'
+          ? systemToUserModels.has(payload.model)
+            ? 'user'
+            : 'developer'
+          : message.role,
+    })),
+    presence_penalty: 0,
+    stream: !disableStreamModels.has(payload.model),
+    temperature: 1,
+    top_p: 1,
+  };
+};
 
 export const LobeOpenAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.openai.com/v1',
@@ -31,7 +39,7 @@ export const LobeOpenAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (reasoningModels.has(model)) {
+      if (model.startsWith('o1') || model.startsWith('o3')) {
         return pruneReasoningPayload(payload) as any;
       }
 
