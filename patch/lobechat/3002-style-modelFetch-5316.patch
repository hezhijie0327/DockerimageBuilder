diff --git a/src/config/aiModels/ai360.ts b/src/config/aiModels/ai360.ts
index 1a5939b73895..743cde3c0524 100644
--- a/src/config/aiModels/ai360.ts
+++ b/src/config/aiModels/ai360.ts
@@ -2,16 +2,29 @@ import { AIChatModelCard } from '@/types/aiModel';
 
 const ai360ChatModels: AIChatModelCard[] = [
   {
-    contextWindowTokens: 8192,
+    contextWindowTokens: 8000,
     description:
-      '360GPT2 Pro 是 360 公司推出的高级自然语言处理模型，具备卓越的文本生成和理解能力，尤其在生成与创作领域表现出色，能够处理复杂的语言转换和角色演绎任务。',
+      '360gpt2-o1 使用树搜索构建思维链，并引入了反思机制，使用强化学习训练，模型具备自我反思与纠错的能力。',
+    displayName: '360GPT2 o1',
+    enabled: true,
+    id: '360gpt2-o1',
+    pricing: {
+      currency: 'CNY',
+      input: 20,
+      output: 50,
+    },
+    type: 'chat',
+  },
+  {
+    contextWindowTokens: 8000,
+    description:
+      '360智脑系列效果最好的主力千亿级大模型，广泛适用于各领域复杂任务场景。',
     displayName: '360GPT2 Pro',
     enabled: true,
     id: '360gpt2-pro',
-    maxOutput: 7000,
     pricing: {
       currency: 'CNY',
-      input: 5,
+      input: 2,
       output: 5,
     },
     type: 'chat',
@@ -20,46 +33,29 @@ const ai360ChatModels: AIChatModelCard[] = [
     abilities: {
       functionCall: true,
     },
-    contextWindowTokens: 8192,
+    contextWindowTokens: 8000,
     description:
-      '360GPT Pro 作为 360 AI 模型系列的重要成员，以高效的文本处理能力满足多样化的自然语言应用场景，支持长文本理解和多轮对话等功能。',
+      '360智脑系列效果最好的主力千亿级大模型，广泛适用于各领域复杂任务场景。',
     displayName: '360GPT Pro',
     enabled: true,
     id: '360gpt-pro',
-    maxOutput: 7000,
     pricing: {
       currency: 'CNY',
-      input: 5,
+      input: 2,
       output: 5,
     },
     type: 'chat',
   },
   {
-    contextWindowTokens: 8192,
+    contextWindowTokens: 7000,
     description:
-      '360GPT Turbo 提供强大的计算和对话能力，具备出色的语义理解和生成效率，是企业和开发者理想的智能助理解决方案。',
+      '兼顾性能和效果的百亿级大模型，适合对性能/成本要求较高 的场景。',
     displayName: '360GPT Turbo',
     enabled: true,
     id: '360gpt-turbo',
-    maxOutput: 7000,
     pricing: {
       currency: 'CNY',
-      input: 2,
-      output: 2,
-    },
-    type: 'chat',
-  },
-  {
-    contextWindowTokens: 8192,
-    description:
-      '360GPT Turbo Responsibility 8K 强调语义安全和责任导向，专为对内容安全有高度要求的应用场景设计，确保用户体验的准确性与稳健性。',
-    displayName: '360GPT Turbo Responsibility 8K',
-    enabled: true,
-    id: '360gpt-turbo-responsibility-8k',
-    maxOutput: 2048,
-    pricing: {
-      currency: 'CNY',
-      input: 2,
+      input: 1,
       output: 2,
     },
     type: 'chat',
diff --git a/src/config/aiModels/giteeai.ts b/src/config/aiModels/giteeai.ts
index afc3e67bbc1d..badd9eb41312 100644
--- a/src/config/aiModels/giteeai.ts
+++ b/src/config/aiModels/giteeai.ts
@@ -7,24 +7,35 @@ const giteeaiChatModels: AIChatModelCard[] = [
     },
     contextWindowTokens: 16_000,
     description:
-      'Qwen2.5-72B-Instruct 支持 16k 上下文, 生成长文本超过 8K 。支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。模型知识明显增加，并且大大提高了编码和数学能力, 多语言支持超过 29 种',
+      'Qwen2.5-72B-Instruct  支持 16k 上下文, 生成长文本超过 8K 。支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。模型知识明显增加，并且大大提高了编码和数学能力, 多语言支持超过 29 种',
     displayName: 'Qwen2.5 72B Instruct',
     enabled: true,
     id: 'Qwen2.5-72B-Instruct',
     type: 'chat',
   },
   {
+    contextWindowTokens: 32_000,
     description:
-      'Qwen2.5-Coder-32B-Instruct 是一款专为代码生成、代码理解和高效开发场景设计的大型语言模型，采用了业界领先的32B参数规模，能够满足多样化的编程需求。',
-    displayName: 'Qwen2.5 Coder 32B Instruct',
+      'Qwen2.5-32B-Instruct 是一款 320 亿参数的大语言模型，性能表现均衡，优化中文和多语言场景，支持智能问答、内容生成等应用。',
+    displayName: 'Qwen2.5 32B Instruct',
     enabled: true,
-    id: 'Qwen2.5-Coder-32B-Instruct',
+    id: 'Qwen2.5-32B-Instruct',
+    type: 'chat',
+  },
+  {
+    contextWindowTokens: 24_000,
+    description:
+      'Qwen2.5-14B-Instruct 是一款 140 亿参数的大语言模型，性能表现优秀，优化中文和多语言场景，支持智能问答、内容生成等应用。',
+    displayName: 'Qwen2.5 14B Instruct',
+    enabled: true,
+    id: 'Qwen2.5-14B-Instruct',
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
     },
+    contextWindowTokens: 32_000,
     description:
       'Qwen2.5-7B-Instruct 是一款 70 亿参数的大语言模型，支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。优化中文和多语言场景，支持智能问答、内容生成等应用。',
     displayName: 'Qwen2.5 7B Instruct',
@@ -33,52 +44,56 @@ const giteeaiChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    contextWindowTokens: 32_000,
     description:
-      'Qwen2.5-32B-Instruct 是一款 320 亿参数的大语言模型，性能表现均衡，优化中文和多语言场景，支持智能问答、内容生成等应用。',
-    displayName: 'Qwen2.5 32B Instruct',
-    enabled: true,
-    id: 'Qwen2.5-32B-Instruct',
+      'Qwen2 是 Qwen 模型的最新系列，支持 128k 上下文，对比当前最优的开源模型，Qwen2-72B 在自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型。',
+    displayName: 'Qwen2 72B Instruct',
+    id: 'Qwen2-72B-Instruct',
     type: 'chat',
   },
   {
+    contextWindowTokens: 24_000,
     description:
-      'Qwen2.5-14B-Instruct 是一款 140 亿参数的大语言模型，性能表现优秀，优化中文和多语言场景，支持智能问答、内容生成等应用。',
-    displayName: 'Qwen2.5 14B Instruct',
-    enabled: true,
-    id: 'Qwen2.5-14B-Instruct',
+      'Qwen2 是 Qwen 模型的最新系列，能够超越同等规模的最优开源模型甚至更大规模的模型，Qwen2 7B 在多个评测上取得显著的优势，尤其是代码及中文理解上。',
+    displayName: 'Qwen2 7B Instruct',
+    id: 'Qwen2-7B-Instruct',
     type: 'chat',
   },
   {
-    contextWindowTokens: 6000,
+    contextWindowTokens: 32_000,
     description:
-      'Qwen2 是 Qwen 模型的最新系列，支持 128k 上下文，对比当前最优的开源模型，Qwen2-72B 在自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型。',
-    displayName: 'Qwen2 72B Instruct',
-    id: 'Qwen2-72B-Instruct',
+      'Qwen2.5-Coder-32B-Instruct 是一款专为代码生成、代码理解和高效开发场景设计的大型语言模型，采用了业界领先的32B参数规模，能够满足多样化的编程需求。',
+    displayName: 'Qwen2.5 Coder 32B Instruct',
+    enabled: true,
+    id: 'Qwen2.5-Coder-32B-Instruct',
     type: 'chat',
   },
   {
-    contextWindowTokens: 32_000,
+    contextWindowTokens: 24_000,
     description:
-      'Qwen2 是 Qwen 模型的最新系列，能够超越同等规模的最优开源模型甚至更大规模的模型，Qwen2 7B 在多个评测上取得显著的优势，尤其是代码及中文理解上。',
-    displayName: 'Qwen2 7B Instruct',
-    id: 'Qwen2-7B-Instruct',
+      'Qwen2.5-Coder-14B-Instruct 是一款基于大规模预训练的编程指令模型，具备强大的代码理解和生成能力，能够高效地处理各种编程任务，特别适合智能代码编写、自动化脚本生成和编程问题解答。',
+    displayName: 'Qwen2.5 Coder 14B Instruct',
+    enabled: true,
+    id: 'Qwen2.5-Coder-14B-Instruct',
     type: 'chat',
   },
   {
     abilities: {
       vision: true,
     },
+    contextWindowTokens: 32_000,
     description:
-      'InternVL2-8B 是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
-    displayName: 'InternVL2 8B',
+      'Qwen2-VL-72B是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
+    displayName: 'Qwen2 VL 72B',
     enabled: true,
-    id: 'InternVL2-8B',
+    id: 'Qwen2-VL-72B',
     type: 'chat',
   },
   {
     abilities: {
       vision: true,
     },
+    contextWindowTokens: 32_000,
     description:
       'InternVL2.5-26B 是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
     displayName: 'InternVL2.5 26B',
@@ -86,6 +101,18 @@ const giteeaiChatModels: AIChatModelCard[] = [
     id: 'InternVL2.5-26B',
     type: 'chat',
   },
+  {
+    abilities: {
+      vision: true,
+    },
+    contextWindowTokens: 32_000,
+    description:
+      'InternVL2-8B 是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
+    displayName: 'InternVL2 8B',
+    enabled: true,
+    id: 'InternVL2-8B',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 32_000,
     description:
@@ -104,25 +131,28 @@ const giteeaiChatModels: AIChatModelCard[] = [
     id: 'Yi-34B-Chat',
     type: 'chat',
   },
+/*
+    // not compatible with OpenAI SDK
   {
-    contextWindowTokens: 8000,
     description:
-      'DeepSeek Coder 33B 是一个代码语言模型， 基于 2 万亿数据训练而成，其中 87% 为代码， 13% 为中英文语言。模型引入 16K 窗口大小和填空任务，提供项目级别的代码补全和片段填充功能。',
-    displayName: 'DeepSeek Coder 33B Instruct',
+      '代码小浣熊是基于商汤大语言模型的软件智能研发助手，覆盖软件需求分析、架构设计、代码编写、软件测试等环节，满足用户代码编写、编程学习等各类需求。代码小浣熊支持 Python、Java、JavaScript、C++、Go、SQL 等 90+主流编程语言和 VS Code、IntelliJ IDEA 等主流 IDE。在实际应用中，代码小浣熊可帮助开发者提升编程效率超 50%。',
+    displayName: 'code raccoon v1',
     enabled: true,
-    id: 'deepseek-coder-33B-instruct',
+    id: 'code-raccoon-v1',
     type: 'chat',
   },
+*/
   {
+    contextWindowTokens: 8000,
     description:
-      '代码小浣熊是基于商汤大语言模型的软件智能研发助手，覆盖软件需求分析、架构设计、代码编写、软件测试等环节，满足用户代码编写、编程学习等各类需求。代码小浣熊支持 Python、Java、JavaScript、C++、Go、SQL 等 90+主流编程语言和 VS Code、IntelliJ IDEA 等主流 IDE。在实际应用中，代码小浣熊可帮助开发者提升编程效率超 50%。',
-    displayName: 'code raccoon v1',
+      'DeepSeek Coder 33B 是一个代码语言模型， 基于 2 万亿数据训练而成，其中 87% 为代码， 13% 为中英文语言。模型引入 16K 窗口大小和填空任务，提供项目级别的代码补全和片段填充功能。',
+    displayName: 'DeepSeek Coder 33B Instruct',
     enabled: true,
-    id: 'code-raccoon-v1',
+    id: 'deepseek-coder-33B-instruct',
     type: 'chat',
   },
   {
-    contextWindowTokens: 40_000,
+    contextWindowTokens: 32_000,
     description:
       'CodeGeeX4-ALL-9B 是一个多语言代码生成模型，支持包括代码补全和生成、代码解释器、网络搜索、函数调用、仓库级代码问答在内的全面功能，覆盖软件开发的各种场景。是参数少于 10B 的顶尖代码生成模型。',
     displayName: 'CodeGeeX4 All 9B',
diff --git a/src/config/aiModels/taichu.ts b/src/config/aiModels/taichu.ts
index 583c1ab1efa7..81dedf56a25c 100644
--- a/src/config/aiModels/taichu.ts
+++ b/src/config/aiModels/taichu.ts
@@ -6,10 +6,31 @@ const taichuChatModels: AIChatModelCard[] = [
       functionCall: true,
     },
     contextWindowTokens: 32_768,
-    description: 'Taichu 2.0 基于海量高质数据训练，具有更强的文本理解、内容创作、对话问答等能力',
+    description: '基于海量高质数据训练，具有更强的文本理解、内容创作、对话问答等能力',
     displayName: 'Taichu 2.0',
     enabled: true,
     id: 'taichu_llm',
+    pricing: {
+      currency: 'CNY',
+      input: 2,
+      output: 2,
+    },
+    type: 'chat',
+  },
+  {
+    abilities: {
+      vision: true,
+    },
+    contextWindowTokens: 4096,
+    description: '融合了图像理解、知识迁移、逻辑归因等能力，在图文问答领域表现突出',
+    displayName: 'Taichu 2.0V',
+    enabled: true,
+    id: 'taichu2_mm',
+    pricing: {
+      currency: 'CNY',
+      input: 5,
+      output: 5,
+    },
     type: 'chat',
   },
 ];
diff --git a/src/config/aiModels/upstage.ts b/src/config/aiModels/upstage.ts
index f3b378e47148..4595fc04fe8d 100644
--- a/src/config/aiModels/upstage.ts
+++ b/src/config/aiModels/upstage.ts
@@ -22,7 +22,7 @@ const upstageChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
-    contextWindowTokens: 4096,
+    contextWindowTokens: 32_768,
     description:
       'Solar Pro 是 Upstage 推出的一款高智能LLM，专注于单GPU的指令跟随能力，IFEval得分80以上。目前支持英语，正式版本计划于2024年11月推出，将扩展语言支持和上下文长度。',
     displayName: 'Solar Pro',
diff --git a/src/config/aiModels/zhipu.ts b/src/config/aiModels/zhipu.ts
index 9531605cc33c..57898d22efaa 100644
--- a/src/config/aiModels/zhipu.ts
+++ b/src/config/aiModels/zhipu.ts
@@ -1,6 +1,19 @@
 import { AIChatModelCard } from '@/types/aiModel';
 
 const zhipuChatModels: AIChatModelCard[] = [
+  {
+    contextWindowTokens: 16_384,
+    description: 'GLM-Zero-Preview具备强大的复杂推理能力，在逻辑推理、数学、编程等领域表现优异。',
+    displayName: 'GLM-Zero-Preview',
+    enabled: true,
+    id: 'glm-zero-preview',
+    pricing: {
+      currency: 'CNY',
+      input: 10,
+      output: 10,
+    },
+    type: 'chat',
+  },
   {
     abilities: {
       functionCall: true,
diff --git a/src/config/modelProviders/ai21.ts b/src/config/modelProviders/ai21.ts
index 28dfedcb182d..743a486fa5f8 100644
--- a/src/config/modelProviders/ai21.ts
+++ b/src/config/modelProviders/ai21.ts
@@ -29,12 +29,10 @@ const Ai21: ModelProviderCard = {
   checkModel: 'jamba-1.5-mini',
   description: 'AI21 Labs 为企业构建基础模型和人工智能系统，加速生成性人工智能在生产中的应用。',
   id: 'ai21',
-  modelList: { showModelFetcher: true },
   modelsUrl: 'https://docs.ai21.com/reference',
   name: 'Ai21Labs',
   settings: {
     sdkType: 'openai',
-    showModelFetcher: true,
   },
   url: 'https://studio.ai21.com',
 };
diff --git a/src/config/modelProviders/ai360.ts b/src/config/modelProviders/ai360.ts
index b0a8712287c6..d1a686733ad2 100644
--- a/src/config/modelProviders/ai360.ts
+++ b/src/config/modelProviders/ai360.ts
@@ -4,59 +4,55 @@ import { ModelProviderCard } from '@/types/llm';
 const Ai360: ModelProviderCard = {
   chatModels: [
     {
-      contextWindowTokens: 8192,
+      contextWindowTokens: 8000,
       description:
-        '360GPT2 Pro 是 360 公司推出的高级自然语言处理模型，具备卓越的文本生成和理解能力，尤其在生成与创作领域表现出色，能够处理复杂的语言转换和角色演绎任务。',
-      displayName: '360GPT2 Pro',
+        '360gpt2-o1 使用树搜索构建思维链，并引入了反思机制，使用强化学习训练，模型具备自我反思与纠错的能力。',
+      displayName: '360GPT2 o1',
       enabled: true,
-      id: '360gpt2-pro',
-      maxOutput: 7000,
+      id: '360gpt2-o1',
       pricing: {
         currency: 'CNY',
-        input: 5,
-        output: 5,
+        input: 20,
+        output: 50,
       },
     },
     {
-      contextWindowTokens: 8192,
+      contextWindowTokens: 8000,
       description:
-        '360GPT Pro 作为 360 AI 模型系列的重要成员，以高效的文本处理能力满足多样化的自然语言应用场景，支持长文本理解和多轮对话等功能。',
-      displayName: '360GPT Pro',
+        '360智脑系列效果最好的主力千亿级大模型，广泛适用于各领域复杂任务场景。',
+      displayName: '360GPT2 Pro',
       enabled: true,
-      functionCall: true,
-      id: '360gpt-pro',
-      maxOutput: 7000,
+      id: '360gpt2-pro',
       pricing: {
         currency: 'CNY',
-        input: 5,
+        input: 2,
         output: 5,
       },
     },
     {
-      contextWindowTokens: 8192,
+      contextWindowTokens: 8000,
       description:
-        '360GPT Turbo 提供强大的计算和对话能力，具备出色的语义理解和生成效率，是企业和开发者理想的智能助理解决方案。',
-      displayName: '360GPT Turbo',
+        '360智脑系列效果最好的主力千亿级大模型，广泛适用于各领域复杂任务场景。',
+      displayName: '360GPT Pro',
       enabled: true,
-      id: '360gpt-turbo',
-      maxOutput: 7000,
+      functionCall: true,
+      id: '360gpt-pro',
       pricing: {
         currency: 'CNY',
         input: 2,
-        output: 2,
+        output: 5,
       },
     },
     {
-      contextWindowTokens: 8192,
+      contextWindowTokens: 7000,
       description:
-        '360GPT Turbo Responsibility 8K 强调语义安全和责任导向，专为对内容安全有高度要求的应用场景设计，确保用户体验的准确性与稳健性。',
-      displayName: '360GPT Turbo Responsibility 8K',
+        '兼顾性能和效果的百亿级大模型，适合对性能/成本要求较高 的场景。',
+      displayName: '360GPT Turbo',
       enabled: true,
-      id: '360gpt-turbo-responsibility-8k',
-      maxOutput: 2048,
+      id: '360gpt-turbo',
       pricing: {
         currency: 'CNY',
-        input: 2,
+        input: 1,
         output: 2,
       },
     },
diff --git a/src/config/modelProviders/anthropic.ts b/src/config/modelProviders/anthropic.ts
index 90f615533366..6865e4bb4ffa 100644
--- a/src/config/modelProviders/anthropic.ts
+++ b/src/config/modelProviders/anthropic.ts
@@ -143,7 +143,6 @@ const Anthropic: ModelProviderCard = {
       placeholder: 'https://api.anthropic.com',
     },
     sdkType: 'anthropic',
-    showModelFetcher: true,
     smoothing: {
       speed: 5,
       text: true,
diff --git a/src/config/modelProviders/giteeai.ts b/src/config/modelProviders/giteeai.ts
index dde54eedc4c0..641eb71fa973 100644
--- a/src/config/modelProviders/giteeai.ts
+++ b/src/config/modelProviders/giteeai.ts
@@ -6,28 +6,14 @@ const GiteeAI: ModelProviderCard = {
     {
       contextWindowTokens: 16_000,
       description:
-        'Qwen2.5-72B-Instruct 支持 16k 上下文, 生成长文本超过 8K 。支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。模型知识明显增加，并且大大提高了编码和数学能力, 多语言支持超过 29 种',
+        'Qwen2.5-72B-Instruct  支持 16k 上下文, 生成长文本超过 8K 。支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。模型知识明显增加，并且大大提高了编码和数学能力, 多语言支持超过 29 种',
       displayName: 'Qwen2.5 72B Instruct',
       enabled: true,
       functionCall: true,
       id: 'Qwen2.5-72B-Instruct',
     },
     {
-      description:
-        'Qwen2.5-Coder-32B-Instruct 是一款专为代码生成、代码理解和高效开发场景设计的大型语言模型，采用了业界领先的32B参数规模，能够满足多样化的编程需求。',
-      displayName: 'Qwen2.5 Coder 32B Instruct',
-      enabled: true,
-      id: 'Qwen2.5-Coder-32B-Instruct',
-    },
-    {
-      description:
-        'Qwen2.5-7B-Instruct 是一款 70 亿参数的大语言模型，支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。优化中文和多语言场景，支持智能问答、内容生成等应用。',
-      displayName: 'Qwen2.5 7B Instruct',
-      enabled: true,
-      functionCall: true,
-      id: 'Qwen2.5-7B-Instruct',
-    },
-    {
+      contextWindowTokens: 32_000,
       description:
         'Qwen2.5-32B-Instruct 是一款 320 亿参数的大语言模型，性能表现均衡，优化中文和多语言场景，支持智能问答、内容生成等应用。',
       displayName: 'Qwen2.5 32B Instruct',
@@ -35,6 +21,7 @@ const GiteeAI: ModelProviderCard = {
       id: 'Qwen2.5-32B-Instruct',
     },
     {
+      contextWindowTokens: 24_000,
       description:
         'Qwen2.5-14B-Instruct 是一款 140 亿参数的大语言模型，性能表现优秀，优化中文和多语言场景，支持智能问答、内容生成等应用。',
       displayName: 'Qwen2.5 14B Instruct',
@@ -42,35 +29,71 @@ const GiteeAI: ModelProviderCard = {
       id: 'Qwen2.5-14B-Instruct',
     },
     {
-      contextWindowTokens: 6000,
+      contextWindowTokens: 32_000,
       description:
-        'Qwen2 是 Qwen 模型的最新系列，支持 128k 上下文，对比当前最优的开源模型，Qwen2-72B 在自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型。',
+        'Qwen2.5-7B-Instruct 是一款 70 亿参数的大语言模型，支持 function call 与外部系统无缝交互，极大提升了灵活性和扩展性。优化中文和多语言场景，支持智能问答、内容生成等应用。',
+      displayName: 'Qwen2.5 7B Instruct',
+      enabled: true,
+      functionCall: true,
+      id: 'Qwen2.5-7B-Instruct',
+    },
+    {
+      contextWindowTokens: 32_000,
+      description:
+        'Qwen2 是 Qwen 模型的最新系列，对比当前最优的开源模型，Qwen2-72B 在自然语言理解、知识、代码、数学及多语言等多项能力上均显著超越当前领先的模型。',
       displayName: 'Qwen2 72B Instruct',
       id: 'Qwen2-72B-Instruct',
     },
     {
-      contextWindowTokens: 32_000,
+      contextWindowTokens: 24_000,
       description:
         'Qwen2 是 Qwen 模型的最新系列，能够超越同等规模的最优开源模型甚至更大规模的模型，Qwen2 7B 在多个评测上取得显著的优势，尤其是代码及中文理解上。',
       displayName: 'Qwen2 7B Instruct',
       id: 'Qwen2-7B-Instruct',
     },
     {
+      contextWindowTokens: 32_000,
       description:
-        'InternVL2-8B 是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
-      displayName: 'InternVL2 8B',
+        'Qwen2.5-Coder-32B-Instruct 是一款专为代码生成、代码理解和高效开发场景设计的大型语言模型，采用了业界领先的32B参数规模，能够满足多样化的编程需求。',
+      displayName: 'Qwen2.5 Coder 32B Instruct',
       enabled: true,
-      id: 'InternVL2-8B',
+      id: 'Qwen2.5-Coder-32B-Instruct',
+    },
+    {
+      contextWindowTokens: 24_000,
+      description:
+        'Qwen2.5-Coder-14B-Instruct 是一款基于大规模预训练的编程指令模型，具备强大的代码理解和生成能力，能够高效地处理各种编程任务，特别适合智能代码编写、自动化脚本生成和编程问题解答。',
+      displayName: 'Qwen2.5 Coder 14B Instruct',
+      enabled: true,
+      id: 'Qwen2.5-Coder-14B-Instruct',
+    },
+    {
+      contextWindowTokens: 32_000,
+      description:
+        'Qwen2-VL-72B是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
+      displayName: 'Qwen2 VL 72B',
+      enabled: true,
+      id: 'Qwen2-VL-72B',
       vision: true,
     },
     {
+      contextWindowTokens: 32_000,
       description:
-        'InternVL2.5-26B 是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
+        'InternVL2.5-26B是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
       displayName: 'InternVL2.5 26B',
       enabled: true,
       id: 'InternVL2.5-26B',
       vision: true,
     },
+    {
+      contextWindowTokens: 32_000,
+      description:
+        'InternVL2-8B 是一款强大的视觉语言模型，支持图像与文本的多模态处理，能够精确识别图像内容并生成相关描述或回答。',
+      displayName: 'InternVL2 8B',
+      enabled: true,
+      id: 'InternVL2-8B',
+      vision: true,
+    },
     {
       contextWindowTokens: 32_000,
       description:
@@ -82,28 +105,31 @@ const GiteeAI: ModelProviderCard = {
     {
       contextWindowTokens: 4000,
       description:
-        'Yi-1.5-34B 在保持原系列模型优秀的通用语言能力的前提下，通过增量训练 5 千亿高质量 token，大幅提高了数学逻辑、代码能力。',
+        'Yi-1.5-34B-Chat 在保持原系列模型优秀的通用语言能力的前提下，通过增量训练 5 千亿高质量 token，大幅提高了数学逻辑、代码能力。',
       displayName: 'Yi 34B Chat',
       enabled: true,
       id: 'Yi-34B-Chat',
     },
+/*
+    // not compatible with OpenAI SDK
     {
-      contextWindowTokens: 8000,
       description:
-        'DeepSeek Coder 33B 是一个代码语言模型， 基于 2 万亿数据训练而成，其中 87% 为代码， 13% 为中英文语言。模型引入 16K 窗口大小和填空任务，提供项目级别的代码补全和片段填充功能。',
-      displayName: 'DeepSeek Coder 33B Instruct',
+        '代码小浣熊是基于商汤大语言模型的软件智能研发助手，覆盖软件需求分析、架构设计、代码编写、软件测试等环节，满足用户代码编写、编程学习等各类需求。代码小浣熊支持 Python、Java、JavaScript、C++、Go、SQL 等 90+主流编程语言和 VS Code、IntelliJ IDEA 等主流 IDE。在实际应用中，代码小浣熊可帮助开发者提升编程效率超 50%。',
+      displayName: 'Code Raccoon v1',
       enabled: true,
-      id: 'deepseek-coder-33B-instruct',
+      id: 'code-raccoon-v1',
     },
+*/
     {
+      contextWindowTokens: 8000,
       description:
-        '代码小浣熊是基于商汤大语言模型的软件智能研发助手，覆盖软件需求分析、架构设计、代码编写、软件测试等环节，满足用户代码编写、编程学习等各类需求。代码小浣熊支持 Python、Java、JavaScript、C++、Go、SQL 等 90+主流编程语言和 VS Code、IntelliJ IDEA 等主流 IDE。在实际应用中，代码小浣熊可帮助开发者提升编程效率超 50%。',
-      displayName: 'code raccoon v1',
+        'DeepSeek Coder 33B 是一个代码语言模型， 基于 2 万亿数据训练而成，其中 87% 为代码， 13% 为中英文语言。模型引入 16K 窗口大小和填空任务，提供项目级别的代码补全和片段填充功能。',
+      displayName: 'DeepSeek Coder 33B Instruct',
       enabled: true,
-      id: 'code-raccoon-v1',
+      id: 'deepseek-coder-33B-instruct',
     },
     {
-      contextWindowTokens: 40_000,
+      contextWindowTokens: 32_000,
       description:
         'CodeGeeX4-ALL-9B 是一个多语言代码生成模型，支持包括代码补全和生成、代码解释器、网络搜索、函数调用、仓库级代码问答在内的全面功能，覆盖软件开发的各种场景。是参数少于 10B 的顶尖代码生成模型。',
       displayName: 'CodeGeeX4 All 9B',
diff --git a/src/config/modelProviders/huggingface.ts b/src/config/modelProviders/huggingface.ts
index ace8370720c0..cba039ac7414 100644
--- a/src/config/modelProviders/huggingface.ts
+++ b/src/config/modelProviders/huggingface.ts
@@ -49,13 +49,11 @@ const HuggingFace: ModelProviderCard = {
     'HuggingFace Inference API 提供了一种快速且免费的方式，让您可以探索成千上万种模型，适用于各种任务。无论您是在为新应用程序进行原型设计，还是在尝试机器学习的功能，这个 API 都能让您即时访问多个领域的高性能模型。',
   disableBrowserRequest: true,
   id: 'huggingface',
-  modelList: { showModelFetcher: true },
   modelsUrl: 'https://huggingface.co/docs/api-inference/en/supported-models',
   name: 'HuggingFace',
   settings: {
     disableBrowserRequest: true,
     sdkType: 'huggingface',
-    showModelFetcher: true,
   },
   url: 'https://huggingface.co',
 };
diff --git a/src/config/modelProviders/hunyuan.ts b/src/config/modelProviders/hunyuan.ts
index 684c385b2894..6581c25e9b02 100644
--- a/src/config/modelProviders/hunyuan.ts
+++ b/src/config/modelProviders/hunyuan.ts
@@ -135,13 +135,11 @@ const Hunyuan: ModelProviderCard = {
     '由腾讯研发的大语言模型，具备强大的中文创作能力，复杂语境下的逻辑推理能力，以及可靠的任务执行能力',
   disableBrowserRequest: true,
   id: 'hunyuan',
-  modelList: { showModelFetcher: true },
   modelsUrl: 'https://cloud.tencent.com/document/product/1729/104753',
   name: 'Hunyuan',
   settings: {
     disableBrowserRequest: true,
     sdkType: 'openai',
-    showModelFetcher: true,
   },
   url: 'https://hunyuan.tencent.com',
 };
diff --git a/src/config/modelProviders/mistral.ts b/src/config/modelProviders/mistral.ts
index 330efd7e0b25..4c45f53fa6a9 100644
--- a/src/config/modelProviders/mistral.ts
+++ b/src/config/modelProviders/mistral.ts
@@ -150,10 +150,12 @@ const Mistral: ModelProviderCard = {
   description:
     'Mistral 提供先进的通用、专业和研究型模型，广泛应用于复杂推理、多语言任务、代码生成等领域，通过功能调用接口，用户可以集成自定义功能，实现特定应用。',
   id: 'mistral',
+  modelList: { showModelFetcher: true },
   modelsUrl: 'https://docs.mistral.ai/getting-started/models',
   name: 'Mistral',
   settings: {
     sdkType: 'openai',
+    showModelFetcher: true,
   },
   url: 'https://mistral.ai',
 };
diff --git a/src/config/modelProviders/moonshot.ts b/src/config/modelProviders/moonshot.ts
index 7fbeaa9fa735..272eaebbbbf0 100644
--- a/src/config/modelProviders/moonshot.ts
+++ b/src/config/modelProviders/moonshot.ts
@@ -35,6 +35,7 @@ const Moonshot: ModelProviderCard = {
   description:
     'Moonshot 是由北京月之暗面科技有限公司推出的开源平台，提供多种自然语言处理模型，应用领域广泛，包括但不限于内容创作、学术研究、智能推荐、医疗诊断等，支持长文本处理和复杂生成任务。',
   id: 'moonshot',
+  modelList: { showModelFetcher: true },
   modelsUrl: 'https://platform.moonshot.cn/docs/intro',
   name: 'Moonshot',
   proxyUrl: {
@@ -45,6 +46,7 @@ const Moonshot: ModelProviderCard = {
       placeholder: 'https://api.moonshot.cn/v1',
     },
     sdkType: 'openai',
+    showModelFetcher: true,
     smoothing: {
       speed: 2,
       text: true,
diff --git a/src/config/modelProviders/spark.ts b/src/config/modelProviders/spark.ts
index 74ac52e65ddf..f2a53796d339 100644
--- a/src/config/modelProviders/spark.ts
+++ b/src/config/modelProviders/spark.ts
@@ -66,12 +66,10 @@ const Spark: ModelProviderCard = {
   description:
     '科大讯飞星火大模型提供多领域、多语言的强大 AI 能力，利用先进的自然语言处理技术，构建适用于智能硬件、智慧医疗、智慧金融等多种垂直场景的创新应用。',
   id: 'spark',
-  modelList: { showModelFetcher: true },
   modelsUrl: 'https://xinghuo.xfyun.cn/spark',
   name: 'Spark',
   settings: {
     sdkType: 'openai',
-    showModelFetcher: true,
     smoothing: {
       speed: 2,
       text: true,
diff --git a/src/config/modelProviders/taichu.ts b/src/config/modelProviders/taichu.ts
index cdf1359593aa..42d24e301e01 100644
--- a/src/config/modelProviders/taichu.ts
+++ b/src/config/modelProviders/taichu.ts
@@ -5,34 +5,39 @@ const Taichu: ModelProviderCard = {
   chatModels: [
     {
       contextWindowTokens: 32_768,
-      description: 'Taichu 2.0 基于海量高质数据训练，具有更强的文本理解、内容创作、对话问答等能力',
+      description: '基于海量高质数据训练，具有更强的文本理解、内容创作、对话问答等能力',
       displayName: 'Taichu 2.0',
       enabled: true,
       functionCall: true,
       id: 'taichu_llm',
+      pricing: {
+        currency: 'CNY',
+        input: 2,
+        output: 2,
+      },
     },
-    /*
-    // TODO: Not support for now
     {
-      description:
-        'Taichu 2.0V 融合了图像理解、知识迁移、逻辑归因等能力，在图文问答领域表现突出',
+      contextWindowTokens: 4096,
+      description: '融合了图像理解、知识迁移、逻辑归因等能力，在图文问答领域表现突出',
       displayName: 'Taichu 2.0V',
-      id: 'taichu_vqa',
-      tokens: 4096,
+      enabled: true,
+      id: 'taichu2_mm',
+      pricing: {
+        currency: 'CNY',
+        input: 5,
+        output: 5,
+      },
       vision: true,
     },
-*/
   ],
   checkModel: 'taichu_llm',
   description:
     '中科院自动化研究所和武汉人工智能研究院推出新一代多模态大模型，支持多轮问答、文本创作、图像生成、3D理解、信号分析等全面问答任务，拥有更强的认知、理解、创作能力，带来全新互动体验。',
   id: 'taichu',
-  modelList: { showModelFetcher: true },
   modelsUrl: 'https://ai-maas.wair.ac.cn/#/doc',
   name: 'Taichu',
   settings: {
     sdkType: 'openai',
-    showModelFetcher: true,
   },
   url: 'https://ai-maas.wair.ac.cn',
 };
diff --git a/src/config/modelProviders/upstage.ts b/src/config/modelProviders/upstage.ts
index 9ecd018ac7ee..f3ac38b71b48 100644
--- a/src/config/modelProviders/upstage.ts
+++ b/src/config/modelProviders/upstage.ts
@@ -21,7 +21,7 @@ const Upstage: ModelProviderCard = {
       id: 'solar-1-mini-chat-ja',
     },
     {
-      contextWindowTokens: 4096,
+      contextWindowTokens: 32_768,
       description:
         'Solar Pro 是 Upstage 推出的一款高智能LLM，专注于单GPU的指令跟随能力，IFEval得分80以上。目前支持英语，正式版本计划于2024年11月推出，将扩展语言支持和上下文长度。',
       displayName: 'Solar Pro',
@@ -34,12 +34,10 @@ const Upstage: ModelProviderCard = {
   description:
     'Upstage 专注于为各种商业需求开发AI模型，包括 Solar LLM 和文档 AI，旨在实现工作的人造通用智能（AGI）。通过 Chat API 创建简单的对话代理，并支持功能调用、翻译、嵌入以及特定领域应用。',
   id: 'upstage',
-  modelList: { showModelFetcher: true },
   modelsUrl: 'https://developers.upstage.ai/docs/getting-started/models',
   name: 'Upstage',
   settings: {
     sdkType: 'openai',
-    showModelFetcher: true,
   },
   url: 'https://upstage.ai',
 };
diff --git a/src/config/modelProviders/zeroone.ts b/src/config/modelProviders/zeroone.ts
index af3c3e1edba8..e267e503f2eb 100644
--- a/src/config/modelProviders/zeroone.ts
+++ b/src/config/modelProviders/zeroone.ts
@@ -141,9 +141,13 @@ const ZeroOne: ModelProviderCard = {
   description:
     '零一万物致力于推动以人为本的AI 2.0技术革命，旨在通过大语言模型创造巨大的经济和社会价值，并开创新的AI生态与商业模式。',
   id: 'zeroone',
+  modelList: { showModelFetcher: true },
   modelsUrl: 'https://platform.lingyiwanwu.com/docs#模型与计费',
   name: '01.AI',
-  settings: { sdkType: 'openai' },
+  settings: {
+    sdkType: 'openai',
+    showModelFetcher: true,
+  },
   url: 'https://www.lingyiwanwu.com/',
 };
 
diff --git a/src/config/modelProviders/zhipu.ts b/src/config/modelProviders/zhipu.ts
index 3af7b39b8d56..738d72c87fa6 100644
--- a/src/config/modelProviders/zhipu.ts
+++ b/src/config/modelProviders/zhipu.ts
@@ -5,6 +5,18 @@ import { ModelProviderCard } from '@/types/llm';
 // ref :https://open.bigmodel.cn/modelcenter/square
 const ZhiPu: ModelProviderCard = {
   chatModels: [
+    {
+      contextWindowTokens: 16_384,
+      description: 'GLM-Zero-Preview具备强大的复杂推理能力，在逻辑推理、数学、编程等领域表现优异。',
+      displayName: 'GLM-Zero-Preview',
+      enabled: true,
+      id: 'glm-zero-preview',
+      pricing: {
+        currency: 'CNY',
+        input: 10,
+        output: 10,
+      },
+    },
     {
       contextWindowTokens: 128_000,
       description: 'GLM-4-Flash 是处理简单任务的理想选择，速度最快且免费。',
@@ -199,9 +211,13 @@ const ZhiPu: ModelProviderCard = {
   description:
     '智谱 AI 提供多模态与语言模型的开放平台，支持广泛的AI应用场景，包括文本处理、图像理解与编程辅助等。',
   id: 'zhipu',
+  modelList: { showModelFetcher: true },
   modelsUrl: 'https://open.bigmodel.cn/dev/howuse/model',
   name: 'ZhiPu',
-  settings: { sdkType: 'openai' },
+  settings: {
+    sdkType: 'openai',
+    showModelFetcher: true
+  },
   url: 'https://zhipuai.cn',
 };
 
diff --git a/src/libs/agent-runtime/ai360/index.ts b/src/libs/agent-runtime/ai360/index.ts
index 175ad951040a..8cc18c87055d 100644
--- a/src/libs/agent-runtime/ai360/index.ts
+++ b/src/libs/agent-runtime/ai360/index.ts
@@ -1,6 +1,14 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface Ai360ModelCard {
+  id: string;
+  max_tokens: number;
+  total_tokens: number;
+}
+
 export const LobeAi360AI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.360.cn/v1',
   chatCompletion: {
@@ -14,5 +22,21 @@ export const LobeAi360AI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_AI360_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as Ai360ModelCard;
+
+      return {
+        contextWindowTokens: model.total_tokens,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: model.id === '360gpt-pro',
+        id: model.id,
+        maxTokens:
+          typeof model.max_tokens === 'number'
+            ? model.max_tokens
+            : undefined,
+      };
+    },
+  },
   provider: ModelProvider.Ai360,
 });
diff --git a/src/libs/agent-runtime/baichuan/index.ts b/src/libs/agent-runtime/baichuan/index.ts
index 3178977af4ac..649308edb7af 100644
--- a/src/libs/agent-runtime/baichuan/index.ts
+++ b/src/libs/agent-runtime/baichuan/index.ts
@@ -3,6 +3,17 @@ import OpenAI from 'openai';
 import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+import type { ChatModelCard } from '@/types/llm';
+
+export interface BaichuanModelCard {
+  function_call: boolean;
+  max_input_length: number;
+  max_tokens: number;
+  model: string;
+  model_show_name: string;
+}
+
 export const LobeBaichuanAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.baichuan-ai.com/v1',
   chatCompletion: {
@@ -20,5 +31,22 @@ export const LobeBaichuanAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_BAICHUAN_CHAT_COMPLETION === '1',
   },
+  models: async ({ client }) => {
+    const modelsPage = await client.models.list() as any;
+    const modelList: BaichuanModelCard[] = modelsPage.data;
+
+    return modelList
+      .map((model) => {
+        return {
+          contextWindowTokens: model.max_input_length,
+          displayName: model.model_show_name,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.model.endsWith(m.id))?.enabled || false,
+          functionCall: model.function_call,
+          id: model.model,
+          maxTokens: model.max_tokens,
+        };
+      })
+      .filter(Boolean) as ChatModelCard[];
+  },
   provider: ModelProvider.Baichuan,
 });
diff --git a/src/libs/agent-runtime/deepseek/index.ts b/src/libs/agent-runtime/deepseek/index.ts
index 9f312cbd28fd..5c29b0a8cf4b 100644
--- a/src/libs/agent-runtime/deepseek/index.ts
+++ b/src/libs/agent-runtime/deepseek/index.ts
@@ -1,10 +1,27 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface DeepSeekModelCard {
+  id: string;
+}
+
 export const LobeDeepSeekAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.deepseek.com/v1',
   debug: {
     chatCompletion: () => process.env.DEBUG_DEEPSEEK_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as DeepSeekModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: true,
+        id: model.id,
+      };
+    },
+  },
   provider: ModelProvider.DeepSeek,
 });
diff --git a/src/libs/agent-runtime/fireworksai/index.ts b/src/libs/agent-runtime/fireworksai/index.ts
index 3f3fe872d8fb..1ce62dfd14dc 100644
--- a/src/libs/agent-runtime/fireworksai/index.ts
+++ b/src/libs/agent-runtime/fireworksai/index.ts
@@ -1,10 +1,32 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface FireworksAIModelCard {
+  context_length: number;
+  id: string;
+  supports_image_input: boolean;
+  supports_tools: boolean;
+}
+
 export const LobeFireworksAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.fireworks.ai/inference/v1',
   debug: {
     chatCompletion: () => process.env.DEBUG_FIREWORKSAI_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as FireworksAIModelCard;
+
+      return {
+        contextWindowTokens: model.context_length,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: model.supports_tools || model.id.toLowerCase().includes('function'),
+        id: model.id,
+        vision: model.supports_image_input,
+      };
+    },
+  },
   provider: ModelProvider.FireworksAI,
 });
diff --git a/src/libs/agent-runtime/giteeai/index.ts b/src/libs/agent-runtime/giteeai/index.ts
index a84af7571a5f..e8749a12b3c0 100644
--- a/src/libs/agent-runtime/giteeai/index.ts
+++ b/src/libs/agent-runtime/giteeai/index.ts
@@ -1,10 +1,38 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface GiteeAIModelCard {
+  id: string;
+}
+
 export const LobeGiteeAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://ai.gitee.com/v1',
   debug: {
     chatCompletion: () => process.env.DEBUG_GITEE_AI_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const functionCallKeywords = [
+        'qwen2.5',
+        'glm-4',
+      ];
+
+      const visionKeywords = [
+        'internvl',
+        'qwen2-vl',
+      ];
+
+      const model = m as unknown as GiteeAIModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)) && !model.id.toLowerCase().includes('qwen2.5-coder'),
+        id: model.id,
+        vision: visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+      };
+    },
+  },
   provider: ModelProvider.GiteeAI,
 });
diff --git a/src/libs/agent-runtime/groq/index.ts b/src/libs/agent-runtime/groq/index.ts
index de6700fe7a8a..271014f12f9b 100644
--- a/src/libs/agent-runtime/groq/index.ts
+++ b/src/libs/agent-runtime/groq/index.ts
@@ -2,6 +2,13 @@ import { AgentRuntimeErrorType } from '../error';
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface GroqModelCard {
+  context_window: number;
+  id: string;
+}
+
 export const LobeGroq = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.groq.com/openai/v1',
   chatCompletion: {
@@ -24,5 +31,27 @@ export const LobeGroq = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_GROQ_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const functionCallKeywords = [
+        'tool',
+        'llama-3.3',
+        'llama-3.1',
+        'llama3-',
+        'mixtral-8x7b-32768',
+        'gemma2-9b-it',
+      ];
+
+      const model = m as unknown as GroqModelCard;
+
+      return {
+        contextWindowTokens: model.context_window,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+        id: model.id,
+        vision: model.id.toLowerCase().includes('vision'),
+      };
+    },
+  },
   provider: ModelProvider.Groq,
 });
diff --git a/src/libs/agent-runtime/internlm/index.ts b/src/libs/agent-runtime/internlm/index.ts
index 3dfaf0edda41..7610da9e3c2d 100644
--- a/src/libs/agent-runtime/internlm/index.ts
+++ b/src/libs/agent-runtime/internlm/index.ts
@@ -1,6 +1,12 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface InternLMModelCard {
+  id: string;
+}
+
 export const LobeInternLMAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://internlm-chat.intern-ai.org.cn/puyu/api/v1',
   chatCompletion: {
@@ -14,5 +20,16 @@ export const LobeInternLMAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_INTERNLM_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as InternLMModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: true,
+        id: model.id,
+      };
+    },
+  },
   provider: ModelProvider.InternLM,
 });
diff --git a/src/libs/agent-runtime/mistral/index.ts b/src/libs/agent-runtime/mistral/index.ts
index 73fb9d0461c1..723206e70e65 100644
--- a/src/libs/agent-runtime/mistral/index.ts
+++ b/src/libs/agent-runtime/mistral/index.ts
@@ -1,6 +1,18 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface MistralModelCard {
+  capabilities: {
+    function_calling: boolean;
+    vision: boolean;
+  };
+  description: string;
+  id: string;
+  max_context_length: number;
+}
+
 export const LobeMistralAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.mistral.ai/v1',
   chatCompletion: {
@@ -18,5 +30,19 @@ export const LobeMistralAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_MISTRAL_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as MistralModelCard;
+
+      return {
+        contextWindowTokens: model.max_context_length,
+        description: model.description,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: model.capabilities.function_calling,
+        id: model.id,
+        vision: model.capabilities.vision,
+      };
+    },
+  },
   provider: ModelProvider.Mistral,
 });
diff --git a/src/libs/agent-runtime/moonshot/index.ts b/src/libs/agent-runtime/moonshot/index.ts
index 28c976d9dcf5..9e024190762a 100644
--- a/src/libs/agent-runtime/moonshot/index.ts
+++ b/src/libs/agent-runtime/moonshot/index.ts
@@ -3,6 +3,12 @@ import OpenAI from 'openai';
 import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface MoonshotModelCard {
+  id: string;
+}
+
 export const LobeMoonshotAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.moonshot.cn/v1',
   chatCompletion: {
@@ -18,5 +24,16 @@ export const LobeMoonshotAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_MOONSHOT_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as MoonshotModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: true,
+        id: model.id,
+      };
+    },
+  },
   provider: ModelProvider.Moonshot,
 });
diff --git a/src/libs/agent-runtime/qwen/index.ts b/src/libs/agent-runtime/qwen/index.ts
index b0cc566f5b0b..0dbcac08168c 100644
--- a/src/libs/agent-runtime/qwen/index.ts
+++ b/src/libs/agent-runtime/qwen/index.ts
@@ -3,6 +3,12 @@ import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 import { QwenAIStream } from '../utils/streams';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface QwenModelCard {
+  id: string;
+}
+
 /*
   QwenLegacyModels: A set of legacy Qwen models that do not support presence_penalty.
   Currently, presence_penalty is only supported on Qwen commercial models and open-source models starting from Qwen 1.5 and later.
@@ -45,6 +51,25 @@ export const LobeQwenAI = LobeOpenAICompatibleFactory({
   },
   debug: {
     chatCompletion: () => process.env.DEBUG_QWEN_CHAT_COMPLETION === '1',
+  },
+    models: {
+    transformModel: (m) => {
+      const functionCallKeywords = [
+        'qwen-max',
+        'qwen-plus',
+        'qwen-turbo',
+        'qwen2.5',
+      ];
+
+      const model = m as unknown as QwenModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+        id: model.id,
+        vision: model.id.toLowerCase().includes('vl'),
+      };
+    },
   },
   provider: ModelProvider.Qwen,
 });
diff --git a/src/libs/agent-runtime/sensenova/index.ts b/src/libs/agent-runtime/sensenova/index.ts
index f551847a693c..ae7d174c6d21 100644
--- a/src/libs/agent-runtime/sensenova/index.ts
+++ b/src/libs/agent-runtime/sensenova/index.ts
@@ -1,6 +1,13 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+import type { ChatModelCard } from '@/types/llm';
+
+export interface SenseNovaModelCard {
+  id: string;
+}
+
 export const LobeSenseNovaAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.sensenova.cn/compatible-mode/v1',
   chatCompletion: {
@@ -25,5 +32,26 @@ export const LobeSenseNovaAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_SENSENOVA_CHAT_COMPLETION === '1',
   },
+  models: async ({ client }) => {
+    const functionCallKeywords = [
+      'sensechat-5',
+    ];
+
+    client.baseURL = 'https://api.sensenova.cn/v1/llm';
+
+    const modelsPage = await client.models.list() as any;
+    const modelList: SenseNovaModelCard[] = modelsPage.data;
+
+    return modelList
+      .map((model) => {
+        return {
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+          functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+          id: model.id,
+          vision: model.id.toLowerCase().includes('vision'),
+        };
+      })
+      .filter(Boolean) as ChatModelCard[];
+  },
   provider: ModelProvider.SenseNova,
 });
diff --git a/src/libs/agent-runtime/siliconcloud/index.ts b/src/libs/agent-runtime/siliconcloud/index.ts
index edc28f455e0f..f3b6495f1f87 100644
--- a/src/libs/agent-runtime/siliconcloud/index.ts
+++ b/src/libs/agent-runtime/siliconcloud/index.ts
@@ -1,6 +1,12 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface SiliconCloudModelCard {
+  id: string;
+}
+
 export const LobeSiliconCloudAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.siliconflow.cn/v1',
   chatCompletion: {
@@ -14,5 +20,33 @@ export const LobeSiliconCloudAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_SILICONCLOUD_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const functionCallKeywords = [
+        'qwen/qwen2.5',
+        'thudm/glm-4',
+        'deepseek-ai/deepSeek',
+        'internlm/internlm2_5',
+        'meta-llama/meta-llama-3.1',
+        'meta-llama/meta-llama-3.3',
+      ];
+
+      const visionKeywords = [
+        'opengvlab/internvl',
+        'qwen/qwen2-vl',
+        'teleai/telemm',
+        'deepseek-ai/deepseek-vl',
+      ];
+
+      const model = m as unknown as SiliconCloudModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+        id: model.id,
+        vision: visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+      };
+    },
+  },
   provider: ModelProvider.SiliconCloud,
 });
diff --git a/src/libs/agent-runtime/stepfun/index.ts b/src/libs/agent-runtime/stepfun/index.ts
index 4ae98b6fe3c7..617ef9aa0429 100644
--- a/src/libs/agent-runtime/stepfun/index.ts
+++ b/src/libs/agent-runtime/stepfun/index.ts
@@ -1,6 +1,12 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface StepfunModelCard {
+  id: string;
+}
+
 export const LobeStepfunAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.stepfun.com/v1',
   chatCompletion: {
@@ -14,5 +20,17 @@ export const LobeStepfunAI = LobeOpenAICompatibleFactory({
   debug: {
     chatCompletion: () => process.env.DEBUG_STEPFUN_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as StepfunModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: model.id !== 'step-1.5v-mini',
+        id: model.id,
+        vision: model.id.toLowerCase().includes('v'),
+      };
+    },
+  },
   provider: ModelProvider.Stepfun,
 });
diff --git a/src/libs/agent-runtime/xai/index.ts b/src/libs/agent-runtime/xai/index.ts
index ed52caa342b4..fe896a5eba97 100644
--- a/src/libs/agent-runtime/xai/index.ts
+++ b/src/libs/agent-runtime/xai/index.ts
@@ -1,10 +1,28 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface XAIModelCard {
+  id: string;
+}
+
 export const LobeXAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.x.ai/v1',
   debug: {
     chatCompletion: () => process.env.DEBUG_XAI_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as XAIModelCard;
+
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: true,
+        id: model.id,
+        vision: model.id.toLowerCase().includes('vision'),
+      };
+    },
+  },
   provider: ModelProvider.XAI,
 });
diff --git a/src/libs/agent-runtime/zeroone/index.ts b/src/libs/agent-runtime/zeroone/index.ts
index 18c0f0a9d5d7..50f1cd48eed4 100644
--- a/src/libs/agent-runtime/zeroone/index.ts
+++ b/src/libs/agent-runtime/zeroone/index.ts
@@ -1,11 +1,28 @@
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+
+export interface ZeroOneModelCard {
+  id: string;
+}
+
 export const LobeZeroOneAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.lingyiwanwu.com/v1',
   debug: {
     chatCompletion: () => process.env.DEBUG_ZEROONE_CHAT_COMPLETION === '1',
   },
+  models: {
+    transformModel: (m) => {
+      const model = m as unknown as ZeroOneModelCard;
 
+      return {
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        functionCall: model.id.toLowerCase().includes('fc'),
+        id: model.id,
+        vision: model.id.toLowerCase().includes('vision'),
+      };
+    },
+  },
   provider: ModelProvider.ZeroOne,
 });
diff --git a/src/libs/agent-runtime/zhipu/index.ts b/src/libs/agent-runtime/zhipu/index.ts
index b49060e0166f..08c35e09c44c 100644
--- a/src/libs/agent-runtime/zhipu/index.ts
+++ b/src/libs/agent-runtime/zhipu/index.ts
@@ -3,6 +3,15 @@ import OpenAI from 'openai';
 import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
+import type { ChatModelCard } from '@/types/llm';
+
+export interface ZhipuModelCard {
+  description: string;
+  modelCode: string;
+  modelName: string;
+}
+
 export const LobeZhipuAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://open.bigmodel.cn/api/paas/v4',
   chatCompletion: {
@@ -25,8 +34,34 @@ export const LobeZhipuAI = LobeOpenAICompatibleFactory({
             }),
       }) as OpenAI.ChatCompletionCreateParamsStreaming,
   },
+  constructorOptions: {
+    defaultHeaders: {
+      'Bigmodel-Organization': 'lobehub',
+      'Bigmodel-project': 'lobechat',
+    },
+  },
   debug: {
     chatCompletion: () => process.env.DEBUG_ZHIPU_CHAT_COMPLETION === '1',
   },
+  models: async ({ client }) => {
+    // ref: https://open.bigmodel.cn/console/modelcenter/square
+    client.baseURL = 'https://open.bigmodel.cn/api/fine-tuning/model_center/list?pageSize=100&pageNum=1';
+
+    const modelsPage = await client.models.list() as any;
+    const modelList: ZhipuModelCard[] = modelsPage.body.rows;
+
+    return modelList
+      .map((model) => {
+        return {
+          description: model.description,
+          displayName: model.modelName,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.modelCode.endsWith(m.id))?.enabled || false,
+          functionCall: model.modelCode.toLowerCase().includes('glm-4') && !model.modelCode.toLowerCase().includes('glm-4v'),
+          id: model.modelCode,
+          vision: model.modelCode.toLowerCase().includes('glm-4v'),
+        };
+      })
+      .filter(Boolean) as ChatModelCard[];
+  },
   provider: ModelProvider.ZhiPu,
 });
