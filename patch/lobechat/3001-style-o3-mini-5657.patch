diff --git a/docs/usage/agents/model.mdx b/docs/usage/agents/model.mdx
index d1742a962e38c..e8056cbbae7c6 100644
--- a/docs/usage/agents/model.mdx
+++ b/docs/usage/agents/model.mdx
@@ -77,3 +77,19 @@ It is a mechanism that penalizes frequently occurring new vocabulary in the text
 - `0.0` When the morning sun poured into the small diner, a tired postman appeared at the door, carrying a bag of letters in his hands. The owner warmly prepared a breakfast for him, and he started sorting the mail while enjoying his breakfast. **(The highest frequency word is "of", accounting for 8.45%)**
 - `1.0` A girl in deep sleep was woken up by a warm ray of sunshine, she saw the first ray of morning light, surrounded by birdsong and flowers, everything was full of vitality. (The highest frequency word is "of", accounting for 5.45%)
 - `2.0` Every morning, he would sit on the balcony to have breakfast. Under the soft setting sun, everything looked very peaceful. However, one day, when he was about to pick up his breakfast, an optimistic little bird flew by, bringing him a good mood for the day.  (The highest frequency word is "of", accounting for 4.94%)
+
+<br />
+
+### `reasoning_effort`
+
+The `reasoning_effort` parameter controls the strength of the reasoning process. This setting affects the depth of reasoning the model performs when generating a response. The available values are **`low`**, **`medium`**, and **`high`**, with the following meanings:
+
+- **low**: Lower reasoning effort, resulting in faster response times. Suitable for scenarios where quick responses are needed, but it may sacrifice some reasoning accuracy.
+- **medium** (default): Balances reasoning accuracy and response speed, suitable for most scenarios.
+- **high**: Higher reasoning effort, producing more detailed and complex responses, but slower response times and greater token consumption.
+
+By adjusting the `reasoning_effort` parameter, you can find an appropriate balance between response speed and reasoning depth based on your needs. For example, in conversational scenarios, if fast responses are a priority, you can choose low reasoning effort; if more complex analysis or reasoning is needed, you can opt for high reasoning effort.
+
+<Callout>
+  This parameter is only applicable to reasoning models, such as OpenAI's `o1`, `o1-mini`, `o3-mini`, etc.
+</Callout>
diff --git a/docs/usage/agents/model.zh-CN.mdx b/docs/usage/agents/model.zh-CN.mdx
index a9659fae58015..578dd4096ce5a 100644
--- a/docs/usage/agents/model.zh-CN.mdx
+++ b/docs/usage/agents/model.zh-CN.mdx
@@ -72,3 +72,19 @@ Presence Penalty 参数可以看作是对生成文本中重复内容的一种惩
 - `0.0` 当清晨的阳光洒进小餐馆时，一名疲倦的邮递员出现在门口，他的手中提着一袋信件。店主热情地为他准备了一份早餐，他在享用早餐的同时开始整理邮件。**（频率最高的词是 “的”，占比 8.45%）**
 - `1.0` 一个深度睡眠的女孩被一阵温暖的阳光唤醒，她看到了早晨的第一缕阳光，周围是鸟语花香，一切都充满了生机。*（频率最高的词是 “的”，占比 5.45%）*
 - `2.0` 每天早上，他都会在阳台上坐着吃早餐。在柔和的夕阳照耀下，一切看起来都非常宁静。然而有一天，当他准备端起早餐的时候，一只乐观的小鸟飞过，给他带来了一天的好心情。 *（频率最高的词是 “的”，占比 4.94%）*
+
+<br />
+
+### `reasoning_effort`
+
+`reasoning_effort` 参数用于控制推理过程的强度。此参数的设置会影响模型在生成回答时的推理深度。可选值包括 **`low`**、**`medium`** 和 **`high`**，具体含义如下：
+
+- **low（低）**：推理强度较低，生成速度较快，适用于需要快速响应的场景，但可能牺牲一定的推理精度。
+- **medium（中，默认值）**：平衡推理精度与响应速度，适用于大多数场景。
+- **high（高）**：推理强度较高，生成更为详细和复杂的回答，但响应时间较长，且消耗更多的 Token。
+
+通过调整 `reasoning_effort` 参数，可以根据需求在生成速度与推理深度之间找到适合的平衡。例如，在对话场景中，如果更关注快速响应，可以选择低推理强度；如果需要更复杂的分析或推理，可以选择高推理强度。
+
+<Callout>
+  该参数仅适用于推理模型，如 OpenAI 的 `o1`、`o1-mini`、`o3-mini` 等。
+</Callout>
diff --git a/locales/ar/discover.json b/locales/ar/discover.json
index a3ab309e5978a..c3d1dd550a028 100644
--- a/locales/ar/discover.json
+++ b/locales/ar/discover.json
@@ -126,6 +126,10 @@
         "title": "جدة الموضوع"
       },
       "range": "نطاق",
+      "reasoning_effort": {
+        "desc": "تُستخدم هذه الإعدادات للتحكم في شدة التفكير التي يقوم بها النموذج قبل توليد الإجابات. الشدة المنخفضة تعطي الأولوية لسرعة الاستجابة وتوفر الرموز، بينما الشدة العالية توفر تفكيرًا أكثر اكتمالًا ولكنها تستهلك المزيد من الرموز وتقلل من سرعة الاستجابة. القيمة الافتراضية هي متوسطة، مما يوازن بين دقة التفكير وسرعة الاستجابة.",
+        "title": "شدة التفكير"
+      },
       "temperature": {
         "desc": "تؤثر هذه الإعدادات على تنوع استجابة النموذج. القيم المنخفضة تؤدي إلى استجابات أكثر توقعًا ونمطية، بينما القيم الأعلى تشجع على استجابات أكثر تنوعًا وغير شائعة. عندما تكون القيمة 0، يعطي النموذج نفس الاستجابة دائمًا لنفس المدخل.",
         "title": "عشوائية"
diff --git a/locales/ar/models.json b/locales/ar/models.json
index fec55d9108224..899c630b89997 100644
--- a/locales/ar/models.json
+++ b/locales/ar/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 هو نموذج استدلال جديد من OpenAI، مناسب للمهام المعقدة التي تتطلب معرفة عامة واسعة. يحتوي هذا النموذج على 128K من السياق وتاريخ انتهاء المعرفة في أكتوبر 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini هو أحدث نموذج استدلال صغير لدينا، يقدم ذكاءً عالياً تحت نفس تكاليف التأخير والأداء مثل o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba هو نموذج لغة Mamba 2 يركز على توليد الشيفرة، ويوفر دعمًا قويًا لمهام الشيفرة المتقدمة والاستدلال."
   },
diff --git a/locales/ar/setting.json b/locales/ar/setting.json
index e01d443ccd2d9..bd62b9c7f4658 100644
--- a/locales/ar/setting.json
+++ b/locales/ar/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "تمكين الحد الأقصى للردود"
     },
+    "enableReasoningEffort": {
+      "title": "تفعيل ضبط قوة الاستدلال"
+    },
     "frequencyPenalty": {
       "desc": "كلما زادت القيمة، زاد احتمال تقليل تكرار الكلمات",
       "title": "عقوبة التكرار"
@@ -216,6 +219,15 @@
       "desc": "كلما زادت القيمة، زاد احتمال التوسع في مواضيع جديدة",
       "title": "جديد الحديث"
     },
+    "reasoningEffort": {
+      "desc": "كلما زادت القيمة، زادت قدرة الاستدلال، ولكن قد يؤدي ذلك إلى زيادة وقت الاستجابة واستهلاك التوكنات",
+      "options": {
+        "high": "عالي",
+        "low": "منخفض",
+        "medium": "متوسط"
+      },
+      "title": "قوة الاستدلال"
+    },
     "temperature": {
       "desc": "كلما زادت القيمة، زادت الردود عشوائية أكثر",
       "title": "التباين",
diff --git a/locales/bg-BG/discover.json b/locales/bg-BG/discover.json
index b3e924c3341c8..a3cdd7736fcea 100644
--- a/locales/bg-BG/discover.json
+++ b/locales/bg-BG/discover.json
@@ -126,6 +126,10 @@
         "title": "Свежест на темата"
       },
       "range": "Обхват",
+      "reasoning_effort": {
+        "desc": "Тази настройка контролира интензивността на разсъжденията на модела преди генерирането на отговор. Ниска интензивност приоритизира скоростта на отговор и спестява токени, докато висока интензивност предоставя по-пълни разсъждения, но изразходва повече токени и намалява скоростта на отговор. Стойността по подразбиране е средна, което балансира точността на разсъжденията и скоростта на отговор.",
+        "title": "Интензивност на разсъжденията"
+      },
       "temperature": {
         "desc": "Тази настройка влияе на разнообразието на отговорите на модела. По-ниски стойности водят до по-предсказуеми и типични отговори, докато по-високи стойности насърчават по-разнообразни и необичайни отговори. Когато стойността е 0, моделът винаги дава един и същ отговор на даден вход.",
         "title": "Случайност"
diff --git a/locales/bg-BG/models.json b/locales/bg-BG/models.json
index 7a88d9c7d5259..f69c70c9fd6cc 100644
--- a/locales/bg-BG/models.json
+++ b/locales/bg-BG/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 е новият модел за изводи на OpenAI, подходящ за сложни задачи, изискващи обширни общи знания. Моделът разполага с контекст от 128K и дата на знание до октомври 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini е нашият най-нов малък модел за инференция, който предлага висока интелигентност при същите разходи и цели за закъснение като o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba е модел на езика Mamba 2, специализиран в генерирането на код, предоставящ мощна поддръжка за напреднали кодови и разсъждателни задачи."
   },
diff --git a/locales/bg-BG/setting.json b/locales/bg-BG/setting.json
index 64a396ecd0d84..c756193aa6687 100644
--- a/locales/bg-BG/setting.json
+++ b/locales/bg-BG/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Активиране на ограничението за максимален брой токени"
     },
+    "enableReasoningEffort": {
+      "title": "Активиране на настройките за интензивност на разсъжденията"
+    },
     "frequencyPenalty": {
       "desc": "Колкото по-висока е стойността, толкова по-вероятно е да се намалят повтарящите се думи",
       "title": "Наказание за честота"
@@ -216,6 +219,15 @@
       "desc": "Колкото по-висока е стойността, толкова по-вероятно е да се разшири до нови теми",
       "title": "Свежест на темата"
     },
+    "reasoningEffort": {
+      "desc": "Колкото по-висока е стойността, толкова по-силна е способността за разсъждение, но може да увеличи времето за отговор и консумацията на токени",
+      "options": {
+        "high": "Висока",
+        "low": "Ниска",
+        "medium": "Средна"
+      },
+      "title": "Интензивност на разсъжденията"
+    },
     "temperature": {
       "desc": "Колкото по-висока е стойността, толкова по-случаен е отговорът",
       "title": "Случайност",
diff --git a/locales/de-DE/discover.json b/locales/de-DE/discover.json
index 4102d2df10bd9..789c094d43086 100644
--- a/locales/de-DE/discover.json
+++ b/locales/de-DE/discover.json
@@ -126,6 +126,10 @@
         "title": "Themenfrische"
       },
       "range": "Bereich",
+      "reasoning_effort": {
+        "desc": "Diese Einstellung steuert die Intensität des Denkprozesses des Modells, bevor es eine Antwort generiert. Eine niedrige Intensität priorisiert die Geschwindigkeit der Antwort und spart Token, während eine hohe Intensität eine umfassendere Argumentation bietet, jedoch mehr Token verbraucht und die Antwortgeschwindigkeit verringert. Der Standardwert ist mittel, um eine Balance zwischen Genauigkeit des Denkens und Antwortgeschwindigkeit zu gewährleisten.",
+        "title": "Denkintensität"
+      },
       "temperature": {
         "desc": "Diese Einstellung beeinflusst die Vielfalt der Antworten des Modells. Niedrigere Werte führen zu vorhersehbareren und typischen Antworten, während höhere Werte zu vielfältigeren und weniger häufigen Antworten anregen. Wenn der Wert auf 0 gesetzt wird, gibt das Modell für einen bestimmten Input immer die gleiche Antwort.",
         "title": "Zufälligkeit"
diff --git a/locales/de-DE/models.json b/locales/de-DE/models.json
index 7164c4c6dc413..ff7e0bc898e8a 100644
--- a/locales/de-DE/models.json
+++ b/locales/de-DE/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 ist OpenAIs neues Inferenzmodell, das für komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini ist unser neuestes kompaktes Inferenzmodell, das bei den gleichen Kosten- und Verzögerungszielen wie o1-mini hohe Intelligenz bietet."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba ist ein auf die Codegenerierung spezialisiertes Mamba 2-Sprachmodell, das starke Unterstützung für fortschrittliche Code- und Schlussfolgerungsaufgaben bietet."
   },
diff --git a/locales/de-DE/setting.json b/locales/de-DE/setting.json
index 86a5e973a152d..acba3440b23a5 100644
--- a/locales/de-DE/setting.json
+++ b/locales/de-DE/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Maximale Token pro Antwort aktivieren"
     },
+    "enableReasoningEffort": {
+      "title": "Aktivieren Sie die Anpassung der Schlussfolgerungsintensität"
+    },
     "frequencyPenalty": {
       "desc": "Je höher der Wert, desto wahrscheinlicher ist es, dass sich wiederholende Wörter reduziert werden",
       "title": "Frequenzstrafe"
@@ -216,6 +219,15 @@
       "desc": "Je höher der Wert, desto wahrscheinlicher ist es, dass sich das Gespräch auf neue Themen ausweitet",
       "title": "Themenfrische"
     },
+    "reasoningEffort": {
+      "desc": "Je höher der Wert, desto stärker die Schlussfolgerungsfähigkeit, aber dies kann die Antwortzeit und den Tokenverbrauch erhöhen.",
+      "options": {
+        "high": "Hoch",
+        "low": "Niedrig",
+        "medium": "Mittel"
+      },
+      "title": "Schlussfolgerungsintensität"
+    },
     "temperature": {
       "desc": "Je höher der Wert, desto zufälliger die Antwort",
       "title": "Zufälligkeit",
diff --git a/locales/en-US/discover.json b/locales/en-US/discover.json
index 84e71d0f6c367..3d2a702acad2a 100644
--- a/locales/en-US/discover.json
+++ b/locales/en-US/discover.json
@@ -126,6 +126,10 @@
         "title": "Topic Freshness"
       },
       "range": "Range",
+      "reasoning_effort": {
+        "desc": "This setting controls the intensity of reasoning the model applies before generating a response. Low intensity prioritizes response speed and saves tokens, while high intensity provides more comprehensive reasoning but consumes more tokens and slows down response time. The default value is medium, balancing reasoning accuracy with response speed.",
+        "title": "Reasoning Intensity"
+      },
       "temperature": {
         "desc": "This setting affects the diversity of the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. When set to 0, the model always gives the same response to a given input.",
         "title": "Randomness"
diff --git a/locales/en-US/models.json b/locales/en-US/models.json
index 0f16a2bb07800..3b16f1769906a 100644
--- a/locales/en-US/models.json
+++ b/locales/en-US/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 is OpenAI's new reasoning model, suitable for complex tasks that require extensive general knowledge. This model features a 128K context and has a knowledge cutoff date of October 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini is our latest small inference model that delivers high intelligence while maintaining the same cost and latency targets as o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba is a language model focused on code generation, providing strong support for advanced coding and reasoning tasks."
   },
diff --git a/locales/en-US/setting.json b/locales/en-US/setting.json
index 10216ebad5f22..80ffae76e5a61 100644
--- a/locales/en-US/setting.json
+++ b/locales/en-US/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Enable Max Tokens Limit"
     },
+    "enableReasoningEffort": {
+      "title": "Enable Reasoning Effort Adjustment"
+    },
     "frequencyPenalty": {
       "desc": "The higher the value, the more likely it is to reduce repeated words",
       "title": "Frequency Penalty"
@@ -216,6 +219,15 @@
       "desc": "The higher the value, the more likely it is to expand to new topics",
       "title": "Topic Freshness"
     },
+    "reasoningEffort": {
+      "desc": "The higher the value, the stronger the reasoning ability, but it may increase response time and token consumption.",
+      "options": {
+        "high": "High",
+        "low": "Low",
+        "medium": "Medium"
+      },
+      "title": "Reasoning Effort"
+    },
     "temperature": {
       "desc": "The higher the value, the more random the response",
       "title": "Randomness",
diff --git a/locales/es-ES/discover.json b/locales/es-ES/discover.json
index 42386cb6e2230..1caad337e1e91 100644
--- a/locales/es-ES/discover.json
+++ b/locales/es-ES/discover.json
@@ -126,6 +126,10 @@
         "title": "Novedad del tema"
       },
       "range": "Rango",
+      "reasoning_effort": {
+        "desc": "Esta configuración se utiliza para controlar la intensidad de razonamiento del modelo antes de generar una respuesta. Una baja intensidad prioriza la velocidad de respuesta y ahorra tokens, mientras que una alta intensidad proporciona un razonamiento más completo, pero consume más tokens y reduce la velocidad de respuesta. El valor predeterminado es medio, equilibrando la precisión del razonamiento con la velocidad de respuesta.",
+        "title": "Intensidad de razonamiento"
+      },
       "temperature": {
         "desc": "Esta configuración afecta la diversidad de las respuestas del modelo. Un valor más bajo resultará en respuestas más predecibles y típicas, mientras que un valor más alto alentará respuestas más diversas y menos comunes. Cuando el valor se establece en 0, el modelo siempre dará la misma respuesta para una entrada dada.",
         "title": "Aleatoriedad"
diff --git a/locales/es-ES/models.json b/locales/es-ES/models.json
index c266330ee4175..a9adf9e40954f 100644
--- a/locales/es-ES/models.json
+++ b/locales/es-ES/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 es el nuevo modelo de inferencia de OpenAI, adecuado para tareas complejas que requieren un amplio conocimiento general. Este modelo tiene un contexto de 128K y una fecha de corte de conocimiento en octubre de 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini es nuestro último modelo de inferencia de tamaño pequeño, que ofrece alta inteligencia con los mismos objetivos de costo y latencia que o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba es un modelo de lenguaje Mamba 2 enfocado en la generación de código, que proporciona un fuerte apoyo para tareas avanzadas de codificación y razonamiento."
   },
diff --git a/locales/es-ES/setting.json b/locales/es-ES/setting.json
index c1ac3233f9434..acd091cfb8938 100644
--- a/locales/es-ES/setting.json
+++ b/locales/es-ES/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Activar límite de tokens por respuesta"
     },
+    "enableReasoningEffort": {
+      "title": "Activar ajuste de intensidad de razonamiento"
+    },
     "frequencyPenalty": {
       "desc": "Cuanto mayor sea el valor, más probable es que se reduzcan las repeticiones de palabras",
       "title": "Penalización de frecuencia"
@@ -216,6 +219,15 @@
       "desc": "Cuanto mayor sea el valor, más probable es que se amplíe a nuevos temas",
       "title": "Penalización de novedad del tema"
     },
+    "reasoningEffort": {
+      "desc": "Cuanto mayor sea el valor, más fuerte será la capacidad de razonamiento, pero puede aumentar el tiempo de respuesta y el consumo de tokens.",
+      "options": {
+        "high": "Alto",
+        "low": "Bajo",
+        "medium": "Medio"
+      },
+      "title": "Intensidad de razonamiento"
+    },
     "temperature": {
       "desc": "Cuanto mayor sea el valor, más aleatoria será la respuesta",
       "title": "Temperatura",
diff --git a/locales/fa-IR/discover.json b/locales/fa-IR/discover.json
index c44d426d0b42e..486075494a6a7 100644
--- a/locales/fa-IR/discover.json
+++ b/locales/fa-IR/discover.json
@@ -126,6 +126,10 @@
         "title": "تازگی موضوع"
       },
       "range": "محدوده",
+      "reasoning_effort": {
+        "desc": "این تنظیم برای کنترل شدت استدلال مدل قبل از تولید پاسخ استفاده می‌شود. شدت پایین به سرعت پاسخ‌دهی اولویت می‌دهد و توکن را صرفه‌جویی می‌کند، در حالی که شدت بالا استدلال کامل‌تری ارائه می‌دهد اما توکن بیشتری مصرف کرده و سرعت پاسخ‌دهی را کاهش می‌دهد. مقدار پیش‌فرض متوسط است که تعادل بین دقت استدلال و سرعت پاسخ‌دهی را برقرار می‌کند.",
+        "title": "شدت استدلال"
+      },
       "temperature": {
         "desc": "این تنظیمات بر تنوع پاسخ‌های مدل تأثیر می‌گذارد. مقادیر پایین‌تر منجر به پاسخ‌های قابل پیش‌بینی‌تر و معمولی‌تر می‌شود، در حالی که مقادیر بالاتر تنوع و پاسخ‌های غیرمعمول‌تر را تشویق می‌کند. وقتی مقدار به 0 تنظیم شود، مدل همیشه برای ورودی داده شده یک پاسخ یکسان ارائه می‌دهد.",
         "title": "تصادفی بودن"
diff --git a/locales/fa-IR/models.json b/locales/fa-IR/models.json
index 86b7f02ee6897..c55db47309dff 100644
--- a/locales/fa-IR/models.json
+++ b/locales/fa-IR/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "تمرکز بر استدلال پیشرفته و حل مسائل پیچیده، از جمله وظایف ریاضی و علمی. بسیار مناسب برای برنامه‌هایی که نیاز به درک عمیق از زمینه و جریان کاری خودمختار دارند."
   },
+  "o3-mini": {
+    "description": "o3-mini جدیدترین مدل استنتاج کوچک ماست که هوش بالایی را با هزینه و هدف تأخیر مشابه o1-mini ارائه می‌دهد."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba یک مدل زبان Mamba 2 است که بر تولید کد تمرکز دارد و پشتیبانی قدرتمندی برای وظایف پیشرفته کدنویسی و استدلال ارائه می‌دهد."
   },
diff --git a/locales/fa-IR/setting.json b/locales/fa-IR/setting.json
index 7f994d979b265..da8fa8c175268 100644
--- a/locales/fa-IR/setting.json
+++ b/locales/fa-IR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "فعال‌سازی محدودیت پاسخ"
     },
+    "enableReasoningEffort": {
+      "title": "فعال‌سازی تنظیم شدت استدلال"
+    },
     "frequencyPenalty": {
       "desc": "هرچه مقدار بیشتر باشد، احتمال کاهش تکرار کلمات بیشتر است",
       "title": "مجازات تکرار"
@@ -216,6 +219,15 @@
       "desc": "هرچه مقدار بیشتر باشد، احتمال گسترش به موضوعات جدید بیشتر است",
       "title": "تازگی موضوع"
     },
+    "reasoningEffort": {
+      "desc": "هرچه مقدار بیشتر باشد، توانایی استدلال قوی‌تر است، اما ممکن است زمان پاسخ و مصرف توکن را افزایش دهد",
+      "options": {
+        "high": "بالا",
+        "low": "پایین",
+        "medium": "متوسط"
+      },
+      "title": "شدت استدلال"
+    },
     "temperature": {
       "desc": "هرچه مقدار بیشتر باشد، پاسخ‌ها تصادفی‌تر خواهند بود",
       "title": "تصادفی بودن",
diff --git a/locales/fr-FR/discover.json b/locales/fr-FR/discover.json
index 508b8d010bb06..5fb3a7588ef88 100644
--- a/locales/fr-FR/discover.json
+++ b/locales/fr-FR/discover.json
@@ -126,6 +126,10 @@
         "title": "Fraîcheur des sujets"
       },
       "range": "Plage",
+      "reasoning_effort": {
+        "desc": "Ce paramètre contrôle l'intensité de raisonnement du modèle avant de générer une réponse. Une faible intensité privilégie la rapidité de réponse et économise des tokens, tandis qu'une forte intensité offre un raisonnement plus complet, mais consomme plus de tokens et ralentit la réponse. La valeur par défaut est moyenne, équilibrant précision du raisonnement et rapidité de réponse.",
+        "title": "Intensité de raisonnement"
+      },
       "temperature": {
         "desc": "Ce paramètre influence la diversité des réponses du modèle. Des valeurs plus basses entraînent des réponses plus prévisibles et typiques, tandis que des valeurs plus élevées encouragent des réponses plus variées et moins courantes. Lorsque la valeur est fixée à 0, le modèle donne toujours la même réponse pour une entrée donnée.",
         "title": "Aléatoire"
diff --git a/locales/fr-FR/models.json b/locales/fr-FR/models.json
index d5ff7dd909ff8..e4b81db7b23aa 100644
--- a/locales/fr-FR/models.json
+++ b/locales/fr-FR/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 est le nouveau modèle de raisonnement d'OpenAI, adapté aux tâches complexes nécessitant une vaste connaissance générale. Ce modèle dispose d'un contexte de 128K et d'une date limite de connaissance en octobre 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini est notre dernier modèle d'inférence compact, offrant une grande intelligence avec les mêmes objectifs de coût et de latence que o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba est un modèle de langage Mamba 2 axé sur la génération de code, offrant un soutien puissant pour des tâches avancées de codage et de raisonnement."
   },
diff --git a/locales/fr-FR/setting.json b/locales/fr-FR/setting.json
index 1b8cd51a407b7..deb9575a0f7c1 100644
--- a/locales/fr-FR/setting.json
+++ b/locales/fr-FR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Activer la limite de tokens par réponse"
     },
+    "enableReasoningEffort": {
+      "title": "Activer l'ajustement de l'intensité de raisonnement"
+    },
     "frequencyPenalty": {
       "desc": "Plus la valeur est élevée, plus il est probable de réduire les mots répétés",
       "title": "Pénalité de fréquence"
@@ -216,6 +219,15 @@
       "desc": "Plus la valeur est élevée, plus il est probable d'explorer de nouveaux sujets",
       "title": "Pénalité de présence"
     },
+    "reasoningEffort": {
+      "desc": "Plus la valeur est élevée, plus la capacité de raisonnement est forte, mais cela peut augmenter le temps de réponse et la consommation de jetons",
+      "options": {
+        "high": "Élevé",
+        "low": "Bas",
+        "medium": "Moyen"
+      },
+      "title": "Intensité de raisonnement"
+    },
     "temperature": {
       "desc": "Plus la valeur est élevée, plus la réponse est aléatoire",
       "title": "Aléatoire",
diff --git a/locales/it-IT/discover.json b/locales/it-IT/discover.json
index ac7805a8e64f8..e59f6cd2ea536 100644
--- a/locales/it-IT/discover.json
+++ b/locales/it-IT/discover.json
@@ -126,6 +126,10 @@
         "title": "Freschezza del tema"
       },
       "range": "Intervallo",
+      "reasoning_effort": {
+        "desc": "Questa impostazione controlla l'intensità del ragionamento del modello prima di generare una risposta. Un'intensità bassa privilegia la velocità di risposta e risparmia Token, mentre un'intensità alta fornisce un ragionamento più completo, ma consuma più Token e riduce la velocità di risposta. Il valore predefinito è medio, bilanciando l'accuratezza del ragionamento e la velocità di risposta.",
+        "title": "Intensità del ragionamento"
+      },
       "temperature": {
         "desc": "Questa impostazione influisce sulla diversità delle risposte del modello. Valori più bassi portano a risposte più prevedibili e tipiche, mentre valori più alti incoraggiano risposte più varie e insolite. Quando il valore è impostato a 0, il modello fornisce sempre la stessa risposta per un dato input.",
         "title": "Casualità"
diff --git a/locales/it-IT/models.json b/locales/it-IT/models.json
index 7f944f6489d21..19c8f781dd068 100644
--- a/locales/it-IT/models.json
+++ b/locales/it-IT/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 è il nuovo modello di inferenza di OpenAI, adatto a compiti complessi che richiedono una vasta conoscenza generale. Questo modello ha un contesto di 128K e una data di cutoff della conoscenza di ottobre 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini è il nostro ultimo modello di inferenza compatto, che offre un'intelligenza elevata con gli stessi obiettivi di costo e latenza di o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba è un modello linguistico Mamba 2 focalizzato sulla generazione di codice, offre un forte supporto per compiti avanzati di codifica e ragionamento."
   },
diff --git a/locales/it-IT/setting.json b/locales/it-IT/setting.json
index 0ab578883a881..21f4ac18acb36 100644
--- a/locales/it-IT/setting.json
+++ b/locales/it-IT/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Abilita limite di risposta singola"
     },
+    "enableReasoningEffort": {
+      "title": "Attiva la regolazione dell'intensità del ragionamento"
+    },
     "frequencyPenalty": {
       "desc": "Più alto è il valore, più probabile è la riduzione delle parole ripetute",
       "title": "Penalità di frequenza"
@@ -216,6 +219,15 @@
       "desc": "Più alto è il valore, più probabile è l'estensione a nuovi argomenti",
       "title": "Freschezza dell'argomento"
     },
+    "reasoningEffort": {
+      "desc": "Maggiore è il valore, più forte è la capacità di ragionamento, ma potrebbe aumentare il tempo di risposta e il consumo di Token",
+      "options": {
+        "high": "Alto",
+        "low": "Basso",
+        "medium": "Medio"
+      },
+      "title": "Intensità del ragionamento"
+    },
     "temperature": {
       "desc": "Più alto è il valore, più casuale è la risposta",
       "title": "Casualità",
diff --git a/locales/ja-JP/discover.json b/locales/ja-JP/discover.json
index c7bb66f3a6c84..291122a9bb483 100644
--- a/locales/ja-JP/discover.json
+++ b/locales/ja-JP/discover.json
@@ -126,6 +126,10 @@
         "title": "トピックの新鮮さ"
       },
       "range": "範囲",
+      "reasoning_effort": {
+        "desc": "この設定は、モデルが回答を生成する前の推論の強度を制御するために使用されます。低強度は応答速度を優先し、トークンを節約しますが、高強度はより完全な推論を提供しますが、より多くのトークンを消費し、応答速度が低下します。デフォルト値は中で、推論の正確性と応答速度のバランスを取ります。",
+        "title": "推論強度"
+      },
       "temperature": {
         "desc": "この設定は、モデルの応答の多様性に影響を与えます。低い値はより予測可能で典型的な応答をもたらし、高い値はより多様で珍しい応答を奨励します。値が0に設定されると、モデルは与えられた入力に対して常に同じ応答を返します。",
         "title": "ランダム性"
diff --git a/locales/ja-JP/models.json b/locales/ja-JP/models.json
index a6c3968b4dfc2..2726b220a9434 100644
--- a/locales/ja-JP/models.json
+++ b/locales/ja-JP/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1はOpenAIの新しい推論モデルで、広範な一般知識を必要とする複雑なタスクに適しています。このモデルは128Kのコンテキストを持ち、2023年10月の知識のカットオフがあります。"
   },
+  "o3-mini": {
+    "description": "o3-miniは、o1-miniと同じコストと遅延目標で高い知能を提供する最新の小型推論モデルです。"
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mambaは、コード生成に特化したMamba 2言語モデルであり、高度なコードおよび推論タスクを強力にサポートします。"
   },
diff --git a/locales/ja-JP/setting.json b/locales/ja-JP/setting.json
index b09714913d6b7..5b948642a254f 100644
--- a/locales/ja-JP/setting.json
+++ b/locales/ja-JP/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "単一応答制限を有効にする"
     },
+    "enableReasoningEffort": {
+      "title": "推論強度調整を有効にする"
+    },
     "frequencyPenalty": {
       "desc": "値が大きいほど、単語の繰り返しを減らす可能性が高くなります",
       "title": "頻度ペナルティ"
@@ -216,6 +219,15 @@
       "desc": "値が大きいほど、新しいトピックに拡張する可能性が高くなります",
       "title": "トピックの新鮮度"
     },
+    "reasoningEffort": {
+      "desc": "値が大きいほど推論能力が高まりますが、応答時間とトークン消費が増加する可能性があります",
+      "options": {
+        "high": "高",
+        "low": "低",
+        "medium": "中"
+      },
+      "title": "推論強度"
+    },
     "temperature": {
       "desc": "値が大きいほど、応答がよりランダムになります",
       "title": "ランダム性",
diff --git a/locales/ko-KR/discover.json b/locales/ko-KR/discover.json
index ba698e083b070..91c64a86bc513 100644
--- a/locales/ko-KR/discover.json
+++ b/locales/ko-KR/discover.json
@@ -126,6 +126,10 @@
         "title": "주제 신선도"
       },
       "range": "범위",
+      "reasoning_effort": {
+        "desc": "이 설정은 모델이 응답을 생성하기 전에 추론 강도를 제어하는 데 사용됩니다. 낮은 강도는 응답 속도를 우선시하고 토큰을 절약하며, 높은 강도는 더 완전한 추론을 제공하지만 더 많은 토큰을 소모하고 응답 속도를 저하시킵니다. 기본값은 중간으로, 추론 정확성과 응답 속도의 균형을 맞춥니다.",
+        "title": "추론 강도"
+      },
       "temperature": {
         "desc": "이 설정은 모델 응답의 다양성에 영향을 미칩니다. 낮은 값은 더 예측 가능하고 전형적인 응답을 유도하며, 높은 값은 더 다양하고 드문 응답을 장려합니다. 값이 0으로 설정되면 모델은 주어진 입력에 대해 항상 동일한 응답을 제공합니다.",
         "title": "무작위성"
diff --git a/locales/ko-KR/models.json b/locales/ko-KR/models.json
index 8aa69d62516bc..d6374e4db8401 100644
--- a/locales/ko-KR/models.json
+++ b/locales/ko-KR/models.json
@@ -180,6 +180,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1은 Meta가 개발한 다국어 대규모 언어 모델 가족으로, 8B, 70B 및 405B의 세 가지 파라미터 규모의 사전 훈련 및 지침 미세 조정 변형을 포함합니다. 이 8B 지침 미세 조정 모델은 다국어 대화 시나리오에 최적화되어 있으며, 여러 산업 벤치마크 테스트에서 우수한 성능을 보입니다. 모델 훈련에는 15조 개 이상의 공개 데이터 토큰이 사용되었으며, 감독 미세 조정 및 인간 피드백 강화 학습과 같은 기술을 통해 모델의 유용성과 안전성을 향상시켰습니다. Llama 3.1은 텍스트 생성 및 코드 생성을 지원하며, 지식 마감일은 2023년 12월입니다."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview는 Qwen 팀이 개발한 시각적 추론 능력에 중점을 둔 연구 모델로, 복잡한 장면 이해 및 시각 관련 수학 문제 해결에서 독특한 장점을 가지고 있습니다."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview는 Qwen의 최신 실험적 연구 모델로, AI 추론 능력을 향상시키는 데 중점을 두고 있습니다. 언어 혼합, 재귀 추론 등 복잡한 메커니즘을 탐구하며, 주요 장점으로는 강력한 추론 분석 능력, 수학 및 프로그래밍 능력이 포함됩니다. 동시에 언어 전환 문제, 추론 루프, 안전성 고려 및 기타 능력 차이와 같은 문제도 존재합니다."
   },
@@ -534,12 +537,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct는 높은 신뢰성을 가진 지시 처리 능력을 제공하며, 다양한 산업 응용을 지원합니다."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1은 강화 학습(RL) 기반의 추론 모델로, 모델 내의 반복성과 가독성 문제를 해결합니다. RL 이전에 DeepSeek-R1은 콜드 스타트 데이터를 도입하여 추론 성능을 더욱 최적화했습니다. 수학, 코드 및 추론 작업에서 OpenAI-o1과 유사한 성능을 보이며, 정교하게 설계된 훈련 방법을 통해 전체적인 효과를 향상시켰습니다."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5는 이전 버전의 우수한 기능을 집약하여 일반 및 인코딩 능력을 강화했습니다."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3는 6710억 개의 매개변수를 가진 혼합 전문가(MoE) 언어 모델로, 다중 헤드 잠재 주의(MLA) 및 DeepSeekMoE 아키텍처를 채택하여 보조 손실 없는 부하 균형 전략을 결합하여 추론 및 훈련 효율성을 최적화합니다. 14.8조 개의 고품질 토큰에서 사전 훈련을 수행하고 감독 미세 조정 및 강화 학습을 통해 DeepSeek-V3는 성능 면에서 다른 오픈 소스 모델을 초월하며, 선도적인 폐쇄형 모델에 근접합니다."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B는 고복잡성 대화를 위해 훈련된 고급 모델입니다."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2는 DeepSeekMoE-27B를 기반으로 개발된 혼합 전문가(MoE) 비주얼 언어 모델로, 희소 활성화 MoE 아키텍처를 사용하여 4.5B 매개변수만 활성화된 상태에서 뛰어난 성능을 발휘합니다. 이 모델은 비주얼 질문 응답, 광학 문자 인식, 문서/표/차트 이해 및 비주얼 위치 지정 등 여러 작업에서 우수한 성과를 보입니다."
+  },
   "deepseek-chat": {
     "description": "일반 및 코드 능력을 융합한 새로운 오픈 소스 모델로, 기존 Chat 모델의 일반 대화 능력과 Coder 모델의 강력한 코드 처리 능력을 유지하면서 인간의 선호에 더 잘 맞춰졌습니다. 또한, DeepSeek-V2.5는 작문 작업, 지시 따르기 등 여러 측면에서 큰 향상을 이루었습니다."
   },
@@ -1173,6 +1185,9 @@
   "o1-preview": {
     "description": "o1은 OpenAI의 새로운 추론 모델로, 광범위한 일반 지식이 필요한 복잡한 작업에 적합합니다. 이 모델은 128K의 컨텍스트와 2023년 10월의 지식 기준일을 가지고 있습니다."
   },
+  "o3-mini": {
+    "description": "o3-mini는 최신 소형 추론 모델로, o1-mini와 동일한 비용과 지연 목표에서 높은 지능을 제공합니다."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba는 코드 생성을 전문으로 하는 Mamba 2 언어 모델로, 고급 코드 및 추론 작업에 강력한 지원을 제공합니다."
   },
diff --git a/locales/ko-KR/setting.json b/locales/ko-KR/setting.json
index 19dbb33bd78dd..72525688ba9c3 100644
--- a/locales/ko-KR/setting.json
+++ b/locales/ko-KR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "단일 응답 제한 활성화"
     },
+    "enableReasoningEffort": {
+      "title": "추론 강도 조정 활성화"
+    },
     "frequencyPenalty": {
       "desc": "값이 클수록 반복 단어가 줄어듭니다",
       "title": "빈도 패널티"
@@ -216,6 +219,15 @@
       "desc": "값이 클수록 새로운 주제로 확장될 가능성이 높아집니다",
       "title": "주제 신선도"
     },
+    "reasoningEffort": {
+      "desc": "값이 클수록 추론 능력이 강해지지만, 응답 시간과 토큰 소모가 증가할 수 있습니다.",
+      "options": {
+        "high": "높음",
+        "low": "낮음",
+        "medium": "중간"
+      },
+      "title": "추론 강도"
+    },
     "temperature": {
       "desc": "값이 클수록 응답이 더 무작위해집니다",
       "title": "랜덤성",
diff --git a/locales/nl-NL/discover.json b/locales/nl-NL/discover.json
index 5ff3bd314579b..c7f34b75ee749 100644
--- a/locales/nl-NL/discover.json
+++ b/locales/nl-NL/discover.json
@@ -126,6 +126,10 @@
         "title": "Onderwerp versheid"
       },
       "range": "Bereik",
+      "reasoning_effort": {
+        "desc": "Deze instelling wordt gebruikt om de redeneerkracht van het model te regelen voordat het een antwoord genereert. Lage kracht geeft prioriteit aan de responssnelheid en bespaart tokens, terwijl hoge kracht een completere redenering biedt, maar meer tokens verbruikt en de responssnelheid verlaagt. De standaardwaarde is gemiddeld, wat een balans biedt tussen redeneringsnauwkeurigheid en responssnelheid.",
+        "title": "Redeneerkracht"
+      },
       "temperature": {
         "desc": "Deze instelling beïnvloedt de diversiteit van de reacties van het model. Lagere waarden leiden tot meer voorspelbare en typische reacties, terwijl hogere waarden meer diverse en ongebruikelijke reacties aanmoedigen. Wanneer de waarde op 0 is ingesteld, geeft het model altijd dezelfde reactie op een gegeven invoer.",
         "title": "Willekeurigheid"
diff --git a/locales/nl-NL/models.json b/locales/nl-NL/models.json
index fd776706613b3..20d3656999d49 100644
--- a/locales/nl-NL/models.json
+++ b/locales/nl-NL/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 is het nieuwe redeneermodel van OpenAI, geschikt voor complexe taken die uitgebreide algemene kennis vereisen. Dit model heeft een context van 128K en een kennisafkapdatum van oktober 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini is ons nieuwste kleine inferentiemodel dat hoge intelligentie biedt met dezelfde kosten- en vertragingdoelen als o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba is een Mamba 2-taalmodel dat zich richt op codegeneratie en krachtige ondersteuning biedt voor geavanceerde code- en inferentietaken."
   },
diff --git a/locales/nl-NL/setting.json b/locales/nl-NL/setting.json
index 97e23bdf116ee..65829d32d6fd5 100644
--- a/locales/nl-NL/setting.json
+++ b/locales/nl-NL/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Limiet voor enkele reacties inschakelen"
     },
+    "enableReasoningEffort": {
+      "title": "Inschakelen van redeneringsinspanningsaanpassing"
+    },
     "frequencyPenalty": {
       "desc": "Hoe hoger de waarde, hoe waarschijnlijker het is dat herhaalde woorden worden verminderd",
       "title": "Frequentieboete"
@@ -216,6 +219,15 @@
       "desc": "Hoe hoger de waarde, hoe waarschijnlijker het is dat het gesprek naar nieuwe onderwerpen wordt uitgebreid",
       "title": "Onderwerpnieuwheid"
     },
+    "reasoningEffort": {
+      "desc": "Hoe hoger de waarde, hoe sterker de redeneringscapaciteit, maar dit kan de responstijd en het tokenverbruik verhogen",
+      "options": {
+        "high": "Hoog",
+        "low": "Laag",
+        "medium": "Gemiddeld"
+      },
+      "title": "Redeneringsinspanningsniveau"
+    },
     "temperature": {
       "desc": "Hoe hoger de waarde, hoe willekeuriger de reactie",
       "title": "Willekeurigheid",
diff --git a/locales/pl-PL/discover.json b/locales/pl-PL/discover.json
index fb78a54a82220..a9071ae39d6c9 100644
--- a/locales/pl-PL/discover.json
+++ b/locales/pl-PL/discover.json
@@ -126,6 +126,10 @@
         "title": "Świeżość tematu"
       },
       "range": "Zakres",
+      "reasoning_effort": {
+        "desc": "To ustawienie kontroluje intensywność rozumowania modelu przed wygenerowaniem odpowiedzi. Niska intensywność priorytetowo traktuje szybkość odpowiedzi i oszczędza tokeny, podczas gdy wysoka intensywność zapewnia pełniejsze rozumowanie, ale zużywa więcej tokenów i obniża szybkość odpowiedzi. Wartość domyślna to średnia, co równoważy dokładność rozumowania z szybkością odpowiedzi.",
+        "title": "Intensywność rozumowania"
+      },
       "temperature": {
         "desc": "To ustawienie wpływa na różnorodność odpowiedzi modelu. Niższe wartości prowadzą do bardziej przewidywalnych i typowych odpowiedzi, podczas gdy wyższe wartości zachęcają do bardziej zróżnicowanych i rzadziej spotykanych odpowiedzi. Gdy wartość wynosi 0, model zawsze daje tę samą odpowiedź na dane wejście.",
         "title": "Losowość"
diff --git a/locales/pl-PL/models.json b/locales/pl-PL/models.json
index 3c6a206312e89..fe6ec62ddf329 100644
--- a/locales/pl-PL/models.json
+++ b/locales/pl-PL/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 to nowy model wnioskowania OpenAI, odpowiedni do złożonych zadań wymagających szerokiej wiedzy ogólnej. Model ten ma kontekst 128K i datę graniczną wiedzy z października 2023 roku."
   },
+  "o3-mini": {
+    "description": "o3-mini to nasz najnowszy mały model wnioskowania, który oferuje wysoką inteligencję przy tych samych kosztach i celach opóźnienia co o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba to model językowy Mamba 2 skoncentrowany na generowaniu kodu, oferujący silne wsparcie dla zaawansowanych zadań kodowania i wnioskowania."
   },
diff --git a/locales/pl-PL/setting.json b/locales/pl-PL/setting.json
index fa66a39b0674a..372f2f4a5779c 100644
--- a/locales/pl-PL/setting.json
+++ b/locales/pl-PL/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Włącz limit jednorazowej odpowiedzi"
     },
+    "enableReasoningEffort": {
+      "title": "Włącz dostosowanie intensywności rozumowania"
+    },
     "frequencyPenalty": {
       "desc": "Im większa wartość, tym większe prawdopodobieństwo zmniejszenia powtarzających się słów",
       "title": "Kara za częstość"
@@ -216,6 +219,15 @@
       "desc": "Im większa wartość, tym większe prawdopodobieństwo rozszerzenia się na nowe tematy",
       "title": "Świeżość tematu"
     },
+    "reasoningEffort": {
+      "desc": "Im wyższa wartość, tym silniejsza zdolność rozumowania, ale może to zwiększyć czas odpowiedzi i zużycie tokenów",
+      "options": {
+        "high": "Wysoki",
+        "low": "Niski",
+        "medium": "Średni"
+      },
+      "title": "Intensywność rozumowania"
+    },
     "temperature": {
       "desc": "Im większa wartość, tym odpowiedzi są bardziej losowe",
       "title": "Losowość",
diff --git a/locales/pt-BR/discover.json b/locales/pt-BR/discover.json
index 4b88bc81c6b29..6f2c6de8923fc 100644
--- a/locales/pt-BR/discover.json
+++ b/locales/pt-BR/discover.json
@@ -126,6 +126,10 @@
         "title": "Novidade do Tópico"
       },
       "range": "Faixa",
+      "reasoning_effort": {
+        "desc": "Esta configuração é usada para controlar a intensidade de raciocínio do modelo antes de gerar uma resposta. Intensidade baixa prioriza a velocidade de resposta e economiza Tokens, enquanto intensidade alta oferece um raciocínio mais completo, mas consome mais Tokens e reduz a velocidade de resposta. O valor padrão é médio, equilibrando a precisão do raciocínio com a velocidade de resposta.",
+        "title": "Intensidade de Raciocínio"
+      },
       "temperature": {
         "desc": "Esta configuração afeta a diversidade das respostas do modelo. Valores mais baixos resultam em respostas mais previsíveis e típicas, enquanto valores mais altos incentivam respostas mais variadas e incomuns. Quando o valor é 0, o modelo sempre dá a mesma resposta para uma entrada dada.",
         "title": "Aleatoriedade"
diff --git a/locales/pt-BR/models.json b/locales/pt-BR/models.json
index 68f203e52d025..af81d042f4640 100644
--- a/locales/pt-BR/models.json
+++ b/locales/pt-BR/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 é o novo modelo de raciocínio da OpenAI, adequado para tarefas complexas que exigem amplo conhecimento geral. Este modelo possui um contexto de 128K e uma data limite de conhecimento em outubro de 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini é nosso mais recente modelo de inferência em miniatura, oferecendo alta inteligência com os mesmos custos e metas de latência que o o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba é um modelo de linguagem Mamba 2 focado em geração de código, oferecendo forte suporte para tarefas avançadas de codificação e raciocínio."
   },
diff --git a/locales/pt-BR/setting.json b/locales/pt-BR/setting.json
index 610fa3018807a..e3383a27eb4f3 100644
--- a/locales/pt-BR/setting.json
+++ b/locales/pt-BR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Ativar limite de resposta única"
     },
+    "enableReasoningEffort": {
+      "title": "Ativar ajuste de intensidade de raciocínio"
+    },
     "frequencyPenalty": {
       "desc": "Quanto maior o valor, maior a probabilidade de reduzir palavras repetidas",
       "title": "Penalidade de frequência"
@@ -216,6 +219,15 @@
       "desc": "Quanto maior o valor, maior a probabilidade de expandir para novos tópicos",
       "title": "Penalidade de novidade do tópico"
     },
+    "reasoningEffort": {
+      "desc": "Quanto maior o valor, mais forte será a capacidade de raciocínio, mas isso pode aumentar o tempo de resposta e o consumo de tokens",
+      "options": {
+        "high": "Alto",
+        "low": "Baixo",
+        "medium": "Médio"
+      },
+      "title": "Intensidade de raciocínio"
+    },
     "temperature": {
       "desc": "Quanto maior o valor, mais aleatória será a resposta",
       "title": "Aleatoriedade",
diff --git a/locales/ru-RU/discover.json b/locales/ru-RU/discover.json
index 1259a910fb9aa..9553af416a675 100644
--- a/locales/ru-RU/discover.json
+++ b/locales/ru-RU/discover.json
@@ -126,6 +126,10 @@
         "title": "Свежесть темы"
       },
       "range": "Диапазон",
+      "reasoning_effort": {
+        "desc": "Эта настройка используется для управления интенсивностью размышлений модели перед генерацией ответа. Низкая интенсивность приоритизирует скорость ответа и экономит токены, высокая интенсивность обеспечивает более полное размышление, но потребляет больше токенов и снижает скорость ответа. Значение по умолчанию - среднее, что обеспечивает баланс между точностью размышлений и скоростью ответа.",
+        "title": "Интенсивность размышлений"
+      },
       "temperature": {
         "desc": "Эта настройка влияет на разнообразие ответов модели. Более низкие значения приводят к более предсказуемым и типичным ответам, в то время как более высокие значения поощряют более разнообразные и необычные ответы. Когда значение установлено на 0, модель всегда дает один и тот же ответ на данный ввод.",
         "title": "Случайность"
diff --git a/locales/ru-RU/models.json b/locales/ru-RU/models.json
index 7b3ddd69c4d12..bf8547cf2c445 100644
--- a/locales/ru-RU/models.json
+++ b/locales/ru-RU/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 — это новая модель вывода от OpenAI, подходящая для сложных задач, требующих обширных общих знаний. Модель имеет контекст 128K и срок знания до октября 2023 года."
   },
+  "o3-mini": {
+    "description": "o3-mini — это наша последняя компактная модель вывода, обеспечивающая высокий уровень интеллекта при тех же затратах и задержках, что и o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba — это языковая модель Mamba 2, сосредоточенная на генерации кода, обеспечивающая мощную поддержку для сложных задач по коду и выводу."
   },
diff --git a/locales/ru-RU/setting.json b/locales/ru-RU/setting.json
index 4c1760d114ea1..4abe96acca0a8 100644
--- a/locales/ru-RU/setting.json
+++ b/locales/ru-RU/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Включить ограничение максимального количества токенов"
     },
+    "enableReasoningEffort": {
+      "title": "Включить настройку интенсивности вывода"
+    },
     "frequencyPenalty": {
       "desc": "Чем выше значение, тем меньше вероятность повторения слов",
       "title": "Штраф за повторение"
@@ -216,6 +219,15 @@
       "desc": "Чем выше значение, тем больше вероятность перехода на новые темы",
       "title": "Штраф за однообразие"
     },
+    "reasoningEffort": {
+      "desc": "Чем больше значение, тем сильнее способность вывода, но это может увеличить время отклика и потребление токенов",
+      "options": {
+        "high": "Высокий",
+        "low": "Низкий",
+        "medium": "Средний"
+      },
+      "title": "Интенсивность вывода"
+    },
     "temperature": {
       "desc": "Чем выше значение, тем более непредсказуемым будет ответ",
       "title": "Непредсказуемость",
diff --git a/locales/tr-TR/discover.json b/locales/tr-TR/discover.json
index 0896540a6138b..d75758a6c4e27 100644
--- a/locales/tr-TR/discover.json
+++ b/locales/tr-TR/discover.json
@@ -126,6 +126,10 @@
         "title": "Konu Tazeliği"
       },
       "range": "Aralık",
+      "reasoning_effort": {
+        "desc": "Bu ayar, modelin yanıt üretmeden önceki akıl yürütme gücünü kontrol etmek için kullanılır. Düşük güç, yanıt hızını önceliklendirir ve Token tasarrufu sağlar; yüksek güç ise daha kapsamlı bir akıl yürütme sunar, ancak daha fazla Token tüketir ve yanıt hızını düşürür. Varsayılan değer orta seviyedir, akıl yürütme doğruluğu ile yanıt hızı arasında bir denge sağlar.",
+        "title": "Akıl Yürütme Gücü"
+      },
       "temperature": {
         "desc": "Bu ayar, modelin yanıtlarının çeşitliliğini etkiler. Daha düşük değerler daha öngörülebilir ve tipik yanıtlar verirken, daha yüksek değerler daha çeşitli ve nadir yanıtları teşvik eder. Değer 0 olarak ayarlandığında, model belirli bir girdi için her zaman aynı yanıtı verir.",
         "title": "Rastgelelik"
diff --git a/locales/tr-TR/models.json b/locales/tr-TR/models.json
index 3f740dd741e6a..c00163f7b6be4 100644
--- a/locales/tr-TR/models.json
+++ b/locales/tr-TR/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1, OpenAI'nin geniş genel bilgiye ihtiyaç duyan karmaşık görevler için uygun yeni bir akıl yürütme modelidir. Bu model, 128K bağlam ve Ekim 2023 bilgi kesim tarihi ile donatılmıştır."
   },
+  "o3-mini": {
+    "description": "o3-mini, aynı maliyet ve gecikme hedefleriyle yüksek zeka sunan en yeni küçük ölçekli çıkarım modelimizdir."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba, kod üretimine odaklanan Mamba 2 dil modelidir ve ileri düzey kod ve akıl yürütme görevlerine güçlü destek sunar."
   },
diff --git a/locales/tr-TR/setting.json b/locales/tr-TR/setting.json
index 30d8758284f1c..4e8095b43673e 100644
--- a/locales/tr-TR/setting.json
+++ b/locales/tr-TR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Max Token Sınırlamasını Etkinleştir"
     },
+    "enableReasoningEffort": {
+      "title": "Akıl yürütme yoğunluğunu ayarla"
+    },
     "frequencyPenalty": {
       "desc": "Değer ne kadar yüksekse, tekrarlayan kelimeleri azaltma olasılığı o kadar yüksektir",
       "title": "Frequency Penalty"
@@ -216,6 +219,15 @@
       "desc": "Değer ne kadar yüksekse, yeni konulara genişleme olasılığı o kadar yüksektir",
       "title": "Presence Penalty"
     },
+    "reasoningEffort": {
+      "desc": "Değer ne kadar yüksekse, akıl yürütme yeteneği o kadar güçlüdür, ancak yanıt süresi ve Token tüketimini artırabilir",
+      "options": {
+        "high": "Yüksek",
+        "low": "Düşük",
+        "medium": "Orta"
+      },
+      "title": "Akıl yürütme yoğunluğu"
+    },
     "temperature": {
       "desc": "Değer ne kadar yüksekse, yanıt o kadar rastgele olur",
       "title": "Randomness",
diff --git a/locales/vi-VN/discover.json b/locales/vi-VN/discover.json
index 804488c247e77..726f4a2bfa475 100644
--- a/locales/vi-VN/discover.json
+++ b/locales/vi-VN/discover.json
@@ -126,6 +126,10 @@
         "title": "Độ mới của chủ đề"
       },
       "range": "Phạm vi",
+      "reasoning_effort": {
+        "desc": "Cài đặt này được sử dụng để kiểm soát mức độ suy luận của mô hình trước khi tạo câu trả lời. Mức độ thấp ưu tiên tốc độ phản hồi và tiết kiệm Token, trong khi mức độ cao cung cấp suy luận đầy đủ hơn nhưng tiêu tốn nhiều Token hơn và làm giảm tốc độ phản hồi. Giá trị mặc định là trung bình, cân bằng giữa độ chính xác của suy luận và tốc độ phản hồi.",
+        "title": "Mức độ suy luận"
+      },
       "temperature": {
         "desc": "Cài đặt này ảnh hưởng đến sự đa dạng trong phản hồi của mô hình. Giá trị thấp hơn dẫn đến phản hồi dễ đoán và điển hình hơn, trong khi giá trị cao hơn khuyến khích phản hồi đa dạng và không thường gặp. Khi giá trị được đặt là 0, mô hình sẽ luôn đưa ra cùng một phản hồi cho đầu vào nhất định.",
         "title": "Ngẫu nhiên"
diff --git a/locales/vi-VN/models.json b/locales/vi-VN/models.json
index 16f22dda909df..f0a0ababab96f 100644
--- a/locales/vi-VN/models.json
+++ b/locales/vi-VN/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1 là mô hình suy diễn mới của OpenAI, phù hợp cho các nhiệm vụ phức tạp cần kiến thức tổng quát rộng rãi. Mô hình này có ngữ cảnh 128K và thời điểm cắt kiến thức vào tháng 10 năm 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini là mô hình suy diễn nhỏ gọn mới nhất của chúng tôi, cung cấp trí thông minh cao với chi phí và độ trễ tương tự như o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba là mô hình ngôn ngữ Mamba 2 tập trung vào sinh mã, cung cấp hỗ trợ mạnh mẽ cho các nhiệm vụ mã và suy luận tiên tiến."
   },
diff --git a/locales/vi-VN/setting.json b/locales/vi-VN/setting.json
index ad1b0062e1142..797fd0206aa81 100644
--- a/locales/vi-VN/setting.json
+++ b/locales/vi-VN/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Bật giới hạn phản hồi một lần"
     },
+    "enableReasoningEffort": {
+      "title": "Bật điều chỉnh cường độ suy luận"
+    },
     "frequencyPenalty": {
       "desc": "Giá trị càng cao, càng có khả năng giảm sự lặp lại của từ/cụm từ",
       "title": "Hình phạt tần suất"
@@ -216,6 +219,15 @@
       "desc": "Giá trị càng cao, càng có khả năng mở rộng đến chủ đề mới",
       "title": "Độ mới của chủ đề"
     },
+    "reasoningEffort": {
+      "desc": "Giá trị càng lớn, khả năng suy luận càng mạnh, nhưng có thể làm tăng thời gian phản hồi và tiêu tốn Token",
+      "options": {
+        "high": "Cao",
+        "low": "Thấp",
+        "medium": "Trung bình"
+      },
+      "title": "Cường độ suy luận"
+    },
     "temperature": {
       "desc": "Giá trị càng cao, phản hồi càng ngẫu nhiên",
       "title": "Độ ngẫu nhiên",
diff --git a/locales/zh-CN/discover.json b/locales/zh-CN/discover.json
index 10fd75bdc74c3..4221531bf55c5 100644
--- a/locales/zh-CN/discover.json
+++ b/locales/zh-CN/discover.json
@@ -126,6 +126,10 @@
         "title": "话题新鲜度"
       },
       "range": "范围",
+      "reasoning_effort": {
+        "desc": "此设置用于控制模型在生成回答前的推理强度。低强度优先响应速度并节省 Token，高强度提供更完整的推理，但会消耗更多 Token 并降低响应速度。默认值为中，平衡推理准确性与响应速度。",
+        "title": "推理强度"
+      },
       "temperature": {
         "desc": "此设置影响模型回应的多样性。较低的值会导致更可预测和典型的回应，而较高的值则鼓励更多样化和不常见的回应。当值设为0时，模型对于给定的输入总是给出相同的回应。",
         "title": "随机性"
diff --git a/locales/zh-CN/models.json b/locales/zh-CN/models.json
index 746da990f7ba7..6ac77b2215676 100644
--- a/locales/zh-CN/models.json
+++ b/locales/zh-CN/models.json
@@ -1176,7 +1176,7 @@
     "description": "Llama 3.1 Nemotron 70B 是由 NVIDIA 定制的大型语言模型，旨在提高 LLM 生成的响应对用户查询的帮助程度。该模型在 Arena Hard、AlpacaEval 2 LC 和 GPT-4-Turbo MT-Bench 等基准测试中表现出色，截至 2024 年 10 月 1 日，在所有三个自动对齐基准测试中排名第一。该模型使用 RLHF（特别是 REINFORCE）、Llama-3.1-Nemotron-70B-Reward 和 HelpSteer2-Preference 提示在 Llama-3.1-70B-Instruct 模型基础上进行训练"
   },
   "o1": {
-    "description": "专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深入上下文理解和代理工作流程的应用程序。"
+    "description": "o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。"
   },
   "o1-mini": {
     "description": "o1-mini是一款针对编程、数学和科学应用场景而设计的快速、经济高效的推理模型。该模型具有128K上下文和2023年10月的知识截止日期。"
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1是OpenAI新的推理模型，适用于需要广泛通用知识的复杂任务。该模型具有128K上下文和2023年10月的知识截止日期。"
   },
+  "o3-mini": {
+    "description": "o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。"
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba是专注于代码生成的Mamba 2语言模型，为先进的代码和推理任务提供强力支持。"
   },
diff --git a/locales/zh-CN/setting.json b/locales/zh-CN/setting.json
index d6406f8c1aa1c..af8aa1e0375c1 100644
--- a/locales/zh-CN/setting.json
+++ b/locales/zh-CN/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "开启单次回复限制"
     },
+    "enableReasoningEffort": {
+      "title": "开启推理强度调整"
+    },
     "frequencyPenalty": {
       "desc": "值越大，越有可能降低重复字词",
       "title": "频率惩罚度"
@@ -216,6 +219,15 @@
       "desc": "值越大，越有可能扩展到新话题",
       "title": "话题新鲜度"
     },
+    "reasoningEffort": {
+      "desc": "值越大，推理能力越强，但可能会增加响应时间和 Token 消耗",
+      "options": {
+        "high": "高",
+        "low": "低",
+        "medium": "中"
+      },
+      "title": "推理强度"
+    },
     "temperature": {
       "desc": "值越大，回复越随机",
       "title": "随机性",
diff --git a/locales/zh-TW/discover.json b/locales/zh-TW/discover.json
index 130f8b9c7a22c..cdb81fd51d114 100644
--- a/locales/zh-TW/discover.json
+++ b/locales/zh-TW/discover.json
@@ -126,6 +126,10 @@
         "title": "話題新鮮度"
       },
       "range": "範圍",
+      "reasoning_effort": {
+        "desc": "此設定用於控制模型在生成回答前的推理強度。低強度優先響應速度並節省 Token，高強度提供更完整的推理，但會消耗更多 Token 並降低響應速度。預設值為中，平衡推理準確性與響應速度。",
+        "title": "推理強度"
+      },
       "temperature": {
         "desc": "此設置影響模型回應的多樣性。較低的值會導致更可預測和典型的回應，而較高的值則鼓勵更多樣化和不常見的回應。當值設為0時，模型對於給定的輸入總是給出相同的回應。",
         "title": "隨機性"
diff --git a/locales/zh-TW/models.json b/locales/zh-TW/models.json
index 7f4742b40950c..06c88565ef43e 100644
--- a/locales/zh-TW/models.json
+++ b/locales/zh-TW/models.json
@@ -1184,6 +1184,9 @@
   "o1-preview": {
     "description": "o1是OpenAI新的推理模型，適用於需要廣泛通用知識的複雜任務。該模型具有128K上下文和2023年10月的知識截止日期。"
   },
+  "o3-mini": {
+    "description": "o3-mini 是我們最新的小型推理模型，在與 o1-mini 相同的成本和延遲目標下提供高智能。"
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba 是專注於代碼生成的 Mamba 2 語言模型，為先進的代碼和推理任務提供強力支持。"
   },
diff --git a/locales/zh-TW/setting.json b/locales/zh-TW/setting.json
index f683dab3577d1..697730b4d3b0d 100644
--- a/locales/zh-TW/setting.json
+++ b/locales/zh-TW/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "啟用單次回覆限制"
     },
+    "enableReasoningEffort": {
+      "title": "開啟推理強度調整"
+    },
     "frequencyPenalty": {
       "desc": "數值越大，越有可能降低重複字詞",
       "title": "頻率懲罰度"
@@ -216,6 +219,15 @@
       "desc": "數值越大，越有可能擴展到新話題",
       "title": "話題新鮮度"
     },
+    "reasoningEffort": {
+      "desc": "值越大，推理能力越強，但可能會增加回應時間和 Token 消耗",
+      "options": {
+        "high": "高",
+        "low": "低",
+        "medium": "中"
+      },
+      "title": "推理強度"
+    },
     "temperature": {
       "desc": "數值越大，回覆越隨機",
       "title": "隨機性",
diff --git a/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx b/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx
index 4d11a87a6e7ae..14f1e6708ad07 100644
--- a/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx
+++ b/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx
@@ -5,6 +5,7 @@ import {
   ChartColumnBig,
   Delete,
   FileMinus,
+  Pickaxe,
   LucideIcon,
   MessageSquareText,
   Thermometer,
@@ -83,6 +84,15 @@ const ParameterList = memo<ParameterListProps>(({ data }) => {
       range: data?.meta?.maxOutput ? [0, formatTokenNumber(data.meta.maxOutput)] : undefined,
       type: 'int',
     },
+    {
+      defaultValue: '--',
+      desc: t('models.parameterList.reasoning_effort.desc'),
+      icon: Pickaxe,
+      key: 'reasoning_effort',
+      label: t('models.parameterList.reasoning_effort.title'),
+      range: ['low', 'high'],
+      type: 'string',
+    },
   ];
 
   return (
diff --git a/src/config/aiModels/github.ts b/src/config/aiModels/github.ts
index e68ab31631d51..2e3db2a7209c9 100644
--- a/src/config/aiModels/github.ts
+++ b/src/config/aiModels/github.ts
@@ -3,21 +3,20 @@ import { AIChatModelCard } from '@/types/aiModel';
 const githubChatModels: AIChatModelCard[] = [
   {
     abilities: {
-      functionCall: false,
-      vision: true,
+      functionCall: true,
     },
     contextWindowTokens: 200_000,
     description:
-      '专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深入上下文理解和代理工作流程的应用程序。',
-    displayName: 'OpenAI o1',
+      'o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。',
+    displayName: 'OpenAI o3-mini',
     enabled: true,
-    id: 'o1',
+    id: 'o3-mini',
     maxOutput: 100_000,
+    releasedAt: '2025-01-31',
     type: 'chat',
   },
   {
     abilities: {
-      functionCall: false,
       vision: true,
     },
     contextWindowTokens: 128_000,
@@ -30,7 +29,19 @@ const githubChatModels: AIChatModelCard[] = [
   },
   {
     abilities: {
-      functionCall: false,
+      vision: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。',
+    displayName: 'OpenAI o1',
+    enabled: true,
+    id: 'o1',
+    maxOutput: 100_000,
+    type: 'chat',
+  },
+  {
+    abilities: {
       vision: true,
     },
     contextWindowTokens: 128_000,
diff --git a/src/config/aiModels/openai.ts b/src/config/aiModels/openai.ts
index c35bfd65da812..bb8f6869c58a9 100644
--- a/src/config/aiModels/openai.ts
+++ b/src/config/aiModels/openai.ts
@@ -8,6 +8,24 @@ import {
 } from '@/types/aiModel';
 
 export const openaiChatModels: AIChatModelCard[] = [
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。',
+    displayName: 'OpenAI o3-mini',
+    enabled: true,
+    id: 'o3-mini',
+    maxOutput: 100_000,
+    pricing: {
+      input: 1.1,
+      output: 4.4,
+    },
+    releasedAt: '2025-01-31',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 128_000,
     description:
@@ -17,12 +35,27 @@ export const openaiChatModels: AIChatModelCard[] = [
     id: 'o1-mini',
     maxOutput: 65_536,
     pricing: {
-      input: 3,
-      output: 12,
+      input: 1.1,
+      output: 4.4,
     },
     releasedAt: '2024-09-12',
     type: 'chat',
   },
+  {
+    contextWindowTokens: 200_000,
+    description:
+      'o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。',
+    displayName: 'OpenAI o1',
+    enabled: true,
+    id: 'o1',
+    maxOutput: 100_000,
+    pricing: {
+      input: 15,
+      output: 60,
+    },
+    releasedAt: '2024-12-17',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 128_000,
     description:
diff --git a/src/features/AgentSetting/AgentModal/index.tsx b/src/features/AgentSetting/AgentModal/index.tsx
index f8af3d1b88d70..a5c935554f6c9 100644
--- a/src/features/AgentSetting/AgentModal/index.tsx
+++ b/src/features/AgentSetting/AgentModal/index.tsx
@@ -1,7 +1,7 @@
 'use client';
 
 import { Form, ItemGroup, SliderWithInput } from '@lobehub/ui';
-import { Switch } from 'antd';
+import { Select, Switch } from 'antd';
 import { memo } from 'react';
 import { useTranslation } from 'react-i18next';
 
@@ -17,9 +17,9 @@ const AgentModal = memo(() => {
   const { t } = useTranslation('setting');
   const [form] = Form.useForm();
 
-  const [enableMaxTokens, updateConfig] = useStore((s) => {
+  const [enableMaxTokens, enableReasoningEffort, updateConfig] = useStore((s) => {
     const config = selectors.chatConfig(s);
-    return [config.enableMaxTokens, s.setAgentConfig];
+    return [config.enableMaxTokens, config.enableReasoningEffort, s.setAgentConfig];
   });
 
   const providerName = useProviderName(useStore((s) => s.config.provider) as string);
@@ -79,6 +79,30 @@ const AgentModal = memo(() => {
         name: ['params', 'max_tokens'],
         tag: 'max_tokens',
       },
+      {
+        children: <Switch />,
+        label: t('settingModel.enableReasoningEffort.title'),
+        minWidth: undefined,
+        name: ['chatConfig', 'enableReasoningEffort'],
+        valuePropName: 'checked',
+      },
+      {
+        children: (
+          <Select
+            defaultValue='medium'
+            options={[
+              { label: t('settingModel.reasoningEffort.options.low'), value: 'low' },
+              { label: t('settingModel.reasoningEffort.options.medium'), value: 'medium' },
+              { label: t('settingModel.reasoningEffort.options.high'), value: 'high' },
+            ]}
+          />
+        ),
+        desc: t('settingModel.reasoningEffort.desc'),
+        hidden: !enableReasoningEffort,
+        label: t('settingModel.reasoningEffort.title'),
+        name: ['params', 'reasoning_effort'],
+        tag: 'reasoning_effort',
+      },
     ],
     title: t('settingModel.title'),
   };
diff --git a/src/libs/agent-runtime/github/index.ts b/src/libs/agent-runtime/github/index.ts
index 7081a73043ef5..0e55d1392c9d8 100644
--- a/src/libs/agent-runtime/github/index.ts
+++ b/src/libs/agent-runtime/github/index.ts
@@ -2,7 +2,7 @@ import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
 import type { ChatModelCard } from '@/types/llm';
 
 import { AgentRuntimeErrorType } from '../error';
-import { o1Models, pruneO1Payload } from '../openai';
+import { pruneReasoningPayload, reasoningModels } from '../openai';
 import { ModelProvider } from '../types';
 import {
   CHAT_MODELS_BLOCK_LIST,
@@ -37,8 +37,8 @@ export const LobeGithubAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (o1Models.has(model)) {
-        return { ...pruneO1Payload(payload), stream: false } as any;
+      if (reasoningModels.has(model)) {
+        return { ...pruneReasoningPayload(payload), stream: false } as any;
       }
 
       return { ...payload, stream: payload.stream ?? true };
diff --git a/src/libs/agent-runtime/openai/index.ts b/src/libs/agent-runtime/openai/index.ts
index 74b7e434976af..8bed6c0702d81 100644
--- a/src/libs/agent-runtime/openai/index.ts
+++ b/src/libs/agent-runtime/openai/index.ts
@@ -2,21 +2,23 @@ import { ChatStreamPayload, ModelProvider, OpenAIChatMessage } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 // TODO: 临时写法，后续要重构成 model card 展示配置
-export const o1Models = new Set([
+export const reasoningModels = new Set([
   'o1-preview',
   'o1-preview-2024-09-12',
   'o1-mini',
   'o1-mini-2024-09-12',
   'o1',
   'o1-2024-12-17',
+  'o3-mini',
+  'o3-mini-2025-01-31',
 ]);
 
-export const pruneO1Payload = (payload: ChatStreamPayload) => ({
+export const pruneReasoningPayload = (payload: ChatStreamPayload) => ({
   ...payload,
   frequency_penalty: 0,
   messages: payload.messages.map((message: OpenAIChatMessage) => ({
     ...message,
-    role: message.role === 'system' ? 'user' : message.role,
+    role: message.role === 'system' ? 'developer' : message.role,
   })),
   presence_penalty: 0,
   temperature: 1,
@@ -29,8 +31,8 @@ export const LobeOpenAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (o1Models.has(model)) {
-        return pruneO1Payload(payload) as any;
+      if (reasoningModels.has(model)) {
+        return pruneReasoningPayload(payload) as any;
       }
 
       return { ...payload, stream: payload.stream ?? true };
diff --git a/src/locales/default/discover.ts b/src/locales/default/discover.ts
index 82907c86c58d2..7effa14326e7f 100644
--- a/src/locales/default/discover.ts
+++ b/src/locales/default/discover.ts
@@ -127,6 +127,10 @@ export default {
         title: '话题新鲜度',
       },
       range: '范围',
+      reasoning_effort: {
+        desc: '此设置用于控制模型在生成回答前的推理强度。低强度优先响应速度并节省 Token，高强度提供更完整的推理，但会消耗更多 Token 并降低响应速度。默认值为中，平衡推理准确性与响应速度。',
+        title: '推理强度',
+      },
       temperature: {
         desc: '此设置影响模型回应的多样性。较低的值会导致更可预测和典型的回应，而较高的值则鼓励更多样化和不常见的回应。当值设为0时，模型对于给定的输入总是给出相同的回应。',
         title: '随机性',
diff --git a/src/locales/default/setting.ts b/src/locales/default/setting.ts
index d20460052773a..4bf814430fbc4 100644
--- a/src/locales/default/setting.ts
+++ b/src/locales/default/setting.ts
@@ -202,6 +202,9 @@ export default {
     enableMaxTokens: {
       title: '开启单次回复限制',
     },
+    enableReasoningEffort: {
+      title: '开启推理强度调整',
+    },
     frequencyPenalty: {
       desc: '值越大，越有可能降低重复字词',
       title: '频率惩罚度',
@@ -218,6 +221,15 @@ export default {
       desc: '值越大，越有可能扩展到新话题',
       title: '话题新鲜度',
     },
+    reasoningEffort: {
+      desc: '值越大，推理能力越强，但可能会增加响应时间和 Token 消耗',
+      options: {
+        high: '高',
+        low: '低',
+        medium: '中',
+      },
+      title: '推理强度',
+    },
     temperature: {
       desc: '值越大，回复越随机',
       title: '随机性',
diff --git a/src/store/chat/slices/aiChat/actions/generateAIChat.ts b/src/store/chat/slices/aiChat/actions/generateAIChat.ts
index 77cd04fca49a7..fff37785c323f 100644
--- a/src/store/chat/slices/aiChat/actions/generateAIChat.ts
+++ b/src/store/chat/slices/aiChat/actions/generateAIChat.ts
@@ -420,6 +420,11 @@ export const generateAIChat: StateCreator<
       ? agentConfig.params.max_tokens
       : undefined;
 
+    // 5. handle reasoning_effort
+    agentConfig.params.reasoning_effort = chatConfig.enableReasoningEffort
+      ? agentConfig.params.reasoning_effort
+      : undefined;
+
     let isFunctionCall = false;
     let msgTraceId: string | undefined;
     let output = '';
diff --git a/src/types/agent/index.ts b/src/types/agent/index.ts
index ee6453b295e8d..bcd426b4c85dc 100644
--- a/src/types/agent/index.ts
+++ b/src/types/agent/index.ts
@@ -68,6 +68,11 @@ export interface LobeAgentChatConfig {
   enableHistoryCount?: boolean;
   enableMaxTokens?: boolean;
 
+  /**
+   * 自定义推理强度
+   */
+  enableReasoningEffort?: boolean;
+
   /**
    * 历史消息条数
    */
@@ -82,6 +87,7 @@ export const AgentChatConfigSchema = z.object({
   enableCompressHistory: z.boolean().optional(),
   enableHistoryCount: z.boolean().optional(),
   enableMaxTokens: z.boolean().optional(),
+  enableReasoningEffort: z.boolean().optional(),
   historyCount: z.number().optional(),
 });
 
diff --git a/src/types/llm.ts b/src/types/llm.ts
index 7230e77a5ed98..bfde6dd57cf58 100644
--- a/src/types/llm.ts
+++ b/src/types/llm.ts
@@ -159,6 +159,11 @@ export interface LLMParams {
    * 生成文本的随机度量，用于控制文本的创造性和多样性
    * @default 1
    */
+  reasoning_effort?: string;
+  /**
+   * 控制模型推理能力
+   * @default medium
+   */
   temperature?: number;
   /**
    * 控制生成文本中最高概率的单个 token
