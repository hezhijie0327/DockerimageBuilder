diff --git a/docs/usage/agents/model.mdx b/docs/usage/agents/model.mdx
index d1742a962e38..e8056cbbae7c 100644
--- a/docs/usage/agents/model.mdx
+++ b/docs/usage/agents/model.mdx
@@ -77,3 +77,19 @@ It is a mechanism that penalizes frequently occurring new vocabulary in the text
 - `0.0` When the morning sun poured into the small diner, a tired postman appeared at the door, carrying a bag of letters in his hands. The owner warmly prepared a breakfast for him, and he started sorting the mail while enjoying his breakfast. **(The highest frequency word is "of", accounting for 8.45%)**
 - `1.0` A girl in deep sleep was woken up by a warm ray of sunshine, she saw the first ray of morning light, surrounded by birdsong and flowers, everything was full of vitality. (The highest frequency word is "of", accounting for 5.45%)
 - `2.0` Every morning, he would sit on the balcony to have breakfast. Under the soft setting sun, everything looked very peaceful. However, one day, when he was about to pick up his breakfast, an optimistic little bird flew by, bringing him a good mood for the day.  (The highest frequency word is "of", accounting for 4.94%)
+
+<br />
+
+### `reasoning_effort`
+
+The `reasoning_effort` parameter controls the strength of the reasoning process. This setting affects the depth of reasoning the model performs when generating a response. The available values are **`low`**, **`medium`**, and **`high`**, with the following meanings:
+
+- **low**: Lower reasoning effort, resulting in faster response times. Suitable for scenarios where quick responses are needed, but it may sacrifice some reasoning accuracy.
+- **medium** (default): Balances reasoning accuracy and response speed, suitable for most scenarios.
+- **high**: Higher reasoning effort, producing more detailed and complex responses, but slower response times and greater token consumption.
+
+By adjusting the `reasoning_effort` parameter, you can find an appropriate balance between response speed and reasoning depth based on your needs. For example, in conversational scenarios, if fast responses are a priority, you can choose low reasoning effort; if more complex analysis or reasoning is needed, you can opt for high reasoning effort.
+
+<Callout>
+  This parameter is only applicable to reasoning models, such as OpenAI's `o1`, `o1-mini`, `o3-mini`, etc.
+</Callout>
diff --git a/docs/usage/agents/model.zh-CN.mdx b/docs/usage/agents/model.zh-CN.mdx
index a9659fae5801..578dd4096ce5 100644
--- a/docs/usage/agents/model.zh-CN.mdx
+++ b/docs/usage/agents/model.zh-CN.mdx
@@ -72,3 +72,19 @@ Presence Penalty 参数可以看作是对生成文本中重复内容的一种惩
 - `0.0` 当清晨的阳光洒进小餐馆时，一名疲倦的邮递员出现在门口，他的手中提着一袋信件。店主热情地为他准备了一份早餐，他在享用早餐的同时开始整理邮件。**（频率最高的词是 “的”，占比 8.45%）**
 - `1.0` 一个深度睡眠的女孩被一阵温暖的阳光唤醒，她看到了早晨的第一缕阳光，周围是鸟语花香，一切都充满了生机。*（频率最高的词是 “的”，占比 5.45%）*
 - `2.0` 每天早上，他都会在阳台上坐着吃早餐。在柔和的夕阳照耀下，一切看起来都非常宁静。然而有一天，当他准备端起早餐的时候，一只乐观的小鸟飞过，给他带来了一天的好心情。 *（频率最高的词是 “的”，占比 4.94%）*
+
+<br />
+
+### `reasoning_effort`
+
+`reasoning_effort` 参数用于控制推理过程的强度。此参数的设置会影响模型在生成回答时的推理深度。可选值包括 **`low`**、**`medium`** 和 **`high`**，具体含义如下：
+
+- **low（低）**：推理强度较低，生成速度较快，适用于需要快速响应的场景，但可能牺牲一定的推理精度。
+- **medium（中，默认值）**：平衡推理精度与响应速度，适用于大多数场景。
+- **high（高）**：推理强度较高，生成更为详细和复杂的回答，但响应时间较长，且消耗更多的 Token。
+
+通过调整 `reasoning_effort` 参数，可以根据需求在生成速度与推理深度之间找到适合的平衡。例如，在对话场景中，如果更关注快速响应，可以选择低推理强度；如果需要更复杂的分析或推理，可以选择高推理强度。
+
+<Callout>
+  该参数仅适用于推理模型，如 OpenAI 的 `o1`、`o1-mini`、`o3-mini` 等。
+</Callout>
diff --git a/locales/ar/discover.json b/locales/ar/discover.json
index a3ab309e5978..c3d1dd550a02 100644
--- a/locales/ar/discover.json
+++ b/locales/ar/discover.json
@@ -126,6 +126,10 @@
         "title": "جدة الموضوع"
       },
       "range": "نطاق",
+      "reasoning_effort": {
+        "desc": "تُستخدم هذه الإعدادات للتحكم في شدة التفكير التي يقوم بها النموذج قبل توليد الإجابات. الشدة المنخفضة تعطي الأولوية لسرعة الاستجابة وتوفر الرموز، بينما الشدة العالية توفر تفكيرًا أكثر اكتمالًا ولكنها تستهلك المزيد من الرموز وتقلل من سرعة الاستجابة. القيمة الافتراضية هي متوسطة، مما يوازن بين دقة التفكير وسرعة الاستجابة.",
+        "title": "شدة التفكير"
+      },
       "temperature": {
         "desc": "تؤثر هذه الإعدادات على تنوع استجابة النموذج. القيم المنخفضة تؤدي إلى استجابات أكثر توقعًا ونمطية، بينما القيم الأعلى تشجع على استجابات أكثر تنوعًا وغير شائعة. عندما تكون القيمة 0، يعطي النموذج نفس الاستجابة دائمًا لنفس المدخل.",
         "title": "عشوائية"
diff --git a/locales/ar/models.json b/locales/ar/models.json
index b4f21b0e3264..8c38f795337a 100644
--- a/locales/ar/models.json
+++ b/locales/ar/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 هو جزء من عائلة نماذج اللغة الكبيرة متعددة اللغات التي طورتها Meta، بما في ذلك متغيرات مدربة مسبقًا ومعدلة وفقًا للتعليمات بحجم 8B و70B و405B. تم تحسين هذا النموذج 8B وفقًا لمشاهدات المحادثات متعددة اللغات، وأظهر أداءً ممتازًا في العديد من اختبارات المعايير الصناعية. تم تدريب النموذج باستخدام أكثر من 15 تريليون توكن من البيانات العامة، واستخدم تقنيات مثل التعديل الخاضع للإشراف والتعلم المعزز من ردود الفعل البشرية لتحسين فائدة النموذج وأمانه. يدعم Llama 3.1 توليد النصوص وتوليد الشيفرة، مع تاريخ معرفة حتى ديسمبر 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview هو نموذج بحثي طورته فريق Qwen يركز على قدرات الاستدلال البصري، وله مزايا فريدة في فهم المشاهد المعقدة وحل المشكلات الرياضية المتعلقة بالرؤية."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview هو أحدث نموذج بحث تجريبي من Qwen، يركز على تعزيز قدرات الاستدلال للذكاء الاصطناعي. من خلال استكشاف آليات معقدة مثل خلط اللغة والاستدلال التكراري، تشمل المزايا الرئيسية القدرة القوية على التحليل الاستدلالي، والقدرات الرياضية والبرمجية. في الوقت نفسه، هناك أيضًا مشكلات في تبديل اللغة، ودورات الاستدلال، واعتبارات الأمان، واختلافات في القدرات الأخرى."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct يوفر قدرة معالجة تعليمات موثوقة، يدعم تطبيقات متعددة الصناعات."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 هو نموذج استدلال مدفوع بالتعلم المعزز (RL)، يعالج مشكلات التكرار والقراءة في النموذج. قبل استخدام RL، قدم DeepSeek-R1 بيانات بدء باردة، مما زاد من تحسين أداء الاستدلال. إنه يتفوق في الرياضيات، البرمجة، ومهام الاستدلال مقارنة بـ OpenAI-o1، ومن خلال طرق تدريب مصممة بعناية، تم تحسين الأداء العام."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 يجمع بين الميزات الممتازة للإصدارات السابقة، ويعزز القدرات العامة والترميز."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 هو نموذج لغوي مختلط الخبراء (MoE) يحتوي على 6710 مليار معلمة، يستخدم انتباه متعدد الرؤوس (MLA) وبنية DeepSeekMoE، مع استراتيجية توازن الحمل بدون خسارة مساعدة، لتحسين كفاءة الاستدلال والتدريب. من خلال التدريب المسبق على 14.8 تريليون توكن عالي الجودة، وإجراء تعديلات إشرافية وتعلم معزز، يتفوق DeepSeek-V3 في الأداء على نماذج المصدر المفتوح الأخرى، ويقترب من النماذج المغلقة الرائدة."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B هو نموذج متقدم تم تدريبه للحوار المعقد."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 هو نموذج لغوي بصري مختلط الخبراء (MoE) تم تطويره بناءً على DeepSeekMoE-27B، يستخدم بنية MoE ذات تفعيل نادر، محققاً أداءً ممتازاً مع تفعيل 4.5 مليار معلمة فقط. يتفوق هذا النموذج في مهام متعددة مثل الإجابة على الأسئلة البصرية، التعرف الضوئي على الحروف، فهم الوثائق/الجداول/الرسوم البيانية، وتحديد المواقع البصرية."
+  },
   "deepseek-chat": {
     "description": "نموذج مفتوح المصدر الجديد الذي يجمع بين القدرات العامة وقدرات البرمجة، لا يحتفظ فقط بالقدرات الحوارية العامة لنموذج الدردشة الأصلي وقدرات معالجة الشيفرة القوية لنموذج Coder، بل يتماشى أيضًا بشكل أفضل مع تفضيلات البشر. بالإضافة إلى ذلك، حقق DeepSeek-V2.5 تحسينات كبيرة في مهام الكتابة، واتباع التعليمات، وغيرها من المجالات."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 هو نموذج استدلال جديد من OpenAI، مناسب للمهام المعقدة التي تتطلب معرفة عامة واسعة. يحتوي هذا النموذج على 128K من السياق وتاريخ انتهاء المعرفة في أكتوبر 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini هو أحدث نموذج استدلال صغير لدينا، يقدم ذكاءً عالياً تحت نفس تكاليف التأخير والأداء مثل o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba هو نموذج لغة Mamba 2 يركز على توليد الشيفرة، ويوفر دعمًا قويًا لمهام الشيفرة المتقدمة والاستدلال."
   },
diff --git a/locales/ar/setting.json b/locales/ar/setting.json
index e01d443ccd2d..bd62b9c7f465 100644
--- a/locales/ar/setting.json
+++ b/locales/ar/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "تمكين الحد الأقصى للردود"
     },
+    "enableReasoningEffort": {
+      "title": "تفعيل ضبط قوة الاستدلال"
+    },
     "frequencyPenalty": {
       "desc": "كلما زادت القيمة، زاد احتمال تقليل تكرار الكلمات",
       "title": "عقوبة التكرار"
@@ -216,6 +219,15 @@
       "desc": "كلما زادت القيمة، زاد احتمال التوسع في مواضيع جديدة",
       "title": "جديد الحديث"
     },
+    "reasoningEffort": {
+      "desc": "كلما زادت القيمة، زادت قدرة الاستدلال، ولكن قد يؤدي ذلك إلى زيادة وقت الاستجابة واستهلاك التوكنات",
+      "options": {
+        "high": "عالي",
+        "low": "منخفض",
+        "medium": "متوسط"
+      },
+      "title": "قوة الاستدلال"
+    },
     "temperature": {
       "desc": "كلما زادت القيمة، زادت الردود عشوائية أكثر",
       "title": "التباين",
diff --git a/locales/bg-BG/discover.json b/locales/bg-BG/discover.json
index b3e924c3341c..a3cdd7736fce 100644
--- a/locales/bg-BG/discover.json
+++ b/locales/bg-BG/discover.json
@@ -126,6 +126,10 @@
         "title": "Свежест на темата"
       },
       "range": "Обхват",
+      "reasoning_effort": {
+        "desc": "Тази настройка контролира интензивността на разсъжденията на модела преди генерирането на отговор. Ниска интензивност приоритизира скоростта на отговор и спестява токени, докато висока интензивност предоставя по-пълни разсъждения, но изразходва повече токени и намалява скоростта на отговор. Стойността по подразбиране е средна, което балансира точността на разсъжденията и скоростта на отговор.",
+        "title": "Интензивност на разсъжденията"
+      },
       "temperature": {
         "desc": "Тази настройка влияе на разнообразието на отговорите на модела. По-ниски стойности водят до по-предсказуеми и типични отговори, докато по-високи стойности насърчават по-разнообразни и необичайни отговори. Когато стойността е 0, моделът винаги дава един и същ отговор на даден вход.",
         "title": "Случайност"
diff --git a/locales/bg-BG/models.json b/locales/bg-BG/models.json
index df13b75f1d2b..e847569de616 100644
--- a/locales/bg-BG/models.json
+++ b/locales/bg-BG/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 е семейство от многоезични големи езикови модели, разработени от Meta, включващо предварително обучени и модели с фино настройване с параметри 8B, 70B и 405B. Този 8B модел с фино настройване на инструкции е оптимизиран за многоезични разговорни сценарии и показва отлични резултати в множество индустриални бенчмаркове. Моделът е обучен с над 15 трилиона токена от публични данни и използва технологии като наблюдавано фино настройване и обучение с човешка обратна връзка, за да подобри полезността и безопасността на модела. Llama 3.1 поддържа генериране на текст и генериране на код, с дата на прекратяване на знанията до декември 2023 г."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview е изследователски модел, разработен от екипа на Qwen, фокусиран върху визуалните способности за инференция, който притежава уникални предимства в разбирането на сложни сцени и решаването на визуално свързани математически проблеми."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview е най-новият експериментален изследователски модел на Qwen, който се фокусира върху подобряване на AI разсъдъчните способности. Чрез изследване на сложни механизми като езикови смеси и рекурсивно разсъждение, основните предимства включват мощни аналитични способности, математически и програмистки умения. В същото време съществуват проблеми с езиковото превключване, цикли на разсъждение, съображения за безопасност и разлики в други способности."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct предлага висока надеждност в обработката на инструкции, поддържаща приложения в множество индустрии."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 е модел за инференция, управляван от подсилено обучение (RL), който решава проблемите с повторяемостта и четимостта в модела. Преди RL, DeepSeek-R1 въвежда данни за студен старт, за да оптимизира допълнително производителността на инференцията. Той показва представяне, сравнимо с OpenAI-o1 в математически, кодови и инференционни задачи, и чрез внимателно проектирани методи на обучение, подобрява общия ефект."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 обединява отличителните характеристики на предишните версии, подобрявайки общите и кодиращите способности."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 е хибриден експертен (MoE) езиков модел с 6710 милиарда параметри, използващ многоглаво внимание (MLA) и архитектурата DeepSeekMoE, комбинираща стратегии за баланс на натоварването без помощни загуби, за оптимизиране на ефективността на инференцията и обучението. Чрез предварително обучение на 14.8 трилиона висококачествени токени и последващо наблюдавано фино настройване и подсилено обучение, DeepSeek-V3 надминава други отворени модели по производителност, приближавайки се до водещите затворени модели."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B е напреднал модел, обучен за диалози с висока сложност."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 е хибриден експертен (MoE) визуално езиков модел, разработен на базата на DeepSeekMoE-27B, използващ архитектура на MoE с рядка активация, постигайки изключителна производителност с активирани само 4.5B параметри. Моделът показва отлични резултати в множество задачи, включително визуални въпроси и отговори, оптично разпознаване на символи, разбиране на документи/таблици/графики и визуална локализация."
+  },
   "deepseek-chat": {
     "description": "Новооткритият отворен модел, който съчетава общи и кодови способности, не само запазва общата диалогова способност на оригиналния Chat модел и мощната способност за обработка на код на Coder модела, но също така по-добре се съгласува с човешките предпочитания. Освен това, DeepSeek-V2.5 постигна значителни подобрения в писателските задачи, следването на инструкции и много други области."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 е новият модел за изводи на OpenAI, подходящ за сложни задачи, изискващи обширни общи знания. Моделът разполага с контекст от 128K и дата на знание до октомври 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini е нашият най-нов малък модел за инференция, който предлага висока интелигентност при същите разходи и цели за закъснение като o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba е модел на езика Mamba 2, специализиран в генерирането на код, предоставящ мощна поддръжка за напреднали кодови и разсъждателни задачи."
   },
diff --git a/locales/bg-BG/setting.json b/locales/bg-BG/setting.json
index 64a396ecd0d8..c756193aa668 100644
--- a/locales/bg-BG/setting.json
+++ b/locales/bg-BG/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Активиране на ограничението за максимален брой токени"
     },
+    "enableReasoningEffort": {
+      "title": "Активиране на настройките за интензивност на разсъжденията"
+    },
     "frequencyPenalty": {
       "desc": "Колкото по-висока е стойността, толкова по-вероятно е да се намалят повтарящите се думи",
       "title": "Наказание за честота"
@@ -216,6 +219,15 @@
       "desc": "Колкото по-висока е стойността, толкова по-вероятно е да се разшири до нови теми",
       "title": "Свежест на темата"
     },
+    "reasoningEffort": {
+      "desc": "Колкото по-висока е стойността, толкова по-силна е способността за разсъждение, но може да увеличи времето за отговор и консумацията на токени",
+      "options": {
+        "high": "Висока",
+        "low": "Ниска",
+        "medium": "Средна"
+      },
+      "title": "Интензивност на разсъжденията"
+    },
     "temperature": {
       "desc": "Колкото по-висока е стойността, толкова по-случаен е отговорът",
       "title": "Случайност",
diff --git a/locales/de-DE/discover.json b/locales/de-DE/discover.json
index 4102d2df10bd..789c094d4308 100644
--- a/locales/de-DE/discover.json
+++ b/locales/de-DE/discover.json
@@ -126,6 +126,10 @@
         "title": "Themenfrische"
       },
       "range": "Bereich",
+      "reasoning_effort": {
+        "desc": "Diese Einstellung steuert die Intensität des Denkprozesses des Modells, bevor es eine Antwort generiert. Eine niedrige Intensität priorisiert die Geschwindigkeit der Antwort und spart Token, während eine hohe Intensität eine umfassendere Argumentation bietet, jedoch mehr Token verbraucht und die Antwortgeschwindigkeit verringert. Der Standardwert ist mittel, um eine Balance zwischen Genauigkeit des Denkens und Antwortgeschwindigkeit zu gewährleisten.",
+        "title": "Denkintensität"
+      },
       "temperature": {
         "desc": "Diese Einstellung beeinflusst die Vielfalt der Antworten des Modells. Niedrigere Werte führen zu vorhersehbareren und typischen Antworten, während höhere Werte zu vielfältigeren und weniger häufigen Antworten anregen. Wenn der Wert auf 0 gesetzt wird, gibt das Modell für einen bestimmten Input immer die gleiche Antwort.",
         "title": "Zufälligkeit"
diff --git a/locales/de-DE/models.json b/locales/de-DE/models.json
index 51cee8d53936..6df969770c4c 100644
--- a/locales/de-DE/models.json
+++ b/locales/de-DE/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 ist eine Familie von mehrsprachigen großen Sprachmodellen, die von Meta entwickelt wurden und vortrainierte sowie anweisungsfeinabgestimmte Varianten mit 8B, 70B und 405B Parametern umfasst. Dieses 8B-Anweisungsfeinabgestimmte Modell wurde für mehrsprachige Dialogszenarien optimiert und zeigt in mehreren Branchen-Benchmark-Tests hervorragende Leistungen. Das Modelltraining verwendete über 150 Billionen Tokens aus öffentlichen Daten und nutzte Techniken wie überwachte Feinabstimmung und verstärkendes Lernen mit menschlichem Feedback, um die Nützlichkeit und Sicherheit des Modells zu verbessern. Llama 3.1 unterstützt Text- und Codegenerierung, mit einem Wissensstichtag von Dezember 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview ist ein forschungsorientiertes Modell, das vom Qwen-Team entwickelt wurde und sich auf visuelle Inferenzfähigkeiten konzentriert. Es hat einzigartige Vorteile beim Verständnis komplexer Szenen und der Lösung visuell verwandter mathematischer Probleme."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview ist das neueste experimentelle Forschungsmodell von Qwen, das sich auf die Verbesserung der KI-Inferenzfähigkeiten konzentriert. Durch die Erforschung komplexer Mechanismen wie Sprachmischung und rekursive Inferenz bietet es Hauptvorteile wie starke Analysefähigkeiten, mathematische und Programmierfähigkeiten. Gleichzeitig gibt es Herausforderungen wie Sprachwechsel, Inferenzzyklen, Sicherheitsüberlegungen und Unterschiede in anderen Fähigkeiten."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct bietet zuverlässige Anweisungsverarbeitungsfähigkeiten und unterstützt Anwendungen in verschiedenen Branchen."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 ist ein auf verstärkendem Lernen (RL) basierendes Inferenzmodell, das die Probleme der Wiederholbarkeit und Lesbarkeit im Modell angeht. Vor dem RL führte DeepSeek-R1 kalte Startdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmatischen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert durch sorgfältig gestaltete Trainingsmethoden die Gesamteffizienz."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 vereint die hervorragenden Merkmale früherer Versionen und verbessert die allgemeinen und kodierenden Fähigkeiten."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 ist ein hybrides Expertenmodell (MoE) mit 671 Milliarden Parametern, das eine Multi-Head-Latente-Attention (MLA) und die DeepSeekMoE-Architektur verwendet, kombiniert mit einer Lastenausgleichsstrategie ohne Hilfskosten, um die Inferenz- und Trainingseffizienz zu optimieren. Durch das Pre-Training auf 14,8 Billionen hochwertigen Tokens und anschließendes überwachten Feintuning und verstärkendes Lernen übertrifft DeepSeek-V3 in der Leistung andere Open-Source-Modelle und nähert sich führenden Closed-Source-Modellen."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B ist ein fortschrittliches Modell, das für komplexe Dialoge trainiert wurde."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 ist ein hybrides Expertenmodell (MoE) für visuelle Sprache, das auf DeepSeekMoE-27B basiert und eine spärliche Aktivierung der MoE-Architektur verwendet, um bei der Aktivierung von nur 4,5 Milliarden Parametern hervorragende Leistungen zu erzielen. Dieses Modell zeigt herausragende Ergebnisse in mehreren Aufgaben, darunter visuelle Fragenbeantwortung, optische Zeichenerkennung, Dokument-/Tabellen-/Diagrammverständnis und visuelle Lokalisierung."
+  },
   "deepseek-chat": {
     "description": "Ein neues Open-Source-Modell, das allgemeine und Codefähigkeiten kombiniert. Es bewahrt nicht nur die allgemeinen Dialogfähigkeiten des ursprünglichen Chat-Modells und die leistungsstarken Codeverarbeitungsfähigkeiten des Coder-Modells, sondern stimmt auch besser mit menschlichen Präferenzen überein. Darüber hinaus hat DeepSeek-V2.5 in mehreren Bereichen wie Schreibaufgaben und Befolgung von Anweisungen erhebliche Verbesserungen erzielt."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 ist OpenAIs neues Inferenzmodell, das für komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini ist unser neuestes kompaktes Inferenzmodell, das bei den gleichen Kosten- und Verzögerungszielen wie o1-mini hohe Intelligenz bietet."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba ist ein auf die Codegenerierung spezialisiertes Mamba 2-Sprachmodell, das starke Unterstützung für fortschrittliche Code- und Schlussfolgerungsaufgaben bietet."
   },
diff --git a/locales/de-DE/setting.json b/locales/de-DE/setting.json
index 86a5e973a152..acba3440b23a 100644
--- a/locales/de-DE/setting.json
+++ b/locales/de-DE/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Maximale Token pro Antwort aktivieren"
     },
+    "enableReasoningEffort": {
+      "title": "Aktivieren Sie die Anpassung der Schlussfolgerungsintensität"
+    },
     "frequencyPenalty": {
       "desc": "Je höher der Wert, desto wahrscheinlicher ist es, dass sich wiederholende Wörter reduziert werden",
       "title": "Frequenzstrafe"
@@ -216,6 +219,15 @@
       "desc": "Je höher der Wert, desto wahrscheinlicher ist es, dass sich das Gespräch auf neue Themen ausweitet",
       "title": "Themenfrische"
     },
+    "reasoningEffort": {
+      "desc": "Je höher der Wert, desto stärker die Schlussfolgerungsfähigkeit, aber dies kann die Antwortzeit und den Tokenverbrauch erhöhen.",
+      "options": {
+        "high": "Hoch",
+        "low": "Niedrig",
+        "medium": "Mittel"
+      },
+      "title": "Schlussfolgerungsintensität"
+    },
     "temperature": {
       "desc": "Je höher der Wert, desto zufälliger die Antwort",
       "title": "Zufälligkeit",
diff --git a/locales/en-US/discover.json b/locales/en-US/discover.json
index 84e71d0f6c36..3d2a702acad2 100644
--- a/locales/en-US/discover.json
+++ b/locales/en-US/discover.json
@@ -126,6 +126,10 @@
         "title": "Topic Freshness"
       },
       "range": "Range",
+      "reasoning_effort": {
+        "desc": "This setting controls the intensity of reasoning the model applies before generating a response. Low intensity prioritizes response speed and saves tokens, while high intensity provides more comprehensive reasoning but consumes more tokens and slows down response time. The default value is medium, balancing reasoning accuracy with response speed.",
+        "title": "Reasoning Intensity"
+      },
       "temperature": {
         "desc": "This setting affects the diversity of the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. When set to 0, the model always gives the same response to a given input.",
         "title": "Randomness"
diff --git a/locales/en-US/models.json b/locales/en-US/models.json
index 1b10f905fc50..3dc96408890d 100644
--- a/locales/en-US/models.json
+++ b/locales/en-US/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 is a family of multilingual large language models developed by Meta, including pre-trained and instruction-tuned variants with parameter sizes of 8B, 70B, and 405B. This 8B instruction-tuned model is optimized for multilingual dialogue scenarios and performs excellently in multiple industry benchmark tests. The model is trained using over 150 trillion tokens of public data and employs techniques such as supervised fine-tuning and human feedback reinforcement learning to enhance the model's usefulness and safety. Llama 3.1 supports text generation and code generation, with a knowledge cutoff date of December 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview is a research-oriented model developed by the Qwen team, focusing on visual reasoning capabilities, with unique advantages in understanding complex scenes and solving visually related mathematical problems."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview is Qwen's latest experimental research model, focusing on enhancing AI reasoning capabilities. By exploring complex mechanisms such as language mixing and recursive reasoning, its main advantages include strong analytical reasoning, mathematical, and programming abilities. However, it also faces challenges such as language switching issues, reasoning loops, safety considerations, and differences in other capabilities."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct provides highly reliable instruction processing capabilities, supporting applications across multiple industries."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 is a reinforcement learning (RL) driven inference model that addresses issues of repetitiveness and readability in models. Prior to RL, DeepSeek-R1 introduced cold start data to further optimize inference performance. It performs comparably to OpenAI-o1 in mathematical, coding, and inference tasks, and enhances overall effectiveness through carefully designed training methods."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 combines the excellent features of previous versions, enhancing general and coding capabilities."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 is a mixture of experts (MoE) language model with 671 billion parameters, utilizing multi-head latent attention (MLA) and the DeepSeekMoE architecture, combined with a load balancing strategy without auxiliary loss to optimize inference and training efficiency. Pre-trained on 14.8 trillion high-quality tokens and fine-tuned with supervision and reinforcement learning, DeepSeek-V3 outperforms other open-source models and approaches leading closed-source models in performance."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B is an advanced model trained for highly complex conversations."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 is a mixture of experts (MoE) visual language model developed based on DeepSeekMoE-27B, employing a sparsely activated MoE architecture that achieves outstanding performance while activating only 4.5 billion parameters. This model excels in various tasks such as visual question answering, optical character recognition, document/table/chart understanding, and visual localization."
+  },
   "deepseek-chat": {
     "description": "A new open-source model that integrates general and coding capabilities, retaining the general conversational abilities of the original Chat model and the powerful code handling capabilities of the Coder model, while better aligning with human preferences. Additionally, DeepSeek-V2.5 has achieved significant improvements in writing tasks, instruction following, and more."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 is OpenAI's new reasoning model, suitable for complex tasks that require extensive general knowledge. This model features a 128K context and has a knowledge cutoff date of October 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini is our latest small inference model that delivers high intelligence while maintaining the same cost and latency targets as o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba is a language model focused on code generation, providing strong support for advanced coding and reasoning tasks."
   },
diff --git a/locales/en-US/setting.json b/locales/en-US/setting.json
index 10216ebad5f2..80ffae76e5a6 100644
--- a/locales/en-US/setting.json
+++ b/locales/en-US/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Enable Max Tokens Limit"
     },
+    "enableReasoningEffort": {
+      "title": "Enable Reasoning Effort Adjustment"
+    },
     "frequencyPenalty": {
       "desc": "The higher the value, the more likely it is to reduce repeated words",
       "title": "Frequency Penalty"
@@ -216,6 +219,15 @@
       "desc": "The higher the value, the more likely it is to expand to new topics",
       "title": "Topic Freshness"
     },
+    "reasoningEffort": {
+      "desc": "The higher the value, the stronger the reasoning ability, but it may increase response time and token consumption.",
+      "options": {
+        "high": "High",
+        "low": "Low",
+        "medium": "Medium"
+      },
+      "title": "Reasoning Effort"
+    },
     "temperature": {
       "desc": "The higher the value, the more random the response",
       "title": "Randomness",
diff --git a/locales/es-ES/discover.json b/locales/es-ES/discover.json
index 42386cb6e223..1caad337e1e9 100644
--- a/locales/es-ES/discover.json
+++ b/locales/es-ES/discover.json
@@ -126,6 +126,10 @@
         "title": "Novedad del tema"
       },
       "range": "Rango",
+      "reasoning_effort": {
+        "desc": "Esta configuración se utiliza para controlar la intensidad de razonamiento del modelo antes de generar una respuesta. Una baja intensidad prioriza la velocidad de respuesta y ahorra tokens, mientras que una alta intensidad proporciona un razonamiento más completo, pero consume más tokens y reduce la velocidad de respuesta. El valor predeterminado es medio, equilibrando la precisión del razonamiento con la velocidad de respuesta.",
+        "title": "Intensidad de razonamiento"
+      },
       "temperature": {
         "desc": "Esta configuración afecta la diversidad de las respuestas del modelo. Un valor más bajo resultará en respuestas más predecibles y típicas, mientras que un valor más alto alentará respuestas más diversas y menos comunes. Cuando el valor se establece en 0, el modelo siempre dará la misma respuesta para una entrada dada.",
         "title": "Aleatoriedad"
diff --git a/locales/es-ES/models.json b/locales/es-ES/models.json
index e97fa882f51d..6dc668d0e4af 100644
--- a/locales/es-ES/models.json
+++ b/locales/es-ES/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 es parte de la familia de modelos de lenguaje a gran escala multilingües desarrollados por Meta, que incluye variantes preentrenadas y de ajuste fino por instrucciones con tamaños de parámetros de 8B, 70B y 405B. Este modelo de 8B ha sido optimizado para escenarios de diálogo multilingüe y ha destacado en múltiples pruebas de referencia de la industria. El entrenamiento del modelo utilizó más de 150 billones de tokens de datos públicos y empleó técnicas como ajuste fino supervisado y aprendizaje por refuerzo con retroalimentación humana para mejorar la utilidad y seguridad del modelo. Llama 3.1 admite generación de texto y generación de código, con una fecha límite de conocimiento hasta diciembre de 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview es un modelo de investigación desarrollado por el equipo de Qwen, enfocado en la capacidad de inferencia visual, que tiene ventajas únicas en la comprensión de escenas complejas y en la resolución de problemas matemáticos relacionados con la visión."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview es el último modelo de investigación experimental de Qwen, enfocado en mejorar la capacidad de razonamiento de la IA. A través de la exploración de mecanismos complejos como la mezcla de lenguajes y el razonamiento recursivo, sus principales ventajas incluyen una poderosa capacidad de análisis de razonamiento, así como habilidades matemáticas y de programación. Sin embargo, también presenta problemas de cambio de idioma, ciclos de razonamiento, consideraciones de seguridad y diferencias en otras capacidades."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct ofrece capacidades de procesamiento de instrucciones de alta fiabilidad, soportando aplicaciones en múltiples industrias."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 es un modelo de inferencia impulsado por aprendizaje por refuerzo (RL) que aborda los problemas de repetitividad y legibilidad en el modelo. Antes de RL, DeepSeek-R1 introdujo datos de arranque en frío, optimizando aún más el rendimiento de inferencia. Su rendimiento en tareas matemáticas, de codificación e inferencia es comparable al de OpenAI-o1, y mediante un método de entrenamiento cuidadosamente diseñado, se mejora el rendimiento general."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 combina las excelentes características de versiones anteriores, mejorando la capacidad general y de codificación."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 es un modelo de lenguaje de expertos mixtos (MoE) con 671 mil millones de parámetros, que utiliza atención latente de múltiples cabezas (MLA) y la arquitectura DeepSeekMoE, combinando una estrategia de balanceo de carga sin pérdidas auxiliares para optimizar la eficiencia de inferencia y entrenamiento. Al ser preentrenado en 14.8 billones de tokens de alta calidad y realizar ajustes finos supervisados y aprendizaje por refuerzo, DeepSeek-V3 supera en rendimiento a otros modelos de código abierto, acercándose a los modelos cerrados líderes."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B es un modelo avanzado entrenado para diálogos de alta complejidad."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 es un modelo de lenguaje visual basado en DeepSeekMoE-27B, que utiliza una arquitectura MoE de activación dispersa, logrando un rendimiento excepcional con solo 4.5B de parámetros activados. Este modelo destaca en múltiples tareas como preguntas visuales, reconocimiento óptico de caracteres, comprensión de documentos/tablas/gráficos y localización visual."
+  },
   "deepseek-chat": {
     "description": "Un nuevo modelo de código abierto que fusiona capacidades generales y de codificación, que no solo conserva la capacidad de diálogo general del modelo Chat original y la potente capacidad de procesamiento de código del modelo Coder, sino que también se alinea mejor con las preferencias humanas. Además, DeepSeek-V2.5 ha logrado mejoras significativas en tareas de escritura, seguimiento de instrucciones y más."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 es el nuevo modelo de inferencia de OpenAI, adecuado para tareas complejas que requieren un amplio conocimiento general. Este modelo tiene un contexto de 128K y una fecha de corte de conocimiento en octubre de 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini es nuestro último modelo de inferencia de tamaño pequeño, que ofrece alta inteligencia con los mismos objetivos de costo y latencia que o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba es un modelo de lenguaje Mamba 2 enfocado en la generación de código, que proporciona un fuerte apoyo para tareas avanzadas de codificación y razonamiento."
   },
diff --git a/locales/es-ES/setting.json b/locales/es-ES/setting.json
index c1ac3233f943..acd091cfb893 100644
--- a/locales/es-ES/setting.json
+++ b/locales/es-ES/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Activar límite de tokens por respuesta"
     },
+    "enableReasoningEffort": {
+      "title": "Activar ajuste de intensidad de razonamiento"
+    },
     "frequencyPenalty": {
       "desc": "Cuanto mayor sea el valor, más probable es que se reduzcan las repeticiones de palabras",
       "title": "Penalización de frecuencia"
@@ -216,6 +219,15 @@
       "desc": "Cuanto mayor sea el valor, más probable es que se amplíe a nuevos temas",
       "title": "Penalización de novedad del tema"
     },
+    "reasoningEffort": {
+      "desc": "Cuanto mayor sea el valor, más fuerte será la capacidad de razonamiento, pero puede aumentar el tiempo de respuesta y el consumo de tokens.",
+      "options": {
+        "high": "Alto",
+        "low": "Bajo",
+        "medium": "Medio"
+      },
+      "title": "Intensidad de razonamiento"
+    },
     "temperature": {
       "desc": "Cuanto mayor sea el valor, más aleatoria será la respuesta",
       "title": "Temperatura",
diff --git a/locales/fa-IR/discover.json b/locales/fa-IR/discover.json
index c44d426d0b42..486075494a6a 100644
--- a/locales/fa-IR/discover.json
+++ b/locales/fa-IR/discover.json
@@ -126,6 +126,10 @@
         "title": "تازگی موضوع"
       },
       "range": "محدوده",
+      "reasoning_effort": {
+        "desc": "این تنظیم برای کنترل شدت استدلال مدل قبل از تولید پاسخ استفاده می‌شود. شدت پایین به سرعت پاسخ‌دهی اولویت می‌دهد و توکن را صرفه‌جویی می‌کند، در حالی که شدت بالا استدلال کامل‌تری ارائه می‌دهد اما توکن بیشتری مصرف کرده و سرعت پاسخ‌دهی را کاهش می‌دهد. مقدار پیش‌فرض متوسط است که تعادل بین دقت استدلال و سرعت پاسخ‌دهی را برقرار می‌کند.",
+        "title": "شدت استدلال"
+      },
       "temperature": {
         "desc": "این تنظیمات بر تنوع پاسخ‌های مدل تأثیر می‌گذارد. مقادیر پایین‌تر منجر به پاسخ‌های قابل پیش‌بینی‌تر و معمولی‌تر می‌شود، در حالی که مقادیر بالاتر تنوع و پاسخ‌های غیرمعمول‌تر را تشویق می‌کند. وقتی مقدار به 0 تنظیم شود، مدل همیشه برای ورودی داده شده یک پاسخ یکسان ارائه می‌دهد.",
         "title": "تصادفی بودن"
diff --git a/locales/fa-IR/models.json b/locales/fa-IR/models.json
index 0b357b2514e3..931e21b282af 100644
--- a/locales/fa-IR/models.json
+++ b/locales/fa-IR/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 یکی از خانواده‌های مدل‌های زبانی بزرگ چند زبانه است که توسط Meta توسعه یافته و شامل واریانت‌های پیش‌آموزش شده و تنظیم دقیق دستوری با اندازه‌های پارامتر 8B، 70B و 405B است. این مدل 8B به طور خاص برای سناریوهای گفتگوی چند زبانه بهینه‌سازی شده و در چندین آزمون معیار صنعتی عملکرد عالی دارد. آموزش مدل با استفاده از بیش از 15 تریلیون توکن داده‌های عمومی انجام شده و از تکنیک‌های تنظیم دقیق نظارتی و یادگیری تقویتی با بازخورد انسانی برای افزایش مفید بودن و ایمنی مدل استفاده شده است. Llama 3.1 از تولید متن و تولید کد پشتیبانی می‌کند و تاریخ قطع دانش آن دسامبر 2023 است."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview یک مدل تحقیقاتی است که توسط تیم Qwen توسعه یافته و بر روی توانایی‌های استنتاج بصری تمرکز دارد و در درک صحنه‌های پیچیده و حل مسائل ریاضی مرتبط با بصری دارای مزیت‌های منحصر به فردی است."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview جدیدترین مدل تحقیقاتی تجربی Qwen است که بر بهبود توانایی استدلال AI تمرکز دارد. با کاوش در مکانیزم‌های پیچیده‌ای مانند ترکیب زبان و استدلال بازگشتی، مزایای اصلی شامل توانایی تحلیل استدلال قوی، توانایی ریاضی و برنامه‌نویسی است. در عین حال، مشکلاتی مانند تغییر زبان، حلقه‌های استدلال، ملاحظات ایمنی و تفاوت‌های دیگر در توانایی‌ها وجود دارد."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct قابلیت پردازش دستورات با قابلیت اطمینان بالا را فراهم می‌کند و از کاربردهای چندین صنعت پشتیبانی می‌کند."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 یک مدل استنتاج مبتنی بر یادگیری تقویتی (RL) است که به مشکلات تکرار و خوانایی در مدل پرداخته است. قبل از RL، DeepSeek-R1 داده‌های شروع سرد را معرفی کرد و عملکرد استنتاج را بهینه‌سازی کرد. این مدل در وظایف ریاضی، کد و استنتاج با OpenAI-o1 عملکرد مشابهی دارد و با روش‌های آموزشی طراحی شده بهبود کلی را ارائه می‌دهد."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 ویژگی‌های برجسته نسخه‌های قبلی را گرد هم آورده و توانایی‌های عمومی و کدنویسی را تقویت کرده است."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 یک مدل زبانی ترکیبی از متخصصان (MoE) با 6710 میلیارد پارامتر است که از توجه چندسر (MLA) و معماری DeepSeekMoE استفاده می‌کند و با ترکیب استراتژی تعادل بار بدون ضرر کمکی، کارایی استنتاج و آموزش را بهینه می‌سازد. با پیش‌آموزش بر روی 14.8 تریلیون توکن با کیفیت بالا و انجام تنظیم دقیق نظارتی و یادگیری تقویتی، DeepSeek-V3 در عملکرد از سایر مدل‌های منبع باز پیشی می‌گیرد و به مدل‌های بسته پیشرو نزدیک می‌شود."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek LLM Chat (67B) یک مدل نوآورانه هوش مصنوعی است که توانایی درک عمیق زبان و تعامل را فراهم می‌کند."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 یک مدل زبانی بصری مبتنی بر DeepSeekMoE-27B است که از معماری MoE با فعال‌سازی پراکنده استفاده می‌کند و در حالی که تنها 4.5 میلیارد پارامتر فعال است، عملکرد فوق‌العاده‌ای را ارائه می‌دهد. این مدل در چندین وظیفه از جمله پرسش و پاسخ بصری، شناسایی کاراکتر نوری، درک اسناد/جدول‌ها/نمودارها و مکان‌یابی بصری عملکرد عالی دارد."
+  },
   "deepseek-chat": {
     "description": "مدل متن‌باز جدیدی که توانایی‌های عمومی و کدنویسی را ترکیب می‌کند. این مدل نه تنها توانایی گفتگوی عمومی مدل Chat و توانایی قدرتمند پردازش کد مدل Coder را حفظ کرده است، بلکه به ترجیحات انسانی نیز بهتر همسو شده است. علاوه بر این، DeepSeek-V2.5 در وظایف نوشتاری، پیروی از دستورات و سایر جنبه‌ها نیز بهبودهای قابل توجهی داشته است."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "تمرکز بر استدلال پیشرفته و حل مسائل پیچیده، از جمله وظایف ریاضی و علمی. بسیار مناسب برای برنامه‌هایی که نیاز به درک عمیق از زمینه و جریان کاری خودمختار دارند."
   },
+  "o3-mini": {
+    "description": "o3-mini جدیدترین مدل استنتاج کوچک ماست که هوش بالایی را با هزینه و هدف تأخیر مشابه o1-mini ارائه می‌دهد."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba یک مدل زبان Mamba 2 است که بر تولید کد تمرکز دارد و پشتیبانی قدرتمندی برای وظایف پیشرفته کدنویسی و استدلال ارائه می‌دهد."
   },
diff --git a/locales/fa-IR/setting.json b/locales/fa-IR/setting.json
index 7f994d979b26..da8fa8c17526 100644
--- a/locales/fa-IR/setting.json
+++ b/locales/fa-IR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "فعال‌سازی محدودیت پاسخ"
     },
+    "enableReasoningEffort": {
+      "title": "فعال‌سازی تنظیم شدت استدلال"
+    },
     "frequencyPenalty": {
       "desc": "هرچه مقدار بیشتر باشد، احتمال کاهش تکرار کلمات بیشتر است",
       "title": "مجازات تکرار"
@@ -216,6 +219,15 @@
       "desc": "هرچه مقدار بیشتر باشد، احتمال گسترش به موضوعات جدید بیشتر است",
       "title": "تازگی موضوع"
     },
+    "reasoningEffort": {
+      "desc": "هرچه مقدار بیشتر باشد، توانایی استدلال قوی‌تر است، اما ممکن است زمان پاسخ و مصرف توکن را افزایش دهد",
+      "options": {
+        "high": "بالا",
+        "low": "پایین",
+        "medium": "متوسط"
+      },
+      "title": "شدت استدلال"
+    },
     "temperature": {
       "desc": "هرچه مقدار بیشتر باشد، پاسخ‌ها تصادفی‌تر خواهند بود",
       "title": "تصادفی بودن",
diff --git a/locales/fr-FR/discover.json b/locales/fr-FR/discover.json
index 508b8d010bb0..5fb3a7588ef8 100644
--- a/locales/fr-FR/discover.json
+++ b/locales/fr-FR/discover.json
@@ -126,6 +126,10 @@
         "title": "Fraîcheur des sujets"
       },
       "range": "Plage",
+      "reasoning_effort": {
+        "desc": "Ce paramètre contrôle l'intensité de raisonnement du modèle avant de générer une réponse. Une faible intensité privilégie la rapidité de réponse et économise des tokens, tandis qu'une forte intensité offre un raisonnement plus complet, mais consomme plus de tokens et ralentit la réponse. La valeur par défaut est moyenne, équilibrant précision du raisonnement et rapidité de réponse.",
+        "title": "Intensité de raisonnement"
+      },
       "temperature": {
         "desc": "Ce paramètre influence la diversité des réponses du modèle. Des valeurs plus basses entraînent des réponses plus prévisibles et typiques, tandis que des valeurs plus élevées encouragent des réponses plus variées et moins courantes. Lorsque la valeur est fixée à 0, le modèle donne toujours la même réponse pour une entrée donnée.",
         "title": "Aléatoire"
diff --git a/locales/fr-FR/models.json b/locales/fr-FR/models.json
index 36433761a4f9..b26347bfb801 100644
--- a/locales/fr-FR/models.json
+++ b/locales/fr-FR/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 est une famille de modèles de langage à grande échelle multilingues développée par Meta, comprenant des variantes pré-entraînées et d'ajustement d'instructions de tailles de paramètres de 8B, 70B et 405B. Ce modèle d'ajustement d'instructions 8B est optimisé pour des scénarios de dialogue multilingue, montrant d'excellentes performances dans plusieurs tests de référence de l'industrie. L'entraînement du modèle a utilisé plus de 150 trillions de tokens de données publiques, et des techniques telles que l'ajustement supervisé et l'apprentissage par renforcement basé sur les retours humains ont été appliquées pour améliorer l'utilité et la sécurité du modèle. Llama 3.1 prend en charge la génération de texte et de code, avec une date limite de connaissances fixée à décembre 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview est un modèle de recherche développé par l'équipe Qwen, axé sur les capacités d'inférence visuelle, avec des avantages uniques dans la compréhension de scènes complexes et la résolution de problèmes mathématiques liés à la vision."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview est le dernier modèle de recherche expérimental de Qwen, axé sur l'amélioration des capacités de raisonnement de l'IA. En explorant des mécanismes complexes tels que le mélange de langues et le raisonnement récursif, ses principaux avantages incluent de puissantes capacités d'analyse de raisonnement, ainsi que des compétences en mathématiques et en programmation. Cependant, il existe également des problèmes de changement de langue, des cycles de raisonnement, des considérations de sécurité et des différences dans d'autres capacités."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct offre des capacités de traitement d'instructions hautement fiables, prenant en charge des applications dans divers secteurs."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 est un modèle d'inférence alimenté par l'apprentissage par renforcement (RL), qui résout les problèmes de répétitivité et de lisibilité dans le modèle. Avant le RL, DeepSeek-R1 a introduit des données de démarrage à froid, optimisant encore les performances d'inférence. Il se compare à OpenAI-o1 en mathématiques, en code et dans les tâches d'inférence, et améliore l'ensemble des résultats grâce à des méthodes d'entraînement soigneusement conçues."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 intègre les excellentes caractéristiques des versions précédentes, renforçant les capacités générales et de codage."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 est un modèle de langage à experts mixtes (MoE) avec 6710 milliards de paramètres, utilisant une attention multi-tête (MLA) et l'architecture DeepSeekMoE, combinée à une stratégie d'équilibrage de charge sans perte auxiliaire, optimisant l'efficacité d'inférence et d'entraînement. Pré-entraîné sur 14,8 trillions de tokens de haute qualité, suivi d'un ajustement supervisé et d'apprentissage par renforcement, DeepSeek-V3 surpasse les autres modèles open source en performance, se rapprochant des modèles fermés de premier plan."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B est un modèle avancé formé pour des dialogues de haute complexité."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 est un modèle de langage visuel basé sur DeepSeekMoE-27B, utilisant une architecture MoE à activation sparse, réalisant des performances exceptionnelles avec seulement 4,5 milliards de paramètres activés. Ce modèle excelle dans plusieurs tâches telles que les questions visuelles, la reconnaissance optique de caractères, la compréhension de documents/tableaux/graphes et le positionnement visuel."
+  },
   "deepseek-chat": {
     "description": "Un nouveau modèle open source qui fusionne des capacités générales et de code, conservant non seulement la capacité de dialogue général du modèle Chat d'origine et la puissante capacité de traitement de code du modèle Coder, mais s'alignant également mieux sur les préférences humaines. De plus, DeepSeek-V2.5 a réalisé des améliorations significatives dans plusieurs domaines tels que les tâches d'écriture et le suivi des instructions."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 est le nouveau modèle de raisonnement d'OpenAI, adapté aux tâches complexes nécessitant une vaste connaissance générale. Ce modèle dispose d'un contexte de 128K et d'une date limite de connaissance en octobre 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini est notre dernier modèle d'inférence compact, offrant une grande intelligence avec les mêmes objectifs de coût et de latence que o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba est un modèle de langage Mamba 2 axé sur la génération de code, offrant un soutien puissant pour des tâches avancées de codage et de raisonnement."
   },
diff --git a/locales/fr-FR/setting.json b/locales/fr-FR/setting.json
index 1b8cd51a407b..deb9575a0f7c 100644
--- a/locales/fr-FR/setting.json
+++ b/locales/fr-FR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Activer la limite de tokens par réponse"
     },
+    "enableReasoningEffort": {
+      "title": "Activer l'ajustement de l'intensité de raisonnement"
+    },
     "frequencyPenalty": {
       "desc": "Plus la valeur est élevée, plus il est probable de réduire les mots répétés",
       "title": "Pénalité de fréquence"
@@ -216,6 +219,15 @@
       "desc": "Plus la valeur est élevée, plus il est probable d'explorer de nouveaux sujets",
       "title": "Pénalité de présence"
     },
+    "reasoningEffort": {
+      "desc": "Plus la valeur est élevée, plus la capacité de raisonnement est forte, mais cela peut augmenter le temps de réponse et la consommation de jetons",
+      "options": {
+        "high": "Élevé",
+        "low": "Bas",
+        "medium": "Moyen"
+      },
+      "title": "Intensité de raisonnement"
+    },
     "temperature": {
       "desc": "Plus la valeur est élevée, plus la réponse est aléatoire",
       "title": "Aléatoire",
diff --git a/locales/it-IT/discover.json b/locales/it-IT/discover.json
index ac7805a8e64f..e59f6cd2ea53 100644
--- a/locales/it-IT/discover.json
+++ b/locales/it-IT/discover.json
@@ -126,6 +126,10 @@
         "title": "Freschezza del tema"
       },
       "range": "Intervallo",
+      "reasoning_effort": {
+        "desc": "Questa impostazione controlla l'intensità del ragionamento del modello prima di generare una risposta. Un'intensità bassa privilegia la velocità di risposta e risparmia Token, mentre un'intensità alta fornisce un ragionamento più completo, ma consuma più Token e riduce la velocità di risposta. Il valore predefinito è medio, bilanciando l'accuratezza del ragionamento e la velocità di risposta.",
+        "title": "Intensità del ragionamento"
+      },
       "temperature": {
         "desc": "Questa impostazione influisce sulla diversità delle risposte del modello. Valori più bassi portano a risposte più prevedibili e tipiche, mentre valori più alti incoraggiano risposte più varie e insolite. Quando il valore è impostato a 0, il modello fornisce sempre la stessa risposta per un dato input.",
         "title": "Casualità"
diff --git a/locales/it-IT/models.json b/locales/it-IT/models.json
index 659f6311eb87..2f3fab7bad32 100644
--- a/locales/it-IT/models.json
+++ b/locales/it-IT/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 è una famiglia di modelli linguistici di grandi dimensioni multilingue sviluppata da Meta, che include varianti pre-addestrate e con fine-tuning per istruzioni con dimensioni di 8B, 70B e 405B. Questo modello di fine-tuning per istruzioni da 8B è ottimizzato per scenari di dialogo multilingue e ha dimostrato prestazioni eccellenti in vari benchmark di settore. L'addestramento del modello ha utilizzato oltre 150 trilioni di token di dati pubblici e ha impiegato tecniche come il fine-tuning supervisionato e l'apprendimento per rinforzo basato su feedback umano per migliorare l'utilità e la sicurezza del modello. Llama 3.1 supporta la generazione di testi e di codice, con una data di scadenza delle conoscenze fissata a dicembre 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview è un modello di ricerca sviluppato dal team Qwen, focalizzato sulle capacità di inferenza visiva, con vantaggi unici nella comprensione di scenari complessi e nella risoluzione di problemi matematici legati alla visione."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview è l'ultimo modello di ricerca sperimentale di Qwen, focalizzato sul miglioramento delle capacità di ragionamento dell'IA. Esplorando meccanismi complessi come la mescolanza linguistica e il ragionamento ricorsivo, i principali vantaggi includono potenti capacità di analisi del ragionamento, abilità matematiche e di programmazione. Tuttavia, ci sono anche problemi di cambio linguistico, cicli di ragionamento, considerazioni di sicurezza e differenze in altre capacità."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct offre capacità di elaborazione di istruzioni altamente affidabili, supportando applicazioni in vari settori."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 è un modello di inferenza guidato da apprendimento rinforzato (RL) che affronta i problemi di ripetitività e leggibilità del modello. Prima dell'RL, DeepSeek-R1 ha introdotto dati di avvio a freddo, ottimizzando ulteriormente le prestazioni di inferenza. Si comporta in modo comparabile a OpenAI-o1 in compiti matematici, di codifica e di inferenza, e migliora l'efficacia complessiva grazie a metodi di addestramento ben progettati."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 combina le eccellenti caratteristiche delle versioni precedenti, migliorando le capacità generali e di codifica."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 è un modello linguistico a esperti misti (MoE) con 6710 miliardi di parametri, che utilizza attenzione multi-testa (MLA) e architettura DeepSeekMoE, combinando strategie di bilanciamento del carico senza perdite ausiliarie per ottimizzare l'efficienza di inferenza e addestramento. Pre-addestrato su 14,8 trilioni di token di alta qualità e successivamente affinato supervisionato e tramite apprendimento rinforzato, DeepSeek-V3 supera le prestazioni di altri modelli open source, avvicinandosi ai modelli closed source leader."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B è un modello avanzato addestrato per dialoghi ad alta complessità."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 è un modello linguistico visivo a esperti misti (MoE) sviluppato sulla base di DeepSeekMoE-27B, che utilizza un'architettura MoE a attivazione sparsa, raggiungendo prestazioni eccezionali attivando solo 4,5 miliardi di parametri. Questo modello eccelle in vari compiti, tra cui domande visive, riconoscimento ottico dei caratteri, comprensione di documenti/tabelle/grafici e localizzazione visiva."
+  },
   "deepseek-chat": {
     "description": "Un nuovo modello open source che integra capacità generali e di codifica, mantenendo non solo le capacità conversazionali generali del modello Chat originale, ma anche la potente capacità di elaborazione del codice del modello Coder, allineandosi meglio alle preferenze umane. Inoltre, DeepSeek-V2.5 ha ottenuto notevoli miglioramenti in vari aspetti, come i compiti di scrittura e il rispetto delle istruzioni."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 è il nuovo modello di inferenza di OpenAI, adatto a compiti complessi che richiedono una vasta conoscenza generale. Questo modello ha un contesto di 128K e una data di cutoff della conoscenza di ottobre 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini è il nostro ultimo modello di inferenza compatto, che offre un'intelligenza elevata con gli stessi obiettivi di costo e latenza di o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba è un modello linguistico Mamba 2 focalizzato sulla generazione di codice, offre un forte supporto per compiti avanzati di codifica e ragionamento."
   },
diff --git a/locales/it-IT/setting.json b/locales/it-IT/setting.json
index 0ab578883a88..21f4ac18acb3 100644
--- a/locales/it-IT/setting.json
+++ b/locales/it-IT/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Abilita limite di risposta singola"
     },
+    "enableReasoningEffort": {
+      "title": "Attiva la regolazione dell'intensità del ragionamento"
+    },
     "frequencyPenalty": {
       "desc": "Più alto è il valore, più probabile è la riduzione delle parole ripetute",
       "title": "Penalità di frequenza"
@@ -216,6 +219,15 @@
       "desc": "Più alto è il valore, più probabile è l'estensione a nuovi argomenti",
       "title": "Freschezza dell'argomento"
     },
+    "reasoningEffort": {
+      "desc": "Maggiore è il valore, più forte è la capacità di ragionamento, ma potrebbe aumentare il tempo di risposta e il consumo di Token",
+      "options": {
+        "high": "Alto",
+        "low": "Basso",
+        "medium": "Medio"
+      },
+      "title": "Intensità del ragionamento"
+    },
     "temperature": {
       "desc": "Più alto è il valore, più casuale è la risposta",
       "title": "Casualità",
diff --git a/locales/ja-JP/discover.json b/locales/ja-JP/discover.json
index c7bb66f3a6c8..291122a9bb48 100644
--- a/locales/ja-JP/discover.json
+++ b/locales/ja-JP/discover.json
@@ -126,6 +126,10 @@
         "title": "トピックの新鮮さ"
       },
       "range": "範囲",
+      "reasoning_effort": {
+        "desc": "この設定は、モデルが回答を生成する前の推論の強度を制御するために使用されます。低強度は応答速度を優先し、トークンを節約しますが、高強度はより完全な推論を提供しますが、より多くのトークンを消費し、応答速度が低下します。デフォルト値は中で、推論の正確性と応答速度のバランスを取ります。",
+        "title": "推論強度"
+      },
       "temperature": {
         "desc": "この設定は、モデルの応答の多様性に影響を与えます。低い値はより予測可能で典型的な応答をもたらし、高い値はより多様で珍しい応答を奨励します。値が0に設定されると、モデルは与えられた入力に対して常に同じ応答を返します。",
         "title": "ランダム性"
diff --git a/locales/ja-JP/models.json b/locales/ja-JP/models.json
index cc411407e6fd..c61edce9f834 100644
--- a/locales/ja-JP/models.json
+++ b/locales/ja-JP/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1はMetaが開発した多言語大規模言語モデルファミリーで、8B、70B、405Bの3つのパラメータ規模の事前訓練および指示微調整バリアントを含みます。この8B指示微調整モデルは多言語対話シーンに最適化されており、複数の業界ベンチマークテストで優れたパフォーマンスを示しています。モデルの訓練には150兆トークン以上の公開データが使用され、監視微調整や人間のフィードバック強化学習などの技術が採用され、モデルの有用性と安全性が向上しています。Llama 3.1はテキスト生成とコード生成をサポートし、知識のカットオフ日は2023年12月です。"
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Previewは、Qwenチームによって開発された視覚推論能力に特化した研究モデルで、複雑なシーン理解や視覚に関連する数学問題の解決において独自の強みを持っています。"
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-PreviewはQwenの最新の実験的研究モデルで、AIの推論能力を向上させることに特化しています。言語の混合、再帰的推論などの複雑なメカニズムを探求することで、主な利点は強力な推論分析能力、数学およびプログラミング能力です。同時に、言語切り替えの問題、推論のループ、安全性の考慮、その他の能力の違いも存在します。"
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instructは、高い信頼性の指示処理能力を提供し、多業界アプリケーションをサポートします。"
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1は、強化学習（RL）駆動の推論モデルで、モデル内の繰り返しと可読性の問題を解決します。RLの前に、DeepSeek-R1はコールドスタートデータを導入し、推論性能をさらに最適化しました。数学、コード、推論タスクにおいてOpenAI-o1と同等の性能を発揮し、精巧に設計されたトレーニング手法によって全体的な効果を向上させています。"
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5は以前のバージョンの優れた特徴を集約し、汎用性とコーディング能力を強化しました。"
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3は、6710億パラメータを持つ混合専門家（MoE）言語モデルで、マルチヘッド潜在注意（MLA）とDeepSeekMoEアーキテクチャを採用し、無補助損失の負荷バランス戦略を組み合わせて推論とトレーニングの効率を最適化しています。14.8兆の高品質トークンで事前トレーニングを行い、監視微調整と強化学習を経て、DeepSeek-V3は他のオープンソースモデルを超え、先進的なクローズドソースモデルに近づいています。"
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67Bは、高い複雑性の対話のために訓練された先進的なモデルです。"
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2は、DeepSeekMoE-27Bを基に開発された混合専門家（MoE）視覚言語モデルで、スパースアクティベーションのMoEアーキテクチャを採用し、わずか4.5Bパラメータを活性化することで卓越した性能を実現しています。このモデルは、視覚的質問応答、光学文字認識、文書/表/グラフ理解、視覚的定位などの複数のタスクで優れたパフォーマンスを発揮します。"
+  },
   "deepseek-chat": {
     "description": "一般的な対話能力と強力なコード処理能力を兼ね備えた新しいオープンソースモデルであり、元のChatモデルの対話能力とCoderモデルのコード処理能力を保持しつつ、人間の好みにより良く整合しています。さらに、DeepSeek-V2.5は、執筆タスクや指示に従う能力など、さまざまな面で大幅な向上を実現しました。"
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1はOpenAIの新しい推論モデルで、広範な一般知識を必要とする複雑なタスクに適しています。このモデルは128Kのコンテキストを持ち、2023年10月の知識のカットオフがあります。"
   },
+  "o3-mini": {
+    "description": "o3-miniは、o1-miniと同じコストと遅延目標で高い知能を提供する最新の小型推論モデルです。"
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mambaは、コード生成に特化したMamba 2言語モデルであり、高度なコードおよび推論タスクを強力にサポートします。"
   },
diff --git a/locales/ja-JP/setting.json b/locales/ja-JP/setting.json
index b09714913d6b..5b948642a254 100644
--- a/locales/ja-JP/setting.json
+++ b/locales/ja-JP/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "単一応答制限を有効にする"
     },
+    "enableReasoningEffort": {
+      "title": "推論強度調整を有効にする"
+    },
     "frequencyPenalty": {
       "desc": "値が大きいほど、単語の繰り返しを減らす可能性が高くなります",
       "title": "頻度ペナルティ"
@@ -216,6 +219,15 @@
       "desc": "値が大きいほど、新しいトピックに拡張する可能性が高くなります",
       "title": "トピックの新鮮度"
     },
+    "reasoningEffort": {
+      "desc": "値が大きいほど推論能力が高まりますが、応答時間とトークン消費が増加する可能性があります",
+      "options": {
+        "high": "高",
+        "low": "低",
+        "medium": "中"
+      },
+      "title": "推論強度"
+    },
     "temperature": {
       "desc": "値が大きいほど、応答がよりランダムになります",
       "title": "ランダム性",
diff --git a/locales/ko-KR/discover.json b/locales/ko-KR/discover.json
index ba698e083b07..91c64a86bc51 100644
--- a/locales/ko-KR/discover.json
+++ b/locales/ko-KR/discover.json
@@ -126,6 +126,10 @@
         "title": "주제 신선도"
       },
       "range": "범위",
+      "reasoning_effort": {
+        "desc": "이 설정은 모델이 응답을 생성하기 전에 추론 강도를 제어하는 데 사용됩니다. 낮은 강도는 응답 속도를 우선시하고 토큰을 절약하며, 높은 강도는 더 완전한 추론을 제공하지만 더 많은 토큰을 소모하고 응답 속도를 저하시킵니다. 기본값은 중간으로, 추론 정확성과 응답 속도의 균형을 맞춥니다.",
+        "title": "추론 강도"
+      },
       "temperature": {
         "desc": "이 설정은 모델 응답의 다양성에 영향을 미칩니다. 낮은 값은 더 예측 가능하고 전형적인 응답을 유도하며, 높은 값은 더 다양하고 드문 응답을 장려합니다. 값이 0으로 설정되면 모델은 주어진 입력에 대해 항상 동일한 응답을 제공합니다.",
         "title": "무작위성"
diff --git a/locales/ko-KR/models.json b/locales/ko-KR/models.json
index f3ae9d4fb514..bc9a745d1f3d 100644
--- a/locales/ko-KR/models.json
+++ b/locales/ko-KR/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1은 Meta가 개발한 다국어 대규모 언어 모델 가족으로, 8B, 70B 및 405B의 세 가지 파라미터 규모의 사전 훈련 및 지침 미세 조정 변형을 포함합니다. 이 8B 지침 미세 조정 모델은 다국어 대화 시나리오에 최적화되어 있으며, 여러 산업 벤치마크 테스트에서 우수한 성능을 보입니다. 모델 훈련에는 15조 개 이상의 공개 데이터 토큰이 사용되었으며, 감독 미세 조정 및 인간 피드백 강화 학습과 같은 기술을 통해 모델의 유용성과 안전성을 향상시켰습니다. Llama 3.1은 텍스트 생성 및 코드 생성을 지원하며, 지식 마감일은 2023년 12월입니다."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview는 Qwen 팀이 개발한 시각적 추론 능력에 중점을 둔 연구 모델로, 복잡한 장면 이해 및 시각 관련 수학 문제 해결에서 독특한 장점을 가지고 있습니다."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview는 Qwen의 최신 실험적 연구 모델로, AI 추론 능력을 향상시키는 데 중점을 두고 있습니다. 언어 혼합, 재귀 추론 등 복잡한 메커니즘을 탐구하며, 주요 장점으로는 강력한 추론 분석 능력, 수학 및 프로그래밍 능력이 포함됩니다. 동시에 언어 전환 문제, 추론 루프, 안전성 고려 및 기타 능력 차이와 같은 문제도 존재합니다."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct는 높은 신뢰성을 가진 지시 처리 능력을 제공하며, 다양한 산업 응용을 지원합니다."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1은 강화 학습(RL) 기반의 추론 모델로, 모델 내의 반복성과 가독성 문제를 해결합니다. RL 이전에 DeepSeek-R1은 콜드 스타트 데이터를 도입하여 추론 성능을 더욱 최적화했습니다. 수학, 코드 및 추론 작업에서 OpenAI-o1과 유사한 성능을 보이며, 정교하게 설계된 훈련 방법을 통해 전체적인 효과를 향상시켰습니다."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5는 이전 버전의 우수한 기능을 집약하여 일반 및 인코딩 능력을 강화했습니다."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3는 6710억 개의 매개변수를 가진 혼합 전문가(MoE) 언어 모델로, 다중 헤드 잠재 주의(MLA) 및 DeepSeekMoE 아키텍처를 채택하여 보조 손실 없는 부하 균형 전략을 결합하여 추론 및 훈련 효율성을 최적화합니다. 14.8조 개의 고품질 토큰에서 사전 훈련을 수행하고 감독 미세 조정 및 강화 학습을 통해 DeepSeek-V3는 성능 면에서 다른 오픈 소스 모델을 초월하며, 선도적인 폐쇄형 모델에 근접합니다."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B는 고복잡성 대화를 위해 훈련된 고급 모델입니다."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2는 DeepSeekMoE-27B를 기반으로 개발된 혼합 전문가(MoE) 비주얼 언어 모델로, 희소 활성화 MoE 아키텍처를 사용하여 4.5B 매개변수만 활성화된 상태에서 뛰어난 성능을 발휘합니다. 이 모델은 비주얼 질문 응답, 광학 문자 인식, 문서/표/차트 이해 및 비주얼 위치 지정 등 여러 작업에서 우수한 성과를 보입니다."
+  },
   "deepseek-chat": {
     "description": "일반 및 코드 능력을 융합한 새로운 오픈 소스 모델로, 기존 Chat 모델의 일반 대화 능력과 Coder 모델의 강력한 코드 처리 능력을 유지하면서 인간의 선호에 더 잘 맞춰졌습니다. 또한, DeepSeek-V2.5는 작문 작업, 지시 따르기 등 여러 측면에서 큰 향상을 이루었습니다."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1은 OpenAI의 새로운 추론 모델로, 광범위한 일반 지식이 필요한 복잡한 작업에 적합합니다. 이 모델은 128K의 컨텍스트와 2023년 10월의 지식 기준일을 가지고 있습니다."
   },
+  "o3-mini": {
+    "description": "o3-mini는 최신 소형 추론 모델로, o1-mini와 동일한 비용과 지연 목표에서 높은 지능을 제공합니다."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba는 코드 생성을 전문으로 하는 Mamba 2 언어 모델로, 고급 코드 및 추론 작업에 강력한 지원을 제공합니다."
   },
diff --git a/locales/ko-KR/setting.json b/locales/ko-KR/setting.json
index 19dbb33bd78d..72525688ba9c 100644
--- a/locales/ko-KR/setting.json
+++ b/locales/ko-KR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "단일 응답 제한 활성화"
     },
+    "enableReasoningEffort": {
+      "title": "추론 강도 조정 활성화"
+    },
     "frequencyPenalty": {
       "desc": "값이 클수록 반복 단어가 줄어듭니다",
       "title": "빈도 패널티"
@@ -216,6 +219,15 @@
       "desc": "값이 클수록 새로운 주제로 확장될 가능성이 높아집니다",
       "title": "주제 신선도"
     },
+    "reasoningEffort": {
+      "desc": "값이 클수록 추론 능력이 강해지지만, 응답 시간과 토큰 소모가 증가할 수 있습니다.",
+      "options": {
+        "high": "높음",
+        "low": "낮음",
+        "medium": "중간"
+      },
+      "title": "추론 강도"
+    },
     "temperature": {
       "desc": "값이 클수록 응답이 더 무작위해집니다",
       "title": "랜덤성",
diff --git a/locales/nl-NL/discover.json b/locales/nl-NL/discover.json
index 5ff3bd314579..c7f34b75ee74 100644
--- a/locales/nl-NL/discover.json
+++ b/locales/nl-NL/discover.json
@@ -126,6 +126,10 @@
         "title": "Onderwerp versheid"
       },
       "range": "Bereik",
+      "reasoning_effort": {
+        "desc": "Deze instelling wordt gebruikt om de redeneerkracht van het model te regelen voordat het een antwoord genereert. Lage kracht geeft prioriteit aan de responssnelheid en bespaart tokens, terwijl hoge kracht een completere redenering biedt, maar meer tokens verbruikt en de responssnelheid verlaagt. De standaardwaarde is gemiddeld, wat een balans biedt tussen redeneringsnauwkeurigheid en responssnelheid.",
+        "title": "Redeneerkracht"
+      },
       "temperature": {
         "desc": "Deze instelling beïnvloedt de diversiteit van de reacties van het model. Lagere waarden leiden tot meer voorspelbare en typische reacties, terwijl hogere waarden meer diverse en ongebruikelijke reacties aanmoedigen. Wanneer de waarde op 0 is ingesteld, geeft het model altijd dezelfde reactie op een gegeven invoer.",
         "title": "Willekeurigheid"
diff --git a/locales/nl-NL/models.json b/locales/nl-NL/models.json
index 86905c9647c2..9ba6e658c35e 100644
--- a/locales/nl-NL/models.json
+++ b/locales/nl-NL/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 is een familie van meertalige grote taalmodellen ontwikkeld door Meta, inclusief voorgetrainde en instructie-fijn afgestelde varianten met parameter groottes van 8B, 70B en 405B. Dit 8B instructie-fijn afgestelde model is geoptimaliseerd voor meertalige gespreksscenario's en presteert uitstekend in verschillende industriële benchmarktests. Het model is getraind met meer dan 150 biljoen tokens van openbare gegevens en maakt gebruik van technieken zoals supervisie-fijn afstemming en versterkend leren met menselijke feedback om de bruikbaarheid en veiligheid van het model te verbeteren. Llama 3.1 ondersteunt tekstgeneratie en codegeneratie, met een kennisafkapdatum van december 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview is een onderzoeksmodel ontwikkeld door het Qwen-team dat zich richt op visuele inferentiecapaciteiten en unieke voordelen heeft in het begrijpen van complexe scenario's en het oplossen van visueel gerelateerde wiskundige problemen."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview is het nieuwste experimentele onderzoeksmodel van Qwen, gericht op het verbeteren van AI-redeneringscapaciteiten. Door het verkennen van complexe mechanismen zoals taalmixing en recursieve redenering, zijn de belangrijkste voordelen onder andere krachtige redeneringsanalyses, wiskundige en programmeervaardigheden. Tegelijkertijd zijn er ook problemen met taalwisseling, redeneringscycli, veiligheidskwesties en verschillen in andere capaciteiten."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct biedt betrouwbare instructieverwerkingscapaciteiten en ondersteunt toepassingen in verschillende sectoren."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 is een inferentiemodel aangedreven door versterkend leren (RL) dat de problemen van herhaling en leesbaarheid in het model aanpakt. Voor RL introduceerde DeepSeek-R1 koude startgegevens om de inferentieprestaties verder te optimaliseren. Het presteert vergelijkbaar met OpenAI-o1 in wiskunde, code en inferentietaken, en verbetert de algehele effectiviteit door zorgvuldig ontworpen trainingsmethoden."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 combineert de uitstekende kenmerken van eerdere versies en versterkt de algemene en coderingscapaciteiten."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 is een hybride expert (MoE) taalmodel met 6710 miljard parameters, dat gebruikmaakt van multi-head latent attention (MLA) en de DeepSeekMoE-architectuur, gecombineerd met een load balancing-strategie zonder extra verlies, om de inferentie- en trainingsefficiëntie te optimaliseren. Door voorgetraind te worden op 14,8 biljoen hoogwaardige tokens en vervolgens te worden fijngetuned met supervisie en versterkend leren, overtreft DeepSeek-V3 de prestaties van andere open-source modellen en komt het dicht in de buurt van toonaangevende gesloten modellen."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B is een geavanceerd model dat is getraind voor complexe gesprekken."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 is een hybride expert (MoE) visueel taalmodel ontwikkeld op basis van DeepSeekMoE-27B, dat gebruikmaakt van een MoE-architectuur met spaarzame activatie en uitstekende prestaties levert met slechts 4,5 miljard geactiveerde parameters. Dit model presteert uitstekend in verschillende taken, waaronder visuele vraag- en antwoord, optische tekenherkenning, document/tabel/grafiekbegrip en visuele positionering."
+  },
   "deepseek-chat": {
     "description": "Een nieuw open-source model dat algemene en code-capaciteiten combineert, behoudt niet alleen de algemene conversatiecapaciteiten van het oorspronkelijke Chat-model en de krachtige codeverwerkingscapaciteiten van het Coder-model, maar is ook beter afgestemd op menselijke voorkeuren. Bovendien heeft DeepSeek-V2.5 aanzienlijke verbeteringen gerealiseerd in schrijfopdrachten, instructievolging en andere gebieden."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 is het nieuwe redeneermodel van OpenAI, geschikt voor complexe taken die uitgebreide algemene kennis vereisen. Dit model heeft een context van 128K en een kennisafkapdatum van oktober 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini is ons nieuwste kleine inferentiemodel dat hoge intelligentie biedt met dezelfde kosten- en vertragingdoelen als o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba is een Mamba 2-taalmodel dat zich richt op codegeneratie en krachtige ondersteuning biedt voor geavanceerde code- en inferentietaken."
   },
diff --git a/locales/nl-NL/setting.json b/locales/nl-NL/setting.json
index 97e23bdf116e..65829d32d6fd 100644
--- a/locales/nl-NL/setting.json
+++ b/locales/nl-NL/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Limiet voor enkele reacties inschakelen"
     },
+    "enableReasoningEffort": {
+      "title": "Inschakelen van redeneringsinspanningsaanpassing"
+    },
     "frequencyPenalty": {
       "desc": "Hoe hoger de waarde, hoe waarschijnlijker het is dat herhaalde woorden worden verminderd",
       "title": "Frequentieboete"
@@ -216,6 +219,15 @@
       "desc": "Hoe hoger de waarde, hoe waarschijnlijker het is dat het gesprek naar nieuwe onderwerpen wordt uitgebreid",
       "title": "Onderwerpnieuwheid"
     },
+    "reasoningEffort": {
+      "desc": "Hoe hoger de waarde, hoe sterker de redeneringscapaciteit, maar dit kan de responstijd en het tokenverbruik verhogen",
+      "options": {
+        "high": "Hoog",
+        "low": "Laag",
+        "medium": "Gemiddeld"
+      },
+      "title": "Redeneringsinspanningsniveau"
+    },
     "temperature": {
       "desc": "Hoe hoger de waarde, hoe willekeuriger de reactie",
       "title": "Willekeurigheid",
diff --git a/locales/pl-PL/discover.json b/locales/pl-PL/discover.json
index fb78a54a8222..a9071ae39d6c 100644
--- a/locales/pl-PL/discover.json
+++ b/locales/pl-PL/discover.json
@@ -126,6 +126,10 @@
         "title": "Świeżość tematu"
       },
       "range": "Zakres",
+      "reasoning_effort": {
+        "desc": "To ustawienie kontroluje intensywność rozumowania modelu przed wygenerowaniem odpowiedzi. Niska intensywność priorytetowo traktuje szybkość odpowiedzi i oszczędza tokeny, podczas gdy wysoka intensywność zapewnia pełniejsze rozumowanie, ale zużywa więcej tokenów i obniża szybkość odpowiedzi. Wartość domyślna to średnia, co równoważy dokładność rozumowania z szybkością odpowiedzi.",
+        "title": "Intensywność rozumowania"
+      },
       "temperature": {
         "desc": "To ustawienie wpływa na różnorodność odpowiedzi modelu. Niższe wartości prowadzą do bardziej przewidywalnych i typowych odpowiedzi, podczas gdy wyższe wartości zachęcają do bardziej zróżnicowanych i rzadziej spotykanych odpowiedzi. Gdy wartość wynosi 0, model zawsze daje tę samą odpowiedź na dane wejście.",
         "title": "Losowość"
diff --git a/locales/pl-PL/models.json b/locales/pl-PL/models.json
index 538437635335..f2ff42b9236f 100644
--- a/locales/pl-PL/models.json
+++ b/locales/pl-PL/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 to rodzina dużych modeli językowych opracowanych przez Meta, obejmująca pretrenowane i dostosowane do instrukcji warianty o rozmiarach parametrów 8B, 70B i 405B. Model 8B dostosowany do instrukcji został zoptymalizowany do scenariuszy rozmów wielojęzycznych, osiągając doskonałe wyniki w wielu branżowych testach benchmarkowych. Trening modelu wykorzystał ponad 150 bilionów tokenów danych publicznych oraz zastosował techniki takie jak nadzorowane dostrajanie i uczenie przez wzmacnianie z ludzkim feedbackiem, aby zwiększyć użyteczność i bezpieczeństwo modelu. Llama 3.1 wspiera generowanie tekstu i kodu, a data graniczna wiedzy to grudzień 2023 roku."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview to model badawczy opracowany przez zespół Qwen, skoncentrowany na zdolnościach wnioskowania wizualnego, który ma unikalne zalety w zrozumieniu złożonych scenariuszy i rozwiązywaniu wizualnie związanych problemów matematycznych."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview to najnowszy eksperymentalny model badawczy Qwen, skoncentrowany na zwiększeniu zdolności wnioskowania AI. Poprzez eksplorację złożonych mechanizmów, takich jak mieszanie języków i wnioskowanie rekurencyjne, główne zalety obejmują silne zdolności analizy wnioskowania, matematyki i programowania. Jednocześnie występują problemy z przełączaniem języków, cyklami wnioskowania, kwestiami bezpieczeństwa oraz różnicami w innych zdolnościach."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct oferuje wysoką niezawodność w przetwarzaniu poleceń, wspierając różne branże."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 to model wnioskowania napędzany uczeniem przez wzmocnienie (RL), który rozwiązuje problemy z powtarzalnością i czytelnością modelu. Przed RL, DeepSeek-R1 wprowadził dane z zimnego startu, co dodatkowo zoptymalizowało wydajność wnioskowania. W zadaniach matematycznych, kodowania i wnioskowania, osiąga wyniki porównywalne z OpenAI-o1, a dzięki starannie zaprojektowanej metodzie treningowej poprawia ogólne wyniki."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 łączy doskonałe cechy wcześniejszych wersji, wzmacniając zdolności ogólne i kodowania."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 to model językowy z 671 miliardami parametrów, oparty na architekturze mieszanych ekspertów (MoE), wykorzystujący wielogłowicową potencjalną uwagę (MLA) oraz strategię równoważenia obciążenia bez dodatkowych strat, co optymalizuje wydajność wnioskowania i treningu. Dzięki wstępnemu treningowi na 14,8 bilionach wysokiej jakości tokenów oraz nadzorowanemu dostrajaniu i uczeniu przez wzmocnienie, DeepSeek-V3 przewyższa inne modele open source, zbliżając się do wiodących modeli zamkniętych."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B to zaawansowany model przeszkolony do złożonych dialogów."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 to model wizualno-językowy oparty na DeepSeekMoE-27B, wykorzystujący architekturę MoE z rzadką aktywacją, osiągający doskonałe wyniki przy aktywacji jedynie 4,5 miliarda parametrów. Model ten wyróżnia się w wielu zadaniach, takich jak wizualne pytania i odpowiedzi, optyczne rozpoznawanie znaków, zrozumienie dokumentów/tabel/wykresów oraz lokalizacja wizualna."
+  },
   "deepseek-chat": {
     "description": "Nowy otwarty model łączący zdolności ogólne i kodowe, który nie tylko zachowuje ogólne zdolności dialogowe oryginalnego modelu czatu i potężne zdolności przetwarzania kodu modelu Coder, ale także lepiej dostosowuje się do ludzkich preferencji. Ponadto, DeepSeek-V2.5 osiągnął znaczne poprawy w zadaniach pisarskich, przestrzeganiu instrukcji i innych obszarach."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 to nowy model wnioskowania OpenAI, odpowiedni do złożonych zadań wymagających szerokiej wiedzy ogólnej. Model ten ma kontekst 128K i datę graniczną wiedzy z października 2023 roku."
   },
+  "o3-mini": {
+    "description": "o3-mini to nasz najnowszy mały model wnioskowania, który oferuje wysoką inteligencję przy tych samych kosztach i celach opóźnienia co o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba to model językowy Mamba 2 skoncentrowany na generowaniu kodu, oferujący silne wsparcie dla zaawansowanych zadań kodowania i wnioskowania."
   },
diff --git a/locales/pl-PL/setting.json b/locales/pl-PL/setting.json
index fa66a39b0674..372f2f4a5779 100644
--- a/locales/pl-PL/setting.json
+++ b/locales/pl-PL/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Włącz limit jednorazowej odpowiedzi"
     },
+    "enableReasoningEffort": {
+      "title": "Włącz dostosowanie intensywności rozumowania"
+    },
     "frequencyPenalty": {
       "desc": "Im większa wartość, tym większe prawdopodobieństwo zmniejszenia powtarzających się słów",
       "title": "Kara za częstość"
@@ -216,6 +219,15 @@
       "desc": "Im większa wartość, tym większe prawdopodobieństwo rozszerzenia się na nowe tematy",
       "title": "Świeżość tematu"
     },
+    "reasoningEffort": {
+      "desc": "Im wyższa wartość, tym silniejsza zdolność rozumowania, ale może to zwiększyć czas odpowiedzi i zużycie tokenów",
+      "options": {
+        "high": "Wysoki",
+        "low": "Niski",
+        "medium": "Średni"
+      },
+      "title": "Intensywność rozumowania"
+    },
     "temperature": {
       "desc": "Im większa wartość, tym odpowiedzi są bardziej losowe",
       "title": "Losowość",
diff --git a/locales/pt-BR/discover.json b/locales/pt-BR/discover.json
index 4b88bc81c6b2..6f2c6de8923f 100644
--- a/locales/pt-BR/discover.json
+++ b/locales/pt-BR/discover.json
@@ -126,6 +126,10 @@
         "title": "Novidade do Tópico"
       },
       "range": "Faixa",
+      "reasoning_effort": {
+        "desc": "Esta configuração é usada para controlar a intensidade de raciocínio do modelo antes de gerar uma resposta. Intensidade baixa prioriza a velocidade de resposta e economiza Tokens, enquanto intensidade alta oferece um raciocínio mais completo, mas consome mais Tokens e reduz a velocidade de resposta. O valor padrão é médio, equilibrando a precisão do raciocínio com a velocidade de resposta.",
+        "title": "Intensidade de Raciocínio"
+      },
       "temperature": {
         "desc": "Esta configuração afeta a diversidade das respostas do modelo. Valores mais baixos resultam em respostas mais previsíveis e típicas, enquanto valores mais altos incentivam respostas mais variadas e incomuns. Quando o valor é 0, o modelo sempre dá a mesma resposta para uma entrada dada.",
         "title": "Aleatoriedade"
diff --git a/locales/pt-BR/models.json b/locales/pt-BR/models.json
index 7d9e9bc01255..fec3c7f7eab2 100644
--- a/locales/pt-BR/models.json
+++ b/locales/pt-BR/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 é uma família de modelos de linguagem em larga escala multilíngue desenvolvida pela Meta, incluindo variantes pré-treinadas e de ajuste fino para instruções com tamanhos de parâmetros de 8B, 70B e 405B. Este modelo de 8B foi otimizado para cenários de diálogo multilíngue e se destacou em vários benchmarks da indústria. O treinamento do modelo utilizou mais de 150 trilhões de tokens de dados públicos e empregou técnicas como ajuste fino supervisionado e aprendizado por reforço com feedback humano para melhorar a utilidade e segurança do modelo. Llama 3.1 suporta geração de texto e geração de código, com data de corte de conhecimento em dezembro de 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview é um modelo de pesquisa desenvolvido pela equipe Qwen, focado em capacidades de inferência visual, com vantagens únicas na compreensão de cenários complexos e na resolução de problemas matemáticos relacionados à visão."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview é o mais recente modelo de pesquisa experimental da Qwen, focado em melhorar a capacidade de raciocínio da IA. Ao explorar mecanismos complexos como mistura de linguagem e raciocínio recursivo, suas principais vantagens incluem forte capacidade de análise de raciocínio, habilidades matemáticas e de programação. Ao mesmo tempo, existem questões de troca de linguagem, ciclos de raciocínio, considerações de segurança e diferenças em outras capacidades."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct oferece capacidade de processamento de instruções altamente confiável, suportando aplicações em diversos setores."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 é um modelo de inferência impulsionado por aprendizado por reforço (RL), que resolve problemas de repetitividade e legibilidade no modelo. Antes do RL, o DeepSeek-R1 introduziu dados de inicialização a frio, otimizando ainda mais o desempenho da inferência. Ele se compara ao OpenAI-o1 em tarefas matemáticas, de codificação e de inferência, e melhora o desempenho geral por meio de métodos de treinamento cuidadosamente projetados."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 combina as excelentes características das versões anteriores, aprimorando a capacidade geral e de codificação."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 é um modelo de linguagem de especialistas mistos (MoE) com 671 bilhões de parâmetros, utilizando atenção latente multi-cabeça (MLA) e a arquitetura DeepSeekMoE, combinando uma estratégia de balanceamento de carga sem perda auxiliar para otimizar a eficiência de inferência e treinamento. Pré-treinado em 14,8 trilhões de tokens de alta qualidade e ajustado supervisionadamente com aprendizado por reforço, o DeepSeek-V3 supera outros modelos de código aberto em desempenho, aproximando-se de modelos fechados líderes."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B é um modelo avançado treinado para diálogos de alta complexidade."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 é um modelo de linguagem visual baseado no DeepSeekMoE-27B, desenvolvido com uma arquitetura MoE de ativação esparsa, alcançando desempenho excepcional com apenas 4,5 bilhões de parâmetros ativados. Este modelo se destaca em várias tarefas, incluindo perguntas visuais, reconhecimento óptico de caracteres, compreensão de documentos/tabelas/gráficos e localização visual."
+  },
   "deepseek-chat": {
     "description": "Um novo modelo de código aberto que combina capacidades gerais e de codificação, não apenas preservando a capacidade de diálogo geral do modelo Chat original e a poderosa capacidade de processamento de código do modelo Coder, mas também alinhando-se melhor às preferências humanas. Além disso, o DeepSeek-V2.5 também alcançou melhorias significativas em várias áreas, como tarefas de escrita e seguimento de instruções."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 é o novo modelo de raciocínio da OpenAI, adequado para tarefas complexas que exigem amplo conhecimento geral. Este modelo possui um contexto de 128K e uma data limite de conhecimento em outubro de 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini é nosso mais recente modelo de inferência em miniatura, oferecendo alta inteligência com os mesmos custos e metas de latência que o o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba é um modelo de linguagem Mamba 2 focado em geração de código, oferecendo forte suporte para tarefas avançadas de codificação e raciocínio."
   },
diff --git a/locales/pt-BR/setting.json b/locales/pt-BR/setting.json
index 610fa3018807..e3383a27eb4f 100644
--- a/locales/pt-BR/setting.json
+++ b/locales/pt-BR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Ativar limite de resposta única"
     },
+    "enableReasoningEffort": {
+      "title": "Ativar ajuste de intensidade de raciocínio"
+    },
     "frequencyPenalty": {
       "desc": "Quanto maior o valor, maior a probabilidade de reduzir palavras repetidas",
       "title": "Penalidade de frequência"
@@ -216,6 +219,15 @@
       "desc": "Quanto maior o valor, maior a probabilidade de expandir para novos tópicos",
       "title": "Penalidade de novidade do tópico"
     },
+    "reasoningEffort": {
+      "desc": "Quanto maior o valor, mais forte será a capacidade de raciocínio, mas isso pode aumentar o tempo de resposta e o consumo de tokens",
+      "options": {
+        "high": "Alto",
+        "low": "Baixo",
+        "medium": "Médio"
+      },
+      "title": "Intensidade de raciocínio"
+    },
     "temperature": {
       "desc": "Quanto maior o valor, mais aleatória será a resposta",
       "title": "Aleatoriedade",
diff --git a/locales/ru-RU/discover.json b/locales/ru-RU/discover.json
index 1259a910fb9a..9553af416a67 100644
--- a/locales/ru-RU/discover.json
+++ b/locales/ru-RU/discover.json
@@ -126,6 +126,10 @@
         "title": "Свежесть темы"
       },
       "range": "Диапазон",
+      "reasoning_effort": {
+        "desc": "Эта настройка используется для управления интенсивностью размышлений модели перед генерацией ответа. Низкая интенсивность приоритизирует скорость ответа и экономит токены, высокая интенсивность обеспечивает более полное размышление, но потребляет больше токенов и снижает скорость ответа. Значение по умолчанию - среднее, что обеспечивает баланс между точностью размышлений и скоростью ответа.",
+        "title": "Интенсивность размышлений"
+      },
       "temperature": {
         "desc": "Эта настройка влияет на разнообразие ответов модели. Более низкие значения приводят к более предсказуемым и типичным ответам, в то время как более высокие значения поощряют более разнообразные и необычные ответы. Когда значение установлено на 0, модель всегда дает один и тот же ответ на данный ввод.",
         "title": "Случайность"
diff --git a/locales/ru-RU/models.json b/locales/ru-RU/models.json
index a4285893086f..8a86fc503718 100644
--- a/locales/ru-RU/models.json
+++ b/locales/ru-RU/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 — это семейство многоязычных крупных языковых моделей, разработанных Meta, включая предобученные и дообученные на инструкциях варианты с параметрами 8B, 70B и 405B. Эта 8B модель с дообучением на инструкциях оптимизирована для многоязычных диалоговых сценариев и показывает отличные результаты в нескольких отраслевых бенчмарках. Обучение модели использовало более 150 триллионов токенов открытых данных и применяло такие технологии, как контролируемое дообучение и обучение с подкреплением на основе человеческой обратной связи для повышения полезности и безопасности модели. Llama 3.1 поддерживает генерацию текста и кода, с датой окончания знаний в декабре 2023 года."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview — это исследовательская модель, разработанная командой Qwen, сосредоточенная на возможностях визуального вывода, обладающая уникальными преимуществами в понимании сложных сцен и решении визуально связанных математических задач."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview — это последняя экспериментальная исследовательская модель Qwen, сосредоточенная на повышении возможностей вывода ИИ. Исследуя сложные механизмы, такие как смешение языков и рекурсивные выводы, основные преимущества включают мощные аналитические способности, математические и программные навыки. В то же время существуют проблемы с переключением языков, циклом вывода, соображениями безопасности и различиями в других способностях."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct предлагает высокую надежность в обработке команд, поддерживая приложения в различных отраслях."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 — это модель вывода, основанная на методах обучения с подкреплением (RL), которая решает проблемы повторяемости и читаемости модели. Перед применением RL, DeepSeek-R1 вводит данные холодного старта, что дополнительно оптимизирует производительность вывода. Она показывает сопоставимые результаты с OpenAI-o1 в математических, кодовых и задачах вывода, а также улучшает общую эффективность благодаря тщательно разработанным методам обучения."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 объединяет отличительные черты предыдущих версий, улучшая общие и кодировочные способности."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 — это языковая модель смешанных экспертов (MoE) с 6710 миллиардами параметров, использующая многоголовое внимание (MLA) и архитектуру DeepSeekMoE, в сочетании с стратегией балансировки нагрузки без вспомогательных потерь, оптимизирующей эффективность вывода и обучения. После предобучения на 14,8 триллионах высококачественных токенов и последующей супервизионной донастройки и обучения с подкреплением, DeepSeek-V3 превосходит другие открытые модели по производительности, приближаясь к ведущим закрытым моделям."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B — это передовая модель, обученная для высококомплексных диалогов."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 — это модель визуального языка, разработанная на основе DeepSeekMoE-27B, использующая архитектуру MoE с разреженной активацией, которая демонстрирует выдающиеся результаты при активации всего 4,5 миллиарда параметров. Эта модель показывает отличные результаты в различных задачах, таких как визуальные вопросы и ответы, оптическое распознавание символов, понимание документов/таблиц/графиков и визуальная локализация."
+  },
   "deepseek-chat": {
     "description": "Новая открытая модель, объединяющая общие и кодовые возможности, не только сохраняет общие диалоговые способности оригинальной модели Chat и мощные возможности обработки кода модели Coder, но и лучше согласуется с человеческими предпочтениями. Кроме того, DeepSeek-V2.5 значительно улучшила производительность в таких задачах, как написание текстов и следование инструкциям."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 — это новая модель вывода от OpenAI, подходящая для сложных задач, требующих обширных общих знаний. Модель имеет контекст 128K и срок знания до октября 2023 года."
   },
+  "o3-mini": {
+    "description": "o3-mini — это наша последняя компактная модель вывода, обеспечивающая высокий уровень интеллекта при тех же затратах и задержках, что и o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba — это языковая модель Mamba 2, сосредоточенная на генерации кода, обеспечивающая мощную поддержку для сложных задач по коду и выводу."
   },
diff --git a/locales/ru-RU/setting.json b/locales/ru-RU/setting.json
index 4c1760d114ea..4abe96acca0a 100644
--- a/locales/ru-RU/setting.json
+++ b/locales/ru-RU/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Включить ограничение максимального количества токенов"
     },
+    "enableReasoningEffort": {
+      "title": "Включить настройку интенсивности вывода"
+    },
     "frequencyPenalty": {
       "desc": "Чем выше значение, тем меньше вероятность повторения слов",
       "title": "Штраф за повторение"
@@ -216,6 +219,15 @@
       "desc": "Чем выше значение, тем больше вероятность перехода на новые темы",
       "title": "Штраф за однообразие"
     },
+    "reasoningEffort": {
+      "desc": "Чем больше значение, тем сильнее способность вывода, но это может увеличить время отклика и потребление токенов",
+      "options": {
+        "high": "Высокий",
+        "low": "Низкий",
+        "medium": "Средний"
+      },
+      "title": "Интенсивность вывода"
+    },
     "temperature": {
       "desc": "Чем выше значение, тем более непредсказуемым будет ответ",
       "title": "Непредсказуемость",
diff --git a/locales/tr-TR/discover.json b/locales/tr-TR/discover.json
index 0896540a6138..d75758a6c4e2 100644
--- a/locales/tr-TR/discover.json
+++ b/locales/tr-TR/discover.json
@@ -126,6 +126,10 @@
         "title": "Konu Tazeliği"
       },
       "range": "Aralık",
+      "reasoning_effort": {
+        "desc": "Bu ayar, modelin yanıt üretmeden önceki akıl yürütme gücünü kontrol etmek için kullanılır. Düşük güç, yanıt hızını önceliklendirir ve Token tasarrufu sağlar; yüksek güç ise daha kapsamlı bir akıl yürütme sunar, ancak daha fazla Token tüketir ve yanıt hızını düşürür. Varsayılan değer orta seviyedir, akıl yürütme doğruluğu ile yanıt hızı arasında bir denge sağlar.",
+        "title": "Akıl Yürütme Gücü"
+      },
       "temperature": {
         "desc": "Bu ayar, modelin yanıtlarının çeşitliliğini etkiler. Daha düşük değerler daha öngörülebilir ve tipik yanıtlar verirken, daha yüksek değerler daha çeşitli ve nadir yanıtları teşvik eder. Değer 0 olarak ayarlandığında, model belirli bir girdi için her zaman aynı yanıtı verir.",
         "title": "Rastgelelik"
diff --git a/locales/tr-TR/models.json b/locales/tr-TR/models.json
index 487a86cd51b8..db25d5fdf06f 100644
--- a/locales/tr-TR/models.json
+++ b/locales/tr-TR/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1, Meta tarafından geliştirilen çok dilli büyük dil modeli ailesidir ve 8B, 70B ve 405B olmak üzere üç parametre ölçeği ile önceden eğitilmiş ve talimat ince ayar varyantları içermektedir. Bu 8B talimat ince ayar modeli, çok dilli diyalog senaryoları için optimize edilmiştir ve birçok endüstri standart testinde mükemmel performans sergilemektedir. Model, 15 trilyon token'dan fazla açık veriler kullanılarak eğitilmiş ve modelin faydasını ve güvenliğini artırmak için denetimli ince ayar ve insan geri bildirimi pekiştirmeli öğrenme gibi teknikler kullanılmıştır. Llama 3.1, metin üretimi ve kod üretimini desteklemekte olup, bilgi kesim tarihi 2023 Aralık'tır."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview, karmaşık sahne anlayışı ve görsel ile ilgili matematik problemlerini çözme konusundaki benzersiz avantajları ile görsel çıkarım yeteneklerine odaklanan Qwen ekibi tarafından geliştirilen bir araştırma modelidir."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview, Qwen'in en son deneysel araştırma modelidir ve AI akıl yürütme yeteneklerini artırmaya odaklanmaktadır. Dil karışımı, özyinelemeli akıl yürütme gibi karmaşık mekanizmaları keşfederek, güçlü akıl yürütme analizi, matematik ve programlama yetenekleri gibi ana avantajlar sunmaktadır. Bununla birlikte, dil geçiş sorunları, akıl yürütme döngüleri, güvenlik endişeleri ve diğer yetenek farklılıkları gibi zorluklar da bulunmaktadır."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct, yüksek güvenilirlikte talimat işleme yetenekleri sunar ve çok çeşitli endüstri uygulamalarını destekler."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1, modeldeki tekrarlılık ve okunabilirlik sorunlarını çözen, pekiştirmeli öğrenme (RL) destekli bir çıkarım modelidir. RL'den önce, DeepSeek-R1, çıkarım performansını daha da optimize etmek için soğuk başlangıç verileri sunmuştur. Matematik, kod ve çıkarım görevlerinde OpenAI-o1 ile benzer bir performans sergilemekte ve özenle tasarlanmış eğitim yöntemleriyle genel etkisini artırmaktadır."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5, önceki sürümlerin mükemmel özelliklerini bir araya getirir, genel ve kodlama yeteneklerini artırır."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3, 6710 milyar parametreye sahip bir karma uzman (MoE) dil modelidir. Çok başlı potansiyel dikkat (MLA) ve DeepSeekMoE mimarisini kullanarak, yardımcı kayıplar olmadan yük dengeleme stratejileri ile çıkarım ve eğitim verimliliğini optimize etmektedir. 14.8 trilyon yüksek kaliteli token üzerinde önceden eğitilmiş ve denetimli ince ayar ile pekiştirmeli öğrenme ile geliştirilmiştir; DeepSeek-V3, performans açısından diğer açık kaynaklı modellere göre üstünlük sağlamış ve lider kapalı kaynak modellere yaklaşmıştır."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B, yüksek karmaşıklıkta diyaloglar için eğitilmiş gelişmiş bir modeldir."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2, yalnızca 4.5B parametreyi etkinleştirerek olağanüstü performans sergileyen, DeepSeekMoE-27B tabanlı bir karma uzman (MoE) görsel dil modelidir. Bu model, görsel soru yanıtlama, optik karakter tanıma, belge/tablolar/grafikler anlama ve görsel konumlandırma gibi birçok görevde mükemmel sonuçlar elde etmektedir."
+  },
   "deepseek-chat": {
     "description": "Genel ve kod yeteneklerini birleştiren yeni bir açık kaynak modeli, yalnızca mevcut Chat modelinin genel diyalog yeteneklerini ve Coder modelinin güçlü kod işleme yeteneklerini korumakla kalmaz, aynı zamanda insan tercihleri ile daha iyi hizalanmıştır. Ayrıca, DeepSeek-V2.5 yazım görevleri, talimat takibi gibi birçok alanda büyük iyileştirmeler sağlamıştır."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1, OpenAI'nin geniş genel bilgiye ihtiyaç duyan karmaşık görevler için uygun yeni bir akıl yürütme modelidir. Bu model, 128K bağlam ve Ekim 2023 bilgi kesim tarihi ile donatılmıştır."
   },
+  "o3-mini": {
+    "description": "o3-mini, aynı maliyet ve gecikme hedefleriyle yüksek zeka sunan en yeni küçük ölçekli çıkarım modelimizdir."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba, kod üretimine odaklanan Mamba 2 dil modelidir ve ileri düzey kod ve akıl yürütme görevlerine güçlü destek sunar."
   },
diff --git a/locales/tr-TR/setting.json b/locales/tr-TR/setting.json
index 30d8758284f1..4e8095b43673 100644
--- a/locales/tr-TR/setting.json
+++ b/locales/tr-TR/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Max Token Sınırlamasını Etkinleştir"
     },
+    "enableReasoningEffort": {
+      "title": "Akıl yürütme yoğunluğunu ayarla"
+    },
     "frequencyPenalty": {
       "desc": "Değer ne kadar yüksekse, tekrarlayan kelimeleri azaltma olasılığı o kadar yüksektir",
       "title": "Frequency Penalty"
@@ -216,6 +219,15 @@
       "desc": "Değer ne kadar yüksekse, yeni konulara genişleme olasılığı o kadar yüksektir",
       "title": "Presence Penalty"
     },
+    "reasoningEffort": {
+      "desc": "Değer ne kadar yüksekse, akıl yürütme yeteneği o kadar güçlüdür, ancak yanıt süresi ve Token tüketimini artırabilir",
+      "options": {
+        "high": "Yüksek",
+        "low": "Düşük",
+        "medium": "Orta"
+      },
+      "title": "Akıl yürütme yoğunluğu"
+    },
     "temperature": {
       "desc": "Değer ne kadar yüksekse, yanıt o kadar rastgele olur",
       "title": "Randomness",
diff --git a/locales/vi-VN/discover.json b/locales/vi-VN/discover.json
index 804488c247e7..726f4a2bfa47 100644
--- a/locales/vi-VN/discover.json
+++ b/locales/vi-VN/discover.json
@@ -126,6 +126,10 @@
         "title": "Độ mới của chủ đề"
       },
       "range": "Phạm vi",
+      "reasoning_effort": {
+        "desc": "Cài đặt này được sử dụng để kiểm soát mức độ suy luận của mô hình trước khi tạo câu trả lời. Mức độ thấp ưu tiên tốc độ phản hồi và tiết kiệm Token, trong khi mức độ cao cung cấp suy luận đầy đủ hơn nhưng tiêu tốn nhiều Token hơn và làm giảm tốc độ phản hồi. Giá trị mặc định là trung bình, cân bằng giữa độ chính xác của suy luận và tốc độ phản hồi.",
+        "title": "Mức độ suy luận"
+      },
       "temperature": {
         "desc": "Cài đặt này ảnh hưởng đến sự đa dạng trong phản hồi của mô hình. Giá trị thấp hơn dẫn đến phản hồi dễ đoán và điển hình hơn, trong khi giá trị cao hơn khuyến khích phản hồi đa dạng và không thường gặp. Khi giá trị được đặt là 0, mô hình sẽ luôn đưa ra cùng một phản hồi cho đầu vào nhất định.",
         "title": "Ngẫu nhiên"
diff --git a/locales/vi-VN/models.json b/locales/vi-VN/models.json
index a85e0ac7d83c..f177e4b0ecc6 100644
--- a/locales/vi-VN/models.json
+++ b/locales/vi-VN/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 là một phần của gia đình mô hình ngôn ngữ lớn đa ngôn ngữ do Meta phát triển, bao gồm các biến thể tiền huấn luyện và tinh chỉnh theo chỉ dẫn với quy mô tham số 8B, 70B và 405B. Mô hình 8B này được tối ưu hóa cho các tình huống đối thoại đa ngôn ngữ, thể hiện xuất sắc trong nhiều bài kiểm tra chuẩn ngành. Mô hình được đào tạo bằng hơn 15 triệu tỷ tokens từ dữ liệu công khai và sử dụng các kỹ thuật như tinh chỉnh giám sát và học tăng cường phản hồi của con người để nâng cao tính hữu ích và an toàn của mô hình. Llama 3.1 hỗ trợ sinh văn bản và sinh mã, với thời điểm cắt kiến thức là tháng 12 năm 2023."
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview là mô hình nghiên cứu do đội ngũ Qwen phát triển, tập trung vào khả năng suy diễn hình ảnh, với lợi thế độc đáo trong việc hiểu các cảnh phức tạp và giải quyết các vấn đề toán học liên quan đến hình ảnh."
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview là mô hình nghiên cứu thử nghiệm mới nhất của Qwen, tập trung vào việc nâng cao khả năng suy luận của AI. Thông qua việc khám phá các cơ chế phức tạp như trộn ngôn ngữ và suy luận đệ quy, những lợi thế chính bao gồm khả năng phân tích suy luận mạnh mẽ, khả năng toán học và lập trình. Tuy nhiên, cũng có những vấn đề về chuyển đổi ngôn ngữ, vòng lặp suy luận, các vấn đề an toàn và sự khác biệt về các khả năng khác."
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct cung cấp khả năng xử lý chỉ dẫn đáng tin cậy, hỗ trợ nhiều ứng dụng trong ngành."
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 là một mô hình suy diễn được điều khiển bởi học tăng cường (RL), giải quyết các vấn đề về tính lặp lại và khả năng đọc hiểu trong mô hình. Trước khi áp dụng RL, DeepSeek-R1 đã giới thiệu dữ liệu khởi động lạnh, tối ưu hóa thêm hiệu suất suy diễn. Nó thể hiện hiệu suất tương đương với OpenAI-o1 trong các nhiệm vụ toán học, mã và suy diễn, và thông qua phương pháp đào tạo được thiết kế cẩn thận, nâng cao hiệu quả tổng thể."
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 kết hợp các đặc điểm xuất sắc của các phiên bản trước, tăng cường khả năng tổng quát và mã hóa."
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 là một mô hình ngôn ngữ hỗn hợp chuyên gia (MoE) với 6710 tỷ tham số, sử dụng cơ cấu chú ý tiềm ẩn đa đầu (MLA) và DeepSeekMoE, kết hợp với chiến lược cân bằng tải không có tổn thất phụ trợ, tối ưu hóa hiệu suất suy diễn và đào tạo. Thông qua việc được tiền huấn luyện trên 14.8 triệu tỷ token chất lượng cao, và thực hiện tinh chỉnh giám sát và học tăng cường, DeepSeek-V3 vượt trội về hiệu suất so với các mô hình mã nguồn mở khác, gần gũi với các mô hình đóng nguồn hàng đầu."
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B là mô hình tiên tiến được huấn luyện cho các cuộc đối thoại phức tạp."
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 là một mô hình ngôn ngữ hình ảnh hỗn hợp chuyên gia (MoE) phát triển dựa trên DeepSeekMoE-27B, sử dụng cơ cấu MoE với kích hoạt thưa, đạt được hiệu suất xuất sắc chỉ với 4.5B tham số được kích hoạt. Mô hình này thể hiện xuất sắc trong nhiều nhiệm vụ như hỏi đáp hình ảnh, nhận diện ký tự quang học, hiểu tài liệu/bảng/biểu đồ và định vị hình ảnh."
+  },
   "deepseek-chat": {
     "description": "Mô hình mã nguồn mở mới kết hợp khả năng tổng quát và mã, không chỉ giữ lại khả năng đối thoại tổng quát của mô hình Chat ban đầu và khả năng xử lý mã mạnh mẽ của mô hình Coder, mà còn tốt hơn trong việc phù hợp với sở thích của con người. Hơn nữa, DeepSeek-V2.5 cũng đã đạt được sự cải thiện lớn trong nhiều khía cạnh như nhiệm vụ viết, theo dõi chỉ dẫn."
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1 là mô hình suy diễn mới của OpenAI, phù hợp cho các nhiệm vụ phức tạp cần kiến thức tổng quát rộng rãi. Mô hình này có ngữ cảnh 128K và thời điểm cắt kiến thức vào tháng 10 năm 2023."
   },
+  "o3-mini": {
+    "description": "o3-mini là mô hình suy diễn nhỏ gọn mới nhất của chúng tôi, cung cấp trí thông minh cao với chi phí và độ trễ tương tự như o1-mini."
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba là mô hình ngôn ngữ Mamba 2 tập trung vào sinh mã, cung cấp hỗ trợ mạnh mẽ cho các nhiệm vụ mã và suy luận tiên tiến."
   },
diff --git a/locales/vi-VN/setting.json b/locales/vi-VN/setting.json
index ad1b0062e114..797fd0206aa8 100644
--- a/locales/vi-VN/setting.json
+++ b/locales/vi-VN/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "Bật giới hạn phản hồi một lần"
     },
+    "enableReasoningEffort": {
+      "title": "Bật điều chỉnh cường độ suy luận"
+    },
     "frequencyPenalty": {
       "desc": "Giá trị càng cao, càng có khả năng giảm sự lặp lại của từ/cụm từ",
       "title": "Hình phạt tần suất"
@@ -216,6 +219,15 @@
       "desc": "Giá trị càng cao, càng có khả năng mở rộng đến chủ đề mới",
       "title": "Độ mới của chủ đề"
     },
+    "reasoningEffort": {
+      "desc": "Giá trị càng lớn, khả năng suy luận càng mạnh, nhưng có thể làm tăng thời gian phản hồi và tiêu tốn Token",
+      "options": {
+        "high": "Cao",
+        "low": "Thấp",
+        "medium": "Trung bình"
+      },
+      "title": "Cường độ suy luận"
+    },
     "temperature": {
       "desc": "Giá trị càng cao, phản hồi càng ngẫu nhiên",
       "title": "Độ ngẫu nhiên",
diff --git a/locales/zh-CN/discover.json b/locales/zh-CN/discover.json
index 10fd75bdc74c..4221531bf55c 100644
--- a/locales/zh-CN/discover.json
+++ b/locales/zh-CN/discover.json
@@ -126,6 +126,10 @@
         "title": "话题新鲜度"
       },
       "range": "范围",
+      "reasoning_effort": {
+        "desc": "此设置用于控制模型在生成回答前的推理强度。低强度优先响应速度并节省 Token，高强度提供更完整的推理，但会消耗更多 Token 并降低响应速度。默认值为中，平衡推理准确性与响应速度。",
+        "title": "推理强度"
+      },
       "temperature": {
         "desc": "此设置影响模型回应的多样性。较低的值会导致更可预测和典型的回应，而较高的值则鼓励更多样化和不常见的回应。当值设为0时，模型对于给定的输入总是给出相同的回应。",
         "title": "随机性"
diff --git a/locales/zh-CN/models.json b/locales/zh-CN/models.json
index 957edfe27237..8f1f6ee1756d 100644
--- a/locales/zh-CN/models.json
+++ b/locales/zh-CN/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 是由 Meta 开发的多语言大型语言模型家族，包括 8B、70B 和 405B 三种参数规模的预训练和指令微调变体。该 8B 指令微调模型针对多语言对话场景进行了优化，在多项行业基准测试中表现优异。模型训练使用了超过 15 万亿个 tokens 的公开数据，并采用了监督微调和人类反馈强化学习等技术来提升模型的有用性和安全性。Llama 3.1 支持文本生成和代码生成，知识截止日期为 2023 年 12 月"
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview 是由 Qwen 团队开发的专注于视觉推理能力的研究型模型，其在复杂场景理解和解决视觉相关的数学问题方面具有独特优势。"
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ模型是由 Qwen 团队开发的实验性研究模型，专注于增强 AI 推理能力。"
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct 提供高可靠性的指令处理能力，支持多行业应用。"
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 是一款强化学习（RL）驱动的推理模型，解决了模型中的重复性和可读性问题。在 RL 之前，DeepSeek-R1 引入了冷启动数据，进一步优化了推理性能。它在数学、代码和推理任务中与 OpenAI-o1 表现相当，并且通过精心设计的训练方法，提升了整体效果。"
+  },
   "deepseek-ai/DeepSeek-V2.5": {
-    "description": "DeepSeek-V2.5 是 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct 的升级版本，集成了两个先前版本的通用和编码能力。该模型在多个方面进行了优化，包括写作和指令跟随能力，更好地与人类偏好保持一致。DeepSeek-V2.5 在各种评估基准上都取得了显著的提升，如 AlpacaEval 2.0、ArenaHard、AlignBench 和 MT-Bench 等"
+    "description": "DeepSeek-V2.5 是 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct 的升级版本，集成了两个先前版本的通用和编码能力。该模型在多个方面进行了优化，包括写作和指令跟随能力，更好地与人类偏好保持一致。DeepSeek-V2.5 在各种评估基准上都取得了显著的提升，如 AlpacaEval 2.0、ArenaHard、AlignBench 和 MT-Bench 等。"
+  },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 是一款拥有 6710 亿参数的混合专家（MoE）语言模型，采用多头潜在注意力（MLA）和 DeepSeekMoE 架构，结合无辅助损失的负载平衡策略，优化推理和训练效率。通过在 14.8 万亿高质量tokens上预训练，并进行监督微调和强化学习，DeepSeek-V3 在性能上超越其他开源模型，接近领先闭源模型。"
   },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek LLM Chat (67B) 是创新的 AI 模型 提供深度语言理解和互动能力。"
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 是一个基于 DeepSeekMoE-27B 开发的混合专家（MoE）视觉语言模型，采用稀疏激活的 MoE 架构，在仅激活 4.5B 参数的情况下实现了卓越性能。该模型在视觉问答、光学字符识别、文档/表格/图表理解和视觉定位等多个任务中表现优异。"
+  },
   "deepseek-chat": {
     "description": "融合通用与代码能力的全新开源模型, 不仅保留了原有 Chat 模型的通用对话能力和 Coder 模型的强大代码处理能力，还更好地对齐了人类偏好。此外，DeepSeek-V2.5 在写作任务、指令跟随等多个方面也实现了大幅提升。"
   },
@@ -1161,7 +1173,7 @@
     "description": "Llama 3.1 Nemotron 70B 是由 NVIDIA 定制的大型语言模型，旨在提高 LLM 生成的响应对用户查询的帮助程度。该模型在 Arena Hard、AlpacaEval 2 LC 和 GPT-4-Turbo MT-Bench 等基准测试中表现出色，截至 2024 年 10 月 1 日，在所有三个自动对齐基准测试中排名第一。该模型使用 RLHF（特别是 REINFORCE）、Llama-3.1-Nemotron-70B-Reward 和 HelpSteer2-Preference 提示在 Llama-3.1-70B-Instruct 模型基础上进行训练"
   },
   "o1": {
-    "description": "专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深入上下文理解和代理工作流程的应用程序。"
+    "description": "o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。"
   },
   "o1-mini": {
     "description": "o1-mini是一款针对编程、数学和科学应用场景而设计的快速、经济高效的推理模型。该模型具有128K上下文和2023年10月的知识截止日期。"
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1是OpenAI新的推理模型，适用于需要广泛通用知识的复杂任务。该模型具有128K上下文和2023年10月的知识截止日期。"
   },
+  "o3-mini": {
+    "description": "o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。"
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba是专注于代码生成的Mamba 2语言模型，为先进的代码和推理任务提供强力支持。"
   },
diff --git a/locales/zh-CN/setting.json b/locales/zh-CN/setting.json
index d6406f8c1aa1..af8aa1e0375c 100644
--- a/locales/zh-CN/setting.json
+++ b/locales/zh-CN/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "开启单次回复限制"
     },
+    "enableReasoningEffort": {
+      "title": "开启推理强度调整"
+    },
     "frequencyPenalty": {
       "desc": "值越大，越有可能降低重复字词",
       "title": "频率惩罚度"
@@ -216,6 +219,15 @@
       "desc": "值越大，越有可能扩展到新话题",
       "title": "话题新鲜度"
     },
+    "reasoningEffort": {
+      "desc": "值越大，推理能力越强，但可能会增加响应时间和 Token 消耗",
+      "options": {
+        "high": "高",
+        "low": "低",
+        "medium": "中"
+      },
+      "title": "推理强度"
+    },
     "temperature": {
       "desc": "值越大，回复越随机",
       "title": "随机性",
diff --git a/locales/zh-TW/discover.json b/locales/zh-TW/discover.json
index 130f8b9c7a22..cdb81fd51d11 100644
--- a/locales/zh-TW/discover.json
+++ b/locales/zh-TW/discover.json
@@ -126,6 +126,10 @@
         "title": "話題新鮮度"
       },
       "range": "範圍",
+      "reasoning_effort": {
+        "desc": "此設定用於控制模型在生成回答前的推理強度。低強度優先響應速度並節省 Token，高強度提供更完整的推理，但會消耗更多 Token 並降低響應速度。預設值為中，平衡推理準確性與響應速度。",
+        "title": "推理強度"
+      },
       "temperature": {
         "desc": "此設置影響模型回應的多樣性。較低的值會導致更可預測和典型的回應，而較高的值則鼓勵更多樣化和不常見的回應。當值設為0時，模型對於給定的輸入總是給出相同的回應。",
         "title": "隨機性"
diff --git a/locales/zh-TW/models.json b/locales/zh-TW/models.json
index 8dc4b9f7af31..cdb8a7827e91 100644
--- a/locales/zh-TW/models.json
+++ b/locales/zh-TW/models.json
@@ -176,6 +176,9 @@
   "Pro/meta-llama/Meta-Llama-3.1-8B-Instruct": {
     "description": "Meta Llama 3.1 是由 Meta 開發的多語言大型語言模型家族，包括 8B、70B 和 405B 三種參數規模的預訓練和指令微調變體。該 8B 指令微調模型針對多語言對話場景進行了優化，在多項行業基準測試中表現優異。模型訓練使用了超過 15 萬億個 tokens 的公開數據，並採用了監督微調和人類反饋強化學習等技術來提升模型的有用性和安全性。Llama 3.1 支持文本生成和代碼生成，知識截止日期為 2023 年 12 月"
   },
+  "Qwen/QVQ-72B-Preview": {
+    "description": "QVQ-72B-Preview 是由 Qwen 團隊開發的專注於視覺推理能力的研究型模型，其在複雜場景理解和解決視覺相關的數學問題方面具有獨特優勢。"
+  },
   "Qwen/QwQ-32B-Preview": {
     "description": "QwQ-32B-Preview是Qwen 最新的實驗性研究模型，專注於提升AI推理能力。通過探索語言混合、遞歸推理等複雜機制，主要優勢包括強大的推理分析能力、數學和編程能力。與此同時，也存在語言切換問題、推理循環、安全性考量、其他能力方面的差異。"
   },
@@ -530,12 +533,21 @@
   "databricks/dbrx-instruct": {
     "description": "DBRX Instruct 提供高可靠性的指令處理能力，支持多行業應用。"
   },
+  "deepseek-ai/DeepSeek-R1": {
+    "description": "DeepSeek-R1 是一款強化學習（RL）驅動的推理模型，解決了模型中的重複性和可讀性問題。在 RL 之前，DeepSeek-R1 引入了冷啟動數據，進一步優化了推理性能。它在數學、代碼和推理任務中與 OpenAI-o1 表現相當，並且通過精心設計的訓練方法，提升了整體效果。"
+  },
   "deepseek-ai/DeepSeek-V2.5": {
     "description": "DeepSeek V2.5 集合了先前版本的優秀特徵，增強了通用和編碼能力。"
   },
+  "deepseek-ai/DeepSeek-V3": {
+    "description": "DeepSeek-V3 是一款擁有 6710 億參數的混合專家（MoE）語言模型，採用多頭潛在注意力（MLA）和 DeepSeekMoE 架構，結合無輔助損失的負載平衡策略，優化推理和訓練效率。通過在 14.8 萬兆高質量 tokens 上預訓練，並進行監督微調和強化學習，DeepSeek-V3 在性能上超越其他開源模型，接近領先閉源模型。"
+  },
   "deepseek-ai/deepseek-llm-67b-chat": {
     "description": "DeepSeek 67B 是為高複雜性對話訓練的先進模型。"
   },
+  "deepseek-ai/deepseek-vl2": {
+    "description": "DeepSeek-VL2 是一個基於 DeepSeekMoE-27B 開發的混合專家（MoE）視覺語言模型，採用稀疏激活的 MoE 架構，在僅激活 4.5B 參數的情況下實現了卓越性能。該模型在視覺問答、光學字符識別、文檔/表格/圖表理解和視覺定位等多個任務中表現優異。"
+  },
   "deepseek-chat": {
     "description": "融合通用與代碼能力的全新開源模型，不僅保留了原有 Chat 模型的通用對話能力和 Coder 模型的強大代碼處理能力，還更好地對齊了人類偏好。此外，DeepSeek-V2.5 在寫作任務、指令跟隨等多個方面也實現了大幅提升。"
   },
@@ -1169,6 +1181,9 @@
   "o1-preview": {
     "description": "o1是OpenAI新的推理模型，適用於需要廣泛通用知識的複雜任務。該模型具有128K上下文和2023年10月的知識截止日期。"
   },
+  "o3-mini": {
+    "description": "o3-mini 是我們最新的小型推理模型，在與 o1-mini 相同的成本和延遲目標下提供高智能。"
+  },
   "open-codestral-mamba": {
     "description": "Codestral Mamba 是專注於代碼生成的 Mamba 2 語言模型，為先進的代碼和推理任務提供強力支持。"
   },
diff --git a/locales/zh-TW/setting.json b/locales/zh-TW/setting.json
index f683dab3577d..697730b4d3b0 100644
--- a/locales/zh-TW/setting.json
+++ b/locales/zh-TW/setting.json
@@ -200,6 +200,9 @@
     "enableMaxTokens": {
       "title": "啟用單次回覆限制"
     },
+    "enableReasoningEffort": {
+      "title": "開啟推理強度調整"
+    },
     "frequencyPenalty": {
       "desc": "數值越大，越有可能降低重複字詞",
       "title": "頻率懲罰度"
@@ -216,6 +219,15 @@
       "desc": "數值越大，越有可能擴展到新話題",
       "title": "話題新鮮度"
     },
+    "reasoningEffort": {
+      "desc": "值越大，推理能力越強，但可能會增加回應時間和 Token 消耗",
+      "options": {
+        "high": "高",
+        "low": "低",
+        "medium": "中"
+      },
+      "title": "推理強度"
+    },
     "temperature": {
       "desc": "數值越大，回覆越隨機",
       "title": "隨機性",
diff --git a/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx b/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx
index 4d11a87a6e7a..14f1e6708ad0 100644
--- a/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx
+++ b/src/app/(main)/discover/(detail)/model/[...slugs]/features/ParameterList/index.tsx
@@ -5,6 +5,7 @@ import {
   ChartColumnBig,
   Delete,
   FileMinus,
+  Pickaxe,
   LucideIcon,
   MessageSquareText,
   Thermometer,
@@ -83,6 +84,15 @@ const ParameterList = memo<ParameterListProps>(({ data }) => {
       range: data?.meta?.maxOutput ? [0, formatTokenNumber(data.meta.maxOutput)] : undefined,
       type: 'int',
     },
+    {
+      defaultValue: '--',
+      desc: t('models.parameterList.reasoning_effort.desc'),
+      icon: Pickaxe,
+      key: 'reasoning_effort',
+      label: t('models.parameterList.reasoning_effort.title'),
+      range: ['low', 'high'],
+      type: 'string',
+    },
   ];
 
   return (
diff --git a/src/config/aiModels/github.ts b/src/config/aiModels/github.ts
index e68ab31631d5..2e3db2a7209c 100644
--- a/src/config/aiModels/github.ts
+++ b/src/config/aiModels/github.ts
@@ -3,21 +3,20 @@ import { AIChatModelCard } from '@/types/aiModel';
 const githubChatModels: AIChatModelCard[] = [
   {
     abilities: {
-      functionCall: false,
-      vision: true,
+      functionCall: true,
     },
     contextWindowTokens: 200_000,
     description:
-      '专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深入上下文理解和代理工作流程的应用程序。',
-    displayName: 'OpenAI o1',
+      'o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。',
+    displayName: 'OpenAI o3-mini',
     enabled: true,
-    id: 'o1',
+    id: 'o3-mini',
     maxOutput: 100_000,
+    releasedAt: '2025-01-31',
     type: 'chat',
   },
   {
     abilities: {
-      functionCall: false,
       vision: true,
     },
     contextWindowTokens: 128_000,
@@ -30,7 +29,19 @@ const githubChatModels: AIChatModelCard[] = [
   },
   {
     abilities: {
-      functionCall: false,
+      vision: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。',
+    displayName: 'OpenAI o1',
+    enabled: true,
+    id: 'o1',
+    maxOutput: 100_000,
+    type: 'chat',
+  },
+  {
+    abilities: {
       vision: true,
     },
     contextWindowTokens: 128_000,
diff --git a/src/config/aiModels/openai.ts b/src/config/aiModels/openai.ts
index c35bfd65da81..bb8f6869c58a 100644
--- a/src/config/aiModels/openai.ts
+++ b/src/config/aiModels/openai.ts
@@ -8,6 +8,24 @@ import {
 } from '@/types/aiModel';
 
 export const openaiChatModels: AIChatModelCard[] = [
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。',
+    displayName: 'OpenAI o3-mini',
+    enabled: true,
+    id: 'o3-mini',
+    maxOutput: 100_000,
+    pricing: {
+      input: 1.1,
+      output: 4.4,
+    },
+    releasedAt: '2025-01-31',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 128_000,
     description:
@@ -17,12 +35,27 @@ export const openaiChatModels: AIChatModelCard[] = [
     id: 'o1-mini',
     maxOutput: 65_536,
     pricing: {
-      input: 3,
-      output: 12,
+      input: 1.1,
+      output: 4.4,
     },
     releasedAt: '2024-09-12',
     type: 'chat',
   },
+  {
+    contextWindowTokens: 200_000,
+    description:
+      'o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。',
+    displayName: 'OpenAI o1',
+    enabled: true,
+    id: 'o1',
+    maxOutput: 100_000,
+    pricing: {
+      input: 15,
+      output: 60,
+    },
+    releasedAt: '2024-12-17',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 128_000,
     description:
diff --git a/src/features/AgentSetting/AgentModal/index.tsx b/src/features/AgentSetting/AgentModal/index.tsx
index f8af3d1b88d7..a5c935554f6c 100644
--- a/src/features/AgentSetting/AgentModal/index.tsx
+++ b/src/features/AgentSetting/AgentModal/index.tsx
@@ -1,7 +1,7 @@
 'use client';
 
 import { Form, ItemGroup, SliderWithInput } from '@lobehub/ui';
-import { Switch } from 'antd';
+import { Select, Switch } from 'antd';
 import { memo } from 'react';
 import { useTranslation } from 'react-i18next';
 
@@ -17,9 +17,9 @@ const AgentModal = memo(() => {
   const { t } = useTranslation('setting');
   const [form] = Form.useForm();
 
-  const [enableMaxTokens, updateConfig] = useStore((s) => {
+  const [enableMaxTokens, enableReasoningEffort, updateConfig] = useStore((s) => {
     const config = selectors.chatConfig(s);
-    return [config.enableMaxTokens, s.setAgentConfig];
+    return [config.enableMaxTokens, config.enableReasoningEffort, s.setAgentConfig];
   });
 
   const providerName = useProviderName(useStore((s) => s.config.provider) as string);
@@ -79,6 +79,30 @@ const AgentModal = memo(() => {
         name: ['params', 'max_tokens'],
         tag: 'max_tokens',
       },
+      {
+        children: <Switch />,
+        label: t('settingModel.enableReasoningEffort.title'),
+        minWidth: undefined,
+        name: ['chatConfig', 'enableReasoningEffort'],
+        valuePropName: 'checked',
+      },
+      {
+        children: (
+          <Select
+            defaultValue='medium'
+            options={[
+              { label: t('settingModel.reasoningEffort.options.low'), value: 'low' },
+              { label: t('settingModel.reasoningEffort.options.medium'), value: 'medium' },
+              { label: t('settingModel.reasoningEffort.options.high'), value: 'high' },
+            ]}
+          />
+        ),
+        desc: t('settingModel.reasoningEffort.desc'),
+        hidden: !enableReasoningEffort,
+        label: t('settingModel.reasoningEffort.title'),
+        name: ['params', 'reasoning_effort'],
+        tag: 'reasoning_effort',
+      },
     ],
     title: t('settingModel.title'),
   };
diff --git a/src/libs/agent-runtime/github/index.ts b/src/libs/agent-runtime/github/index.ts
index 7081a73043ef..0e55d1392c9d 100644
--- a/src/libs/agent-runtime/github/index.ts
+++ b/src/libs/agent-runtime/github/index.ts
@@ -2,7 +2,7 @@ import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
 import type { ChatModelCard } from '@/types/llm';
 
 import { AgentRuntimeErrorType } from '../error';
-import { o1Models, pruneO1Payload } from '../openai';
+import { pruneReasoningPayload, reasoningModels } from '../openai';
 import { ModelProvider } from '../types';
 import {
   CHAT_MODELS_BLOCK_LIST,
@@ -37,8 +37,8 @@ export const LobeGithubAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (o1Models.has(model)) {
-        return { ...pruneO1Payload(payload), stream: false } as any;
+      if (reasoningModels.has(model)) {
+        return { ...pruneReasoningPayload(payload), stream: false } as any;
       }
 
       return { ...payload, stream: payload.stream ?? true };
diff --git a/src/libs/agent-runtime/openai/index.ts b/src/libs/agent-runtime/openai/index.ts
index 74b7e434976a..8bed6c0702d8 100644
--- a/src/libs/agent-runtime/openai/index.ts
+++ b/src/libs/agent-runtime/openai/index.ts
@@ -2,21 +2,23 @@ import { ChatStreamPayload, ModelProvider, OpenAIChatMessage } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 // TODO: 临时写法，后续要重构成 model card 展示配置
-export const o1Models = new Set([
+export const reasoningModels = new Set([
   'o1-preview',
   'o1-preview-2024-09-12',
   'o1-mini',
   'o1-mini-2024-09-12',
   'o1',
   'o1-2024-12-17',
+  'o3-mini',
+  'o3-mini-2025-01-31',
 ]);
 
-export const pruneO1Payload = (payload: ChatStreamPayload) => ({
+export const pruneReasoningPayload = (payload: ChatStreamPayload) => ({
   ...payload,
   frequency_penalty: 0,
   messages: payload.messages.map((message: OpenAIChatMessage) => ({
     ...message,
-    role: message.role === 'system' ? 'user' : message.role,
+    role: message.role === 'system' ? 'developer' : message.role,
   })),
   presence_penalty: 0,
   temperature: 1,
@@ -29,8 +31,8 @@ export const LobeOpenAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (o1Models.has(model)) {
-        return pruneO1Payload(payload) as any;
+      if (reasoningModels.has(model)) {
+        return pruneReasoningPayload(payload) as any;
       }
 
       return { ...payload, stream: payload.stream ?? true };
diff --git a/src/locales/default/discover.ts b/src/locales/default/discover.ts
index 82907c86c58d..7effa14326e7 100644
--- a/src/locales/default/discover.ts
+++ b/src/locales/default/discover.ts
@@ -127,6 +127,10 @@ export default {
         title: '话题新鲜度',
       },
       range: '范围',
+      reasoning_effort: {
+        desc: '此设置用于控制模型在生成回答前的推理强度。低强度优先响应速度并节省 Token，高强度提供更完整的推理，但会消耗更多 Token 并降低响应速度。默认值为中，平衡推理准确性与响应速度。',
+        title: '推理强度',
+      },
       temperature: {
         desc: '此设置影响模型回应的多样性。较低的值会导致更可预测和典型的回应，而较高的值则鼓励更多样化和不常见的回应。当值设为0时，模型对于给定的输入总是给出相同的回应。',
         title: '随机性',
diff --git a/src/locales/default/setting.ts b/src/locales/default/setting.ts
index d20460052773..4bf814430fbc 100644
--- a/src/locales/default/setting.ts
+++ b/src/locales/default/setting.ts
@@ -202,6 +202,9 @@ export default {
     enableMaxTokens: {
       title: '开启单次回复限制',
     },
+    enableReasoningEffort: {
+      title: '开启推理强度调整',
+    },
     frequencyPenalty: {
       desc: '值越大，越有可能降低重复字词',
       title: '频率惩罚度',
@@ -218,6 +221,15 @@ export default {
       desc: '值越大，越有可能扩展到新话题',
       title: '话题新鲜度',
     },
+    reasoningEffort: {
+      desc: '值越大，推理能力越强，但可能会增加响应时间和 Token 消耗',
+      options: {
+        high: '高',
+        low: '低',
+        medium: '中',
+      },
+      title: '推理强度',
+    },
     temperature: {
       desc: '值越大，回复越随机',
       title: '随机性',
diff --git a/src/store/chat/slices/aiChat/actions/generateAIChat.ts b/src/store/chat/slices/aiChat/actions/generateAIChat.ts
index 77cd04fca49a..fff37785c323 100644
--- a/src/store/chat/slices/aiChat/actions/generateAIChat.ts
+++ b/src/store/chat/slices/aiChat/actions/generateAIChat.ts
@@ -420,6 +420,11 @@ export const generateAIChat: StateCreator<
       ? agentConfig.params.max_tokens
       : undefined;
 
+    // 5. handle reasoning_effort
+    agentConfig.params.reasoning_effort = chatConfig.enableReasoningEffort
+      ? agentConfig.params.reasoning_effort
+      : undefined;
+
     let isFunctionCall = false;
     let msgTraceId: string | undefined;
     let output = '';
diff --git a/src/types/agent/index.ts b/src/types/agent/index.ts
index ee6453b295e8..bcd426b4c85d 100644
--- a/src/types/agent/index.ts
+++ b/src/types/agent/index.ts
@@ -68,6 +68,11 @@ export interface LobeAgentChatConfig {
   enableHistoryCount?: boolean;
   enableMaxTokens?: boolean;
 
+  /**
+   * 自定义推理强度
+   */
+  enableReasoningEffort?: boolean;
+
   /**
    * 历史消息条数
    */
@@ -82,6 +87,7 @@ export const AgentChatConfigSchema = z.object({
   enableCompressHistory: z.boolean().optional(),
   enableHistoryCount: z.boolean().optional(),
   enableMaxTokens: z.boolean().optional(),
+  enableReasoningEffort: z.boolean().optional(),
   historyCount: z.number().optional(),
 });
 
diff --git a/src/types/llm.ts b/src/types/llm.ts
index 7230e77a5ed9..bfde6dd57cf5 100644
--- a/src/types/llm.ts
+++ b/src/types/llm.ts
@@ -159,6 +159,11 @@ export interface LLMParams {
    * 生成文本的随机度量，用于控制文本的创造性和多样性
    * @default 1
    */
+  reasoning_effort?: string;
+  /**
+   * 控制模型推理能力
+   * @default medium
+   */
   temperature?: number;
   /**
    * 控制生成文本中最高概率的单个 token
