diff --git a/src/config/aiModels/github.ts b/src/config/aiModels/github.ts
index e68ab31631d5..2e3db2a7209c 100644
--- a/src/config/aiModels/github.ts
+++ b/src/config/aiModels/github.ts
@@ -3,21 +3,20 @@ import { AIChatModelCard } from '@/types/aiModel';
 const githubChatModels: AIChatModelCard[] = [
   {
     abilities: {
-      functionCall: false,
-      vision: true,
+      functionCall: true,
     },
     contextWindowTokens: 200_000,
     description:
-      '专注于高级推理和解决复杂问题，包括数学和科学任务。非常适合需要深入上下文理解和代理工作流程的应用程序。',
-    displayName: 'OpenAI o1',
+      'o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。',
+    displayName: 'OpenAI o3-mini',
     enabled: true,
-    id: 'o1',
+    id: 'o3-mini',
     maxOutput: 100_000,
+    releasedAt: '2025-01-31',
     type: 'chat',
   },
   {
     abilities: {
-      functionCall: false,
       vision: true,
     },
     contextWindowTokens: 128_000,
@@ -30,7 +29,19 @@ const githubChatModels: AIChatModelCard[] = [
   },
   {
     abilities: {
-      functionCall: false,
+      vision: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。',
+    displayName: 'OpenAI o1',
+    enabled: true,
+    id: 'o1',
+    maxOutput: 100_000,
+    type: 'chat',
+  },
+  {
+    abilities: {
       vision: true,
     },
     contextWindowTokens: 128_000,
diff --git a/src/config/aiModels/openai.ts b/src/config/aiModels/openai.ts
index c35bfd65da81..bb8f6869c58a 100644
--- a/src/config/aiModels/openai.ts
+++ b/src/config/aiModels/openai.ts
@@ -8,6 +8,24 @@ import {
 } from '@/types/aiModel';
 
 export const openaiChatModels: AIChatModelCard[] = [
+  {
+    abilities: {
+      functionCall: true,
+    },
+    contextWindowTokens: 200_000,
+    description:
+      'o3-mini 是我们最新的小型推理模型，在与 o1-mini 相同的成本和延迟目标下提供高智能。',
+    displayName: 'OpenAI o3-mini',
+    enabled: true,
+    id: 'o3-mini',
+    maxOutput: 100_000,
+    pricing: {
+      input: 1.1,
+      output: 4.4,
+    },
+    releasedAt: '2025-01-31',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 128_000,
     description:
@@ -17,12 +35,27 @@ export const openaiChatModels: AIChatModelCard[] = [
     id: 'o1-mini',
     maxOutput: 65_536,
     pricing: {
-      input: 3,
-      output: 12,
+      input: 1.1,
+      output: 4.4,
     },
     releasedAt: '2024-09-12',
     type: 'chat',
   },
+  {
+    contextWindowTokens: 200_000,
+    description:
+      'o1是OpenAI新的推理模型，支持图文输入并输出文本，适用于需要广泛通用知识的复杂任务。该模型具有200K上下文和2023年10月的知识截止日期。',
+    displayName: 'OpenAI o1',
+    enabled: true,
+    id: 'o1',
+    maxOutput: 100_000,
+    pricing: {
+      input: 15,
+      output: 60,
+    },
+    releasedAt: '2024-12-17',
+    type: 'chat',
+  },
   {
     contextWindowTokens: 128_000,
     description:
diff --git a/src/features/AgentSetting/AgentModal/index.tsx b/src/features/AgentSetting/AgentModal/index.tsx
index f8af3d1b88d7..3994e2f07409 100644
--- a/src/features/AgentSetting/AgentModal/index.tsx
+++ b/src/features/AgentSetting/AgentModal/index.tsx
@@ -1,7 +1,7 @@
 'use client';
 
 import { Form, ItemGroup, SliderWithInput } from '@lobehub/ui';
-import { Switch } from 'antd';
+import { Select, Switch } from 'antd';
 import { memo } from 'react';
 import { useTranslation } from 'react-i18next';
 
@@ -63,6 +63,21 @@ const AgentModal = memo(() => {
         name: ['params', 'frequency_penalty'],
         tag: 'frequency_penalty',
       },
+      {
+        children: (
+          <Select
+            options={[
+              { label: t('settingModel.reasoningEffort.options.low'), value: 'low' },
+              { label: t('settingModel.reasoningEffort.options.medium'), value: 'medium' },
+              { label: t('settingModel.reasoningEffort.options.high'), value: 'high' },
+            ]}
+          />
+        ),
+        desc: t('settingModel.reasoningEffort.desc'),
+        label: t('settingModel.reasoningEffort.title'),
+        name: ['params', 'reasoning_effort'],
+        tag: 'reasoning_effort',
+      },
       {
         children: <Switch />,
         label: t('settingModel.enableMaxTokens.title'),
diff --git a/src/libs/agent-runtime/github/index.ts b/src/libs/agent-runtime/github/index.ts
index 7081a73043ef..0e55d1392c9d 100644
--- a/src/libs/agent-runtime/github/index.ts
+++ b/src/libs/agent-runtime/github/index.ts
@@ -2,7 +2,7 @@ import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
 import type { ChatModelCard } from '@/types/llm';
 
 import { AgentRuntimeErrorType } from '../error';
-import { o1Models, pruneO1Payload } from '../openai';
+import { pruneReasoningPayload, reasoningModels } from '../openai';
 import { ModelProvider } from '../types';
 import {
   CHAT_MODELS_BLOCK_LIST,
@@ -37,8 +37,8 @@ export const LobeGithubAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (o1Models.has(model)) {
-        return { ...pruneO1Payload(payload), stream: false } as any;
+      if (reasoningModels.has(model)) {
+        return { ...pruneReasoningPayload(payload), stream: false } as any;
       }
 
       return { ...payload, stream: payload.stream ?? true };
diff --git a/src/libs/agent-runtime/openai/index.ts b/src/libs/agent-runtime/openai/index.ts
index 74b7e434976a..ec55401042d8 100644
--- a/src/libs/agent-runtime/openai/index.ts
+++ b/src/libs/agent-runtime/openai/index.ts
@@ -2,21 +2,23 @@ import { ChatStreamPayload, ModelProvider, OpenAIChatMessage } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 // TODO: 临时写法，后续要重构成 model card 展示配置
-export const o1Models = new Set([
+export const reasoningModels = new Set([
   'o1-preview',
   'o1-preview-2024-09-12',
   'o1-mini',
   'o1-mini-2024-09-12',
   'o1',
   'o1-2024-12-17',
+  'o3-mini',
+  'o3-mini-2025-01-31',
 ]);
 
-export const pruneO1Payload = (payload: ChatStreamPayload) => ({
+export const pruneReasoningPayload = (payload: ChatStreamPayload) => ({
   ...payload,
   frequency_penalty: 0,
   messages: payload.messages.map((message: OpenAIChatMessage) => ({
     ...message,
-    role: message.role === 'system' ? 'user' : message.role,
+    role: message.role === 'system' ? 'developer' : message.role,
   })),
   presence_penalty: 0,
   temperature: 1,
@@ -29,8 +31,10 @@ export const LobeOpenAI = LobeOpenAICompatibleFactory({
     handlePayload: (payload) => {
       const { model } = payload;
 
-      if (o1Models.has(model)) {
-        return pruneO1Payload(payload) as any;
+      console.log(payload)
+
+      if (reasoningModels.has(model)) {
+        return pruneReasoningPayload(payload) as any;
       }
 
       return { ...payload, stream: payload.stream ?? true };
diff --git a/src/locales/default/discover.ts b/src/locales/default/discover.ts
index 82907c86c58d..c46a81a7da6b 100644
--- a/src/locales/default/discover.ts
+++ b/src/locales/default/discover.ts
@@ -127,6 +127,10 @@ export default {
         title: '话题新鲜度',
       },
       range: '范围',
+      reasoning_effort: {
+        desc: '此设置控制推理的深入程度。较高的值会增加推理的复杂性，可能导致更详细的回答，但也会增加响应时间和 Token 消耗。较低的值会减少推理深度，提高响应速度。',
+        title: '推理强度',
+      },
       temperature: {
         desc: '此设置影响模型回应的多样性。较低的值会导致更可预测和典型的回应，而较高的值则鼓励更多样化和不常见的回应。当值设为0时，模型对于给定的输入总是给出相同的回应。',
         title: '随机性',
diff --git a/src/locales/default/setting.ts b/src/locales/default/setting.ts
index d20460052773..df390bbf8867 100644
--- a/src/locales/default/setting.ts
+++ b/src/locales/default/setting.ts
@@ -218,6 +218,15 @@ export default {
       desc: '值越大，越有可能扩展到新话题',
       title: '话题新鲜度',
     },
+    reasoningEffort: {
+      desc: '值越大，推理能力越强，但可能会增加响应时间和 Token 消耗',
+      options: {
+        high: '高',
+        low: '低',
+        medium: '中',
+      },
+      title: '推理强度',
+    },
     temperature: {
       desc: '值越大，回复越随机',
       title: '随机性',
