diff --git a/src/config/modelProviders/bedrock.ts b/src/config/modelProviders/bedrock.ts
index 38e8a0a02fd4..e7375dabb04e 100644
--- a/src/config/modelProviders/bedrock.ts
+++ b/src/config/modelProviders/bedrock.ts
@@ -191,15 +191,13 @@ const Bedrock: ModelProviderCard = {
       tokens: 4000,
     },
 */
-/*
-    // TODO: Not support for now
     {
       description: 'The latest Foundation Model from AI21 Labs, Jamba-Instruct offers an impressive 256K context window and delivers the best value per price on core text generation, summarization, and question answering tasks for the enterprise.',
       displayName: 'Jamba-Instruct',
+      enabled: true,
       id: 'ai21.jamba-instruct-v1:0',
       tokens: 256_000,
     },
-*/
 /*
     // Cohere Command (Text) and AI21 Labs Jurassic-2 (Text) don't support chat with the Converse API
     {
diff --git a/src/libs/agent-runtime/bedrock/index.ts b/src/libs/agent-runtime/bedrock/index.ts
index c3d5d94cd59e..f508b1ffae48 100644
--- a/src/libs/agent-runtime/bedrock/index.ts
+++ b/src/libs/agent-runtime/bedrock/index.ts
@@ -12,6 +12,7 @@ import { AgentRuntimeError } from '../utils/createError';
 import { debugStream } from '../utils/debugStream';
 import { StreamingResponse } from '../utils/response';
 import {
+  AWSBedrockAi21Stream,
   AWSBedrockClaudeStream,
   AWSBedrockLlamaStream,
   createBedrockStream,
@@ -44,11 +45,64 @@ export class LobeBedrockAI implements LobeRuntimeAI {
   }
 
   async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
+    if (payload.model.startsWith('ai21')) return this.invokeAi21Model(payload, options);
+
     if (payload.model.startsWith('meta')) return this.invokeLlamaModel(payload, options);
 
     return this.invokeClaudeModel(payload, options);
   }
 
+  private invokeAi21Model = async (
+    payload: ChatStreamPayload,
+    options?: ChatCompetitionOptions,
+  ): Promise<Response> => {
+    const { frequency_penalty, max_tokens, messages, model, presence_penalty, temperature, top_p } = payload;
+    const command = new InvokeModelWithResponseStreamCommand({
+      accept: 'application/json',
+      body: JSON.stringify({
+        frequency_penalty: frequency_penalty,
+        max_tokens: max_tokens || 4096,
+        messages: messages,
+        presence_penalty: presence_penalty,
+        temperature: temperature,
+        top_p: top_p,
+      }),
+      contentType: 'application/json',
+      modelId: model,
+    });
+
+    try {
+      // Ask Claude for a streaming chat completion given the prompt
+      const res = await this.client.send(command);
+
+      const stream = createBedrockStream(res);
+
+      const [prod, debug] = stream.tee();
+
+      if (process.env.DEBUG_BEDROCK_CHAT_COMPLETION === '1') {
+        debugStream(debug).catch(console.error);
+      }
+      // Respond with the stream
+      return StreamingResponse(AWSBedrockAi21Stream(prod, options?.callback), {
+        headers: options?.headers,
+      });
+    } catch (e) {
+      const err = e as Error & { $metadata: any };
+
+      throw AgentRuntimeError.chat({
+        error: {
+          body: err.$metadata,
+          message: err.message,
+          region: this.region,
+          type: err.name,
+        },
+        errorType: AgentRuntimeErrorType.ProviderBizError,
+        provider: ModelProvider.Bedrock,
+        region: this.region,
+      });
+    }
+  };
+
   private invokeClaudeModel = async (
     payload: ChatStreamPayload,
     options?: ChatCompetitionOptions,
diff --git a/src/libs/agent-runtime/utils/streams/bedrock/ai21.ts b/src/libs/agent-runtime/utils/streams/bedrock/ai21.ts
new file mode 100644
index 000000000000..684e609a7595
--- /dev/null
+++ b/src/libs/agent-runtime/utils/streams/bedrock/ai21.ts
@@ -0,0 +1,83 @@
+import { InvokeModelWithResponseStreamResponse } from '@aws-sdk/client-bedrock-runtime';
+
+import { nanoid } from '@/utils/uuid';
+
+import { ChatStreamCallbacks } from '../../../types';
+import {
+  StreamProtocolChunk,
+  StreamStack,
+  createCallbacksTransformer,
+  createSSEProtocolTransformer,
+} from '../protocol';
+import { createBedrockStream } from './common';
+
+interface AmazonBedrockInvocationMetrics {
+  firstByteLatency: number;
+  inputTokenCount: number;
+  invocationLatency: number;
+  outputTokenCount: number;
+}
+
+// https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jamba.html
+// ai21_chunk: {"id":"chat-ae86a1e555f04e5cbddb86cc6a98ce5e","choices":[{"index":0,"delta":{"content":"?"},"finish_reason":"stop","stop_reason":"<|eom|>"}],"usage":{"prompt_tokens":144,"total_tokens":158,"completion_tokens":14},"meta":{"requestDurationMillis":146}},"amazon-bedrock-invocationMetrics":{"inputTokenCount":63,"outputTokenCount":263,"invocationLatency":5330,"firstByteLatency":122}}
+interface BedrockAi21StreamChunk {
+  'amazon-bedrock-invocationMetrics'?: AmazonBedrockInvocationMetrics;
+  'choices': {
+    'delta': {
+      'content': string;
+    };
+    'finish_reason'?: null | 'stop' | string;
+    'index'?: number;
+    'stop_reason'?: null | string;
+  }[];
+  'id'?: string;
+  'meta'?: {
+    'requestDurationMillis': number;
+  };
+  'usage'?: {
+    'completion_tokens': number;
+    'prompt_tokens': number;
+    'total_tokens': number;
+  };
+}
+
+export const transformAi21Stream = (
+  chunk: BedrockAi21StreamChunk,
+  stack: StreamStack,
+): StreamProtocolChunk => {
+  // remove 'amazon-bedrock-invocationMetrics' from chunk
+  delete chunk['amazon-bedrock-invocationMetrics'];
+
+  if (!chunk.choices || chunk.choices.length === 0) {
+    return { data: chunk, id: stack.id, type: 'data' };
+  }
+
+  const item = chunk.choices[0];
+
+  if (typeof item.delta?.content === 'string') {
+    return { data: item.delta.content, id: stack.id, type: 'text' };
+  }
+
+  if (item.finish_reason) {
+    return { data: item.finish_reason, id: stack.id, type: 'stop' };
+  }
+
+  return {
+    data: { delta: item.delta, id: stack.id, index: item.index },
+    id: stack.id,
+    type: 'data',
+  };
+};
+
+export const AWSBedrockAi21Stream = (
+  res: InvokeModelWithResponseStreamResponse | ReadableStream,
+  cb?: ChatStreamCallbacks,
+): ReadableStream<string> => {
+  const streamStack: StreamStack = { id: 'chat_' + nanoid() };
+
+  const stream = res instanceof ReadableStream ? res : createBedrockStream(res);
+
+  return stream
+    .pipeThrough(createSSEProtocolTransformer(transformAi21Stream, streamStack))
+    .pipeThrough(createCallbacksTransformer(cb));
+};
diff --git a/src/libs/agent-runtime/utils/streams/bedrock/index.ts b/src/libs/agent-runtime/utils/streams/bedrock/index.ts
index a25c3d7a6b43..f9e0fc316717 100644
--- a/src/libs/agent-runtime/utils/streams/bedrock/index.ts
+++ b/src/libs/agent-runtime/utils/streams/bedrock/index.ts
@@ -1,3 +1,4 @@
+export * from './ai21';
 export * from './claude';
 export * from './common';
 export * from './llama';

diff --git a/src/config/modelProviders/bedrock.ts b/src/config/modelProviders/bedrock.ts
index 38e8a0a02fd4..6da505b6c050 100644
--- a/src/config/modelProviders/bedrock.ts
+++ b/src/config/modelProviders/bedrock.ts
@@ -133,6 +133,7 @@ const Bedrock: ModelProviderCard = {
       id: 'mistral.mixtral-8x7b-instruct-v0:1',
       tokens: 32_000,
     },
+*/
     {
       description: 'Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation. It provides outstanding performance at a cost-effective price point.',
       displayName: 'Mistral Small',
@@ -156,7 +157,6 @@ const Bedrock: ModelProviderCard = {
       id: 'mistral.mistral-large-2402-v1:0',
       tokens: 32_000,
     },
-*/
 /*
     // TODO: Not support for now
     {
diff --git a/src/libs/agent-runtime/bedrock/index.ts b/src/libs/agent-runtime/bedrock/index.ts
index c3d5d94cd59e..1a3622591f2a 100644
--- a/src/libs/agent-runtime/bedrock/index.ts
+++ b/src/libs/agent-runtime/bedrock/index.ts
@@ -14,6 +14,7 @@ import { StreamingResponse } from '../utils/response';
 import {
   AWSBedrockClaudeStream,
   AWSBedrockLlamaStream,
+  AWSBedrockMistralStream,
   createBedrockStream,
 } from '../utils/streams';
 
@@ -46,6 +47,8 @@ export class LobeBedrockAI implements LobeRuntimeAI {
   async chat(payload: ChatStreamPayload, options?: ChatCompetitionOptions) {
     if (payload.model.startsWith('meta')) return this.invokeLlamaModel(payload, options);
 
+    if (payload.model.startsWith('mistral')) return this.invokeMistralModel(payload, options);
+
     return this.invokeClaudeModel(payload, options);
   }
 
@@ -150,6 +153,57 @@ export class LobeBedrockAI implements LobeRuntimeAI {
       });
     }
   };
+
+  private invokeMistralModel = async (
+    payload: ChatStreamPayload,
+    options?: ChatCompetitionOptions,
+  ): Promise<Response> => {
+    const { max_tokens, messages, model, temperature, tool_choice, tools, top_p } = payload;
+    const command = new InvokeModelWithResponseStreamCommand({
+      accept: 'application/json',
+      body: JSON.stringify({
+        max_tokens: max_tokens || 4096,
+        messages: messages,
+        temperature: temperature,
+        tool_choice: tool_choice,
+        tools: tools,
+        top_p: top_p,
+      }),
+      contentType: 'application/json',
+      modelId: model,
+    });
+
+    try {
+      // Ask Claude for a streaming chat completion given the prompt
+      const res = await this.client.send(command);
+
+      const stream = createBedrockStream(res);
+
+      const [prod, debug] = stream.tee();
+
+      if (process.env.DEBUG_BEDROCK_CHAT_COMPLETION === '1') {
+        debugStream(debug).catch(console.error);
+      }
+      // Respond with the stream
+      return StreamingResponse(AWSBedrockMistralStream(prod, options?.callback), {
+        headers: options?.headers,
+      });
+    } catch (e) {
+      const err = e as Error & { $metadata: any };
+
+      throw AgentRuntimeError.chat({
+        error: {
+          body: err.$metadata,
+          message: err.message,
+          region: this.region,
+          type: err.name,
+        },
+        errorType: AgentRuntimeErrorType.ProviderBizError,
+        provider: ModelProvider.Bedrock,
+        region: this.region,
+      });
+    }
+  };
 }
 
 export default LobeBedrockAI;
diff --git a/src/libs/agent-runtime/utils/streams/bedrock/index.ts b/src/libs/agent-runtime/utils/streams/bedrock/index.ts
index a25c3d7a6b43..5a1fcb2f0b57 100644
--- a/src/libs/agent-runtime/utils/streams/bedrock/index.ts
+++ b/src/libs/agent-runtime/utils/streams/bedrock/index.ts
@@ -1,3 +1,4 @@
 export * from './claude';
 export * from './common';
 export * from './llama';
+export * from './mistral';
diff --git a/src/libs/agent-runtime/utils/streams/bedrock/mistral.ts b/src/libs/agent-runtime/utils/streams/bedrock/mistral.ts
new file mode 100644
index 000000000000..ca7b6a394c14
--- /dev/null
+++ b/src/libs/agent-runtime/utils/streams/bedrock/mistral.ts
@@ -0,0 +1,104 @@
+import { InvokeModelWithResponseStreamResponse } from '@aws-sdk/client-bedrock-runtime';
+
+import { nanoid } from '@/utils/uuid';
+
+import { ChatStreamCallbacks } from '../../../types';
+import {
+  StreamProtocolChunk,
+  StreamProtocolToolCallChunk,
+  StreamStack,
+  StreamToolCallChunkData,
+  createCallbacksTransformer,
+  createSSEProtocolTransformer,
+  generateToolCallId,
+} from '../protocol';
+import { createBedrockStream } from './common';
+
+interface AmazonBedrockInvocationMetrics {
+  firstByteLatency: number;
+  inputTokenCount: number;
+  invocationLatency: number;
+  outputTokenCount: number;
+}
+
+// https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral-chat-completion.html
+// mistral_chunk: {"choices":[{"index":0,"message":{"role":"assistant","content":""},"stop_reason":"stop"}],"amazon-bedrock-invocationMetrics":{"inputTokenCount":63,"outputTokenCount":263,"invocationLatency":5330,"firstByteLatency":122}}
+interface BedrockMistralStreamChunk {
+  'amazon-bedrock-invocationMetrics'?: AmazonBedrockInvocationMetrics;
+  'choices': {
+    'index'?: number;
+    'message': {
+      'content': string;
+      'role'?: string;
+      'tool_call_id'?: string;
+      'tool_calls'?: {
+        'function': any;
+        'id'?: string;
+        'index'?: number;
+        'type'?: string;
+      }[];
+    };
+    'stop_reason'?: null | string;
+  }[];
+}
+
+export const transformMistralStream = (
+  chunk: BedrockMistralStreamChunk,
+  stack: StreamStack,
+): StreamProtocolChunk => {
+  // remove 'amazon-bedrock-invocationMetrics' from chunk
+  delete chunk['amazon-bedrock-invocationMetrics'];
+
+  if (!chunk.choices || chunk.choices.length === 0) {
+    return { data: chunk, id: stack.id, type: 'data' };
+  }
+
+  const item = chunk.choices[0];
+
+  // mistral_chunk_tool_calls: {"choices":[{"index":0,"message":{"role":"assistant","content":"","tool_calls":[{"id":"3NcHNntdRyaHu8zisKJAhQ","function":{"name":"realtime-weather____fetchCurrentWeather","arguments":"{\"city\": \"Singapore\"}"}}]},"stop_reason":"tool_calls"}]}
+  if (item.message?.tool_calls) {
+    return {
+      data: item.message.tool_calls.map(
+        (value, index): StreamToolCallChunkData => ({
+          function: value.function,
+          id: value.id || generateToolCallId(index, value.function?.name),
+          index: typeof value.index !== 'undefined' ? value.index : index,
+          type: value.type || 'function',
+        }),
+      ),
+      id: stack.id,
+      type: 'tool_calls',
+    } as StreamProtocolToolCallChunk;
+  }
+  
+  if (item.stop_reason) {
+    return { data: item.stop_reason, id: stack.id, type: 'stop' };
+  }
+
+  if (typeof item.message?.content === 'string') {
+    return { data: item.message.content, id: stack.id, type: 'text' };
+  }
+
+  if (item.message?.content === null) {
+    return { data: item.message, id: stack.id, type: 'data' };
+  }
+
+  return {
+    data: { id: stack.id, index: item.index, message: item.message },
+    id: stack.id,
+    type: 'data',
+  };
+};
+
+export const AWSBedrockMistralStream = (
+  res: InvokeModelWithResponseStreamResponse | ReadableStream,
+  cb?: ChatStreamCallbacks,
+): ReadableStream<string> => {
+  const streamStack: StreamStack = { id: 'chat_' + nanoid() };
+
+  const stream = res instanceof ReadableStream ? res : createBedrockStream(res);
+
+  return stream
+    .pipeThrough(createSSEProtocolTransformer(transformMistralStream, streamStack))
+    .pipeThrough(createCallbacksTransformer(cb));
+};
