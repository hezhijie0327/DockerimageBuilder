diff --git a/src/libs/agent-runtime/ai360/index.ts b/src/libs/agent-runtime/ai360/index.ts
index efaf1d831e0e4..93887d7c7381a 100644
--- a/src/libs/agent-runtime/ai360/index.ts
+++ b/src/libs/agent-runtime/ai360/index.ts
@@ -24,17 +24,24 @@ export const LobeAi360AI = LobeOpenAICompatibleFactory({
   },
   models: {
     transformModel: (m) => {
+      const reasoningKeywords = [
+        '360gpt2-o1',
+        '360zhinao2-o1',
+      ];
+
       const model = m as unknown as Ai360ModelCard;
 
       return {
         contextWindowTokens: model.total_tokens,
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: model.id === '360gpt-pro',
         id: model.id,
         maxTokens:
           typeof model.max_tokens === 'number'
             ? model.max_tokens
             : undefined,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
       };
     },
   },
diff --git a/src/libs/agent-runtime/anthropic/index.ts b/src/libs/agent-runtime/anthropic/index.ts
index e2a6d2cb29ffd..d40c4a4f4bd3f 100644
--- a/src/libs/agent-runtime/anthropic/index.ts
+++ b/src/libs/agent-runtime/anthropic/index.ts
@@ -129,8 +129,9 @@ export class LobeAnthropicAI implements LobeRuntimeAI {
     return modelList
       .map((model) => {
         return {
+          contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
           displayName: model.display_name,
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
           functionCall: model.id.toLowerCase().includes('claude-3'),
           id: model.id,
           vision: model.id.toLowerCase().includes('claude-3') && !model.id.toLowerCase().includes('claude-3-5-haiku'),
diff --git a/src/libs/agent-runtime/baichuan/index.ts b/src/libs/agent-runtime/baichuan/index.ts
index 160adb7ed2c1d..3a4b25ae467a1 100644
--- a/src/libs/agent-runtime/baichuan/index.ts
+++ b/src/libs/agent-runtime/baichuan/index.ts
@@ -40,7 +40,7 @@ export const LobeBaichuanAI = LobeOpenAICompatibleFactory({
         return {
           contextWindowTokens: model.max_input_length,
           displayName: model.model_show_name,
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.model.endsWith(m.id))?.enabled || false,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.model === m.id)?.enabled || false,
           functionCall: model.function_call,
           id: model.model,
           maxTokens: model.max_tokens,
diff --git a/src/libs/agent-runtime/cloudflare/index.test.ts b/src/libs/agent-runtime/cloudflare/index.test.ts
index 1e8535f07e190..9d5938fb3f2d0 100644
--- a/src/libs/agent-runtime/cloudflare/index.test.ts
+++ b/src/libs/agent-runtime/cloudflare/index.test.ts
@@ -527,122 +527,5 @@ describe('LobeCloudflareAI', () => {
 
       expect(result).toHaveLength(2);
     });
-
-    it('should set id to name', async () => {
-      // Arrange
-      const instance = new LobeCloudflareAI({
-        apiKey: 'test_api_key',
-        baseURLOrAccountID: accountID,
-      });
-
-      vi.spyOn(globalThis, 'fetch').mockResolvedValue(
-        new Response(
-          JSON.stringify({
-            result: [
-              {
-                id: 'id1',
-                name: 'name1',
-                task: { name: 'Text Generation' },
-              },
-            ],
-          }),
-        ),
-      );
-
-      // Act
-      const result = await instance.models();
-
-      // Assert
-      expect(result).toEqual([
-        expect.objectContaining({
-          displayName: 'name1',
-          id: 'name1',
-        }),
-      ]);
-    });
-
-    it('should filter text generation models', async () => {
-      // Arrange
-      const instance = new LobeCloudflareAI({
-        apiKey: 'test_api_key',
-        baseURLOrAccountID: accountID,
-      });
-
-      vi.spyOn(globalThis, 'fetch').mockResolvedValue(
-        new Response(
-          JSON.stringify({
-            result: [
-              {
-                id: '1',
-                name: 'model1',
-                task: { name: 'Text Generation' },
-              },
-              {
-                id: '2',
-                name: 'model2',
-                task: { name: 'Text Classification' },
-              },
-            ],
-          }),
-        ),
-      );
-
-      // Act
-      const result = await instance.models();
-
-      // Assert
-      expect(result).toEqual([
-        expect.objectContaining({
-          displayName: 'model1',
-          id: 'model1',
-        }),
-      ]);
-    });
-
-    it('should enable non-beta models and mark beta models', async () => {
-      // Arrange
-      const instance = new LobeCloudflareAI({
-        apiKey: 'test_api_key',
-        baseURLOrAccountID: accountID,
-      });
-
-      vi.spyOn(globalThis, 'fetch').mockResolvedValue(
-        new Response(
-          JSON.stringify({
-            result: [
-              {
-                id: '1',
-                name: 'model1',
-                task: { name: 'Text Generation' },
-                properties: [{ property_id: 'beta', value: 'false' }],
-              },
-              {
-                id: '2',
-                name: 'model2',
-                task: { name: 'Text Generation' },
-                properties: [{ property_id: 'beta', value: 'true' }],
-              },
-            ],
-          }),
-        ),
-      );
-
-      // Act
-      const result = await instance.models();
-
-      // Assert
-      expect(result).toEqual([
-        expect.objectContaining({
-          displayName: 'model1',
-          enabled: true,
-          id: 'model1',
-        }),
-        expect.objectContaining({
-          displayName: 'model2 (Beta)',
-          enabled: false,
-          id: 'model2',
-        }),
-      ]);
-    });
   });
 });
diff --git a/src/libs/agent-runtime/cloudflare/index.ts b/src/libs/agent-runtime/cloudflare/index.ts
index 885e3fd7543b8..35e68ec706f89 100644
--- a/src/libs/agent-runtime/cloudflare/index.ts
+++ b/src/libs/agent-runtime/cloudflare/index.ts
@@ -1,12 +1,9 @@
-import { ChatModelCard } from '@/types/llm';
-
 import { LobeRuntimeAI } from '../BaseAI';
 import { AgentRuntimeErrorType } from '../error';
 import { ChatCompetitionOptions, ChatStreamPayload, ModelProvider } from '../types';
 import {
   CloudflareStreamTransformer,
   DEFAULT_BASE_URL_PREFIX,
-  convertModelManifest,
   desensitizeCloudflareUrl,
   fillUrl,
 } from '../utils/cloudflareHelpers';
@@ -15,6 +12,19 @@ import { debugStream } from '../utils/debugStream';
 import { StreamingResponse } from '../utils/response';
 import { createCallbacksTransformer } from '../utils/streams';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/aiModels';
+import { ChatModelCard } from '@/types/llm';
+
+export interface CloudflareModelCard {
+  description: string;
+  name: string;
+  properties?: Record<string, string>;
+  task?: {
+    description?: string;
+    name: string;
+  };
+}
+
 export interface LobeCloudflareParams {
   apiKey?: string;
   baseURLOrAccountID?: string;
@@ -111,13 +121,24 @@ export class LobeCloudflareAI implements LobeRuntimeAI {
       },
       method: 'GET',
     });
-    const j = await response.json();
-    const models: any[] = j['result'].filter(
-      (model: any) => model['task']['name'] === 'Text Generation',
-    );
-    const chatModels: ChatModelCard[] = models
-      .map((model) => convertModelManifest(model))
-      .sort((a, b) => a.displayName.localeCompare(b.displayName));
-    return chatModels;
+    const json = await response.json();
+
+    const modelList: CloudflareModelCard[] = json.result;
+
+    return modelList
+      .map((model) => {
+        return {
+          contextWindowTokens: model.properties?.max_total_tokens
+            ? Number(model.properties.max_total_tokens)
+            : LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.contextWindowTokens ?? undefined,
+          displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.displayName ?? undefined,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.enabled || false,
+          functionCall: model.description.toLowerCase().includes('function call') || model.properties?.["function_calling"] === "true",
+          id: model.name,
+          reasoning: model.name.toLowerCase().includes('deepseek-r1'),
+          vision: model.name.toLowerCase().includes('vision') || model.task?.name.toLowerCase().includes('image-to-text') || model.description.toLowerCase().includes('vision'),
+        };
+      })
+      .filter(Boolean) as ChatModelCard[];
   }
 }
diff --git a/src/libs/agent-runtime/deepseek/index.ts b/src/libs/agent-runtime/deepseek/index.ts
index 5aee3fef3eff0..aa873d605b984 100644
--- a/src/libs/agent-runtime/deepseek/index.ts
+++ b/src/libs/agent-runtime/deepseek/index.ts
@@ -64,9 +64,12 @@ export const LobeDeepSeekAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as DeepSeekModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: !model.id.toLowerCase().includes('deepseek-reasoner'),
         id: model.id,
+        reasoning: model.id.toLowerCase().includes('deepseek-reasoner'),
       };
     },
   },
diff --git a/src/libs/agent-runtime/fireworksai/index.ts b/src/libs/agent-runtime/fireworksai/index.ts
index 661a2ec555fc0..dc52d8dfdf20a 100644
--- a/src/libs/agent-runtime/fireworksai/index.ts
+++ b/src/libs/agent-runtime/fireworksai/index.ts
@@ -17,13 +17,20 @@ export const LobeFireworksAI = LobeOpenAICompatibleFactory({
   },
   models: {
     transformModel: (m) => {
+      const reasoningKeywords = [
+        'deepseek-r1',
+        'qwq',
+      ];
+
       const model = m as unknown as FireworksAIModelCard;
 
       return {
         contextWindowTokens: model.context_length,
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: model.supports_tools || model.id.toLowerCase().includes('function'),
         id: model.id,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         vision: model.supports_image_input,
       };
     },
diff --git a/src/libs/agent-runtime/giteeai/index.ts b/src/libs/agent-runtime/giteeai/index.ts
index 590a5b65b2f9f..62ceacefdaa3d 100644
--- a/src/libs/agent-runtime/giteeai/index.ts
+++ b/src/libs/agent-runtime/giteeai/index.ts
@@ -24,12 +24,20 @@ export const LobeGiteeAI = LobeOpenAICompatibleFactory({
         'qwen2-vl',
       ];
 
+      const reasoningKeywords = [
+        'deepseek-r1',
+        'qwq',
+      ];
+
       const model = m as unknown as GiteeAIModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)) && !model.id.toLowerCase().includes('qwen2.5-coder'),
         id: model.id,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         vision: visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
       };
     },
diff --git a/src/libs/agent-runtime/github/index.test.ts b/src/libs/agent-runtime/github/index.test.ts
index 346345b170de3..b7be7c5d0f8e3 100644
--- a/src/libs/agent-runtime/github/index.test.ts
+++ b/src/libs/agent-runtime/github/index.test.ts
@@ -217,22 +217,6 @@ describe('LobeGithubAI', () => {
     it('should return a list of models', async () => {
       // Arrange
       const arr = [
-        {
-          id: 'azureml://registries/azureml-ai21/models/AI21-Jamba-Instruct/versions/2',
-          name: 'AI21-Jamba-Instruct',
-          friendly_name: 'AI21-Jamba-Instruct',
-          model_version: 2,
-          publisher: 'AI21 Labs',
-          model_family: 'AI21 Labs',
-          model_registry: 'azureml-ai21',
-          license: 'custom',
-          task: 'chat-completion',
-          description:
-            "Jamba-Instruct is the world's first production-grade Mamba-based LLM model and leverages its hybrid Mamba-Transformer architecture to achieve best-in-class performance, quality, and cost efficiency.\n\n**Model Developer Name**: _AI21 Labs_\n\n## Model Architecture\n\nJamba-Instruct leverages a hybrid Mamba-Transformer architecture to achieve best-in-class performance, quality, and cost efficiency.\nAI21's Jamba architecture features a blocks-and-layers approach that allows Jamba to successfully integrate the two architectures. Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers.\n",
-          summary:
-            "Jamba-Instruct is the world's first production-grade Mamba-based LLM model and leverages its hybrid Mamba-Transformer architecture to achieve best-in-class performance, quality, and cost efficiency.",
-          tags: ['chat', 'rag'],
-        },
         {
           id: 'azureml://registries/azureml-cohere/models/Cohere-command-r/versions/3',
           name: 'Cohere-command-r',
@@ -263,9 +247,14 @@ describe('LobeGithubAI', () => {
       for (let i = 0; i < arr.length; i++) {
         const model = models[i];
         expect(model).toEqual({
+          contextWindowTokens: undefined,
           description: arr[i].description,
           displayName: arr[i].friendly_name,
+          enabled: false,
+          functionCall: true,
           id: arr[i].name,
+          reasoning: false,
+          vision: false,
         });
       }
     });
diff --git a/src/libs/agent-runtime/github/index.ts b/src/libs/agent-runtime/github/index.ts
index e5241cefb8078..7552b98b5aa0b 100644
--- a/src/libs/agent-runtime/github/index.ts
+++ b/src/libs/agent-runtime/github/index.ts
@@ -1,34 +1,20 @@
-import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
-import type { ChatModelCard } from '@/types/llm';
-
 import { AgentRuntimeErrorType } from '../error';
 import { pruneReasoningPayload } from '../openai';
 import { ModelProvider } from '../types';
-import {
-  CHAT_MODELS_BLOCK_LIST,
-  LobeOpenAICompatibleFactory,
-} from '../utils/openaiCompatibleFactory';
+import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
-enum Task {
-  'chat-completion',
-  'embeddings',
-}
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/aiModels';
+import type { ChatModelCard } from '@/types/llm';
 
-/* eslint-disable typescript-sort-keys/interface */
-type Model = {
+export interface GithubModelCard {
+  description: string;
+  friendly_name: string;
   id: string;
   name: string;
-  friendly_name: string;
-  model_version: number;
-  publisher: string;
-  model_family: string;
-  model_registry: string;
-  license: string;
-  task: Task;
-  description: string;
-  summary: string;
   tags: string[];
-};
+  task: string;
+}
+
 /* eslint-enable typescript-sort-keys/interface */
 
 export const LobeGithubAI = LobeOpenAICompatibleFactory({
@@ -52,23 +38,35 @@ export const LobeGithubAI = LobeOpenAICompatibleFactory({
     invalidAPIKey: AgentRuntimeErrorType.InvalidGithubToken,
   },
   models: async ({ client }) => {
+    const functionCallKeywords = [
+      'function',
+      'tool',
+    ];
+
+    const visionKeywords = [
+      'vision',
+    ];
+
+    const reasoningKeywords = [
+      'deepseek-r1',
+      'o1',
+      'o3',
+    ];
+
     const modelsPage = (await client.models.list()) as any;
-    const modelList: Model[] = modelsPage.body;
+    const modelList: GithubModelCard[] = modelsPage.body;
+
     return modelList
-      .filter((model) => {
-        return CHAT_MODELS_BLOCK_LIST.every(
-          (keyword) => !model.name.toLowerCase().includes(keyword),
-        );
-      })
       .map((model) => {
-        const knownModel = LOBE_DEFAULT_MODEL_LIST.find((m) => m.id === model.name);
-
-        if (knownModel) return knownModel;
-
         return {
+          contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.contextWindowTokens ?? undefined,
           description: model.description,
           displayName: model.friendly_name,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.enabled || false,
+          functionCall: functionCallKeywords.some(keyword => model.description.toLowerCase().includes(keyword)),
           id: model.name,
+          reasoning: reasoningKeywords.some(keyword => model.name.toLowerCase().includes(keyword)),
+          vision: visionKeywords.some(keyword => model.description.toLowerCase().includes(keyword)),
         };
       })
       .filter(Boolean) as ChatModelCard[];
diff --git a/src/libs/agent-runtime/google/index.ts b/src/libs/agent-runtime/google/index.ts
index 0e744f33f5346..8f7de84ecb401 100644
--- a/src/libs/agent-runtime/google/index.ts
+++ b/src/libs/agent-runtime/google/index.ts
@@ -152,9 +152,10 @@ export class LobeGoogleAI implements LobeRuntimeAI {
         return {
           contextWindowTokens: model.inputTokenLimit + model.outputTokenLimit,
           displayName: model.displayName,
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => modelName.endsWith(m.id))?.enabled || false,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => modelName === m.id)?.enabled || false,
           functionCall: modelName.toLowerCase().includes('gemini'),
           id: modelName,
+          reasoning: modelName.toLowerCase().includes('thinking'),
           vision:
             modelName.toLowerCase().includes('vision') ||
             (modelName.toLowerCase().includes('gemini') &&
diff --git a/src/libs/agent-runtime/groq/index.ts b/src/libs/agent-runtime/groq/index.ts
index 473255acbd98b..c8dd645b41999 100644
--- a/src/libs/agent-runtime/groq/index.ts
+++ b/src/libs/agent-runtime/groq/index.ts
@@ -42,13 +42,19 @@ export const LobeGroq = LobeOpenAICompatibleFactory({
         'gemma2-9b-it',
       ];
 
+      const reasoningKeywords = [
+        'deepseek-r1',
+      ];
+
       const model = m as unknown as GroqModelCard;
 
       return {
         contextWindowTokens: model.context_window,
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         id: model.id,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         vision: model.id.toLowerCase().includes('vision'),
       };
     },
diff --git a/src/libs/agent-runtime/higress/index.ts b/src/libs/agent-runtime/higress/index.ts
index 6b591ffe8357b..04554cd4d785a 100644
--- a/src/libs/agent-runtime/higress/index.ts
+++ b/src/libs/agent-runtime/higress/index.ts
@@ -26,7 +26,7 @@ export const LobeHigressAI = LobeOpenAICompatibleFactory({
         contextWindowTokens: model.context_length,
         description: model.description,
         displayName: model.name,
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall:
           model.description.includes('function calling') || model.description.includes('tools'),
         id: model.id,
@@ -34,6 +34,7 @@ export const LobeHigressAI = LobeOpenAICompatibleFactory({
           typeof model.top_provider.max_completion_tokens === 'number'
             ? model.top_provider.max_completion_tokens
             : undefined,
+        reasoning: model.description.includes('reasoning'),
         vision:
           model.description.includes('vision') ||
           model.description.includes('multimodal') ||
diff --git a/src/libs/agent-runtime/huggingface/index.ts b/src/libs/agent-runtime/huggingface/index.ts
index c5b6d3cf59d13..d1847f41fa696 100644
--- a/src/libs/agent-runtime/huggingface/index.ts
+++ b/src/libs/agent-runtime/huggingface/index.ts
@@ -62,6 +62,12 @@ export const LobeHuggingFaceAI = LobeOpenAICompatibleFactory({
       'vision',
     ];
 
+    const reasoningKeywords = [
+      'deepseek-r1',
+      'qvq',
+      'qwq',
+    ];
+
     // ref: https://huggingface.co/docs/hub/api
     const url = 'https://huggingface.co/api/models';
     const response = await fetch(url, {
@@ -74,9 +80,12 @@ export const LobeHuggingFaceAI = LobeOpenAICompatibleFactory({
     return modelList
       .map((model) => {
         return {
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+          contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+          displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
           functionCall: model.tags.some(tag => tag.toLowerCase().includes('function-calling')),
           id: model.id,
+          reasoning: model.tags.some(tag => tag.toLowerCase().includes('reasoning')) || reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
           vision: model.tags.some(tag =>
             visionKeywords.some(keyword => tag.toLowerCase().includes(keyword))
           ),
diff --git a/src/libs/agent-runtime/hunyuan/index.ts b/src/libs/agent-runtime/hunyuan/index.ts
index bd26c0fc77a33..5bcf827cc54ec 100644
--- a/src/libs/agent-runtime/hunyuan/index.ts
+++ b/src/libs/agent-runtime/hunyuan/index.ts
@@ -23,7 +23,9 @@ export const LobeHunyuanAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as HunyuanModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)) && !model.id.toLowerCase().includes('vision'),
         id: model.id,
         vision: model.id.toLowerCase().includes('vision'),
diff --git a/src/libs/agent-runtime/internlm/index.ts b/src/libs/agent-runtime/internlm/index.ts
index 0c590ee4d46df..267371360eafa 100644
--- a/src/libs/agent-runtime/internlm/index.ts
+++ b/src/libs/agent-runtime/internlm/index.ts
@@ -25,7 +25,9 @@ export const LobeInternLMAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as InternLMModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: true,
         id: model.id,
       };
diff --git a/src/libs/agent-runtime/mistral/index.ts b/src/libs/agent-runtime/mistral/index.ts
index 3c76a0d34a3ac..27f695e36195a 100644
--- a/src/libs/agent-runtime/mistral/index.ts
+++ b/src/libs/agent-runtime/mistral/index.ts
@@ -37,7 +37,8 @@ export const LobeMistralAI = LobeOpenAICompatibleFactory({
       return {
         contextWindowTokens: model.max_context_length,
         description: model.description,
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: model.capabilities.function_calling,
         id: model.id,
         vision: model.capabilities.vision,
diff --git a/src/libs/agent-runtime/moonshot/index.ts b/src/libs/agent-runtime/moonshot/index.ts
index 9bb015e9e91e3..9bb92fa2dc2a3 100644
--- a/src/libs/agent-runtime/moonshot/index.ts
+++ b/src/libs/agent-runtime/moonshot/index.ts
@@ -29,7 +29,9 @@ export const LobeMoonshotAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as MoonshotModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: true,
         id: model.id,
         vision: model.id.toLowerCase().includes('vision'),
diff --git a/src/libs/agent-runtime/novita/__snapshots__/index.test.ts.snap b/src/libs/agent-runtime/novita/__snapshots__/index.test.ts.snap
index b765e49432c89..fd4d625499a73 100644
--- a/src/libs/agent-runtime/novita/__snapshots__/index.test.ts.snap
+++ b/src/libs/agent-runtime/novita/__snapshots__/index.test.ts.snap
@@ -6,17 +6,21 @@ exports[`NovitaAI > models > should get models 1`] = `
     "contextWindowTokens": 8192,
     "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
     "displayName": "meta-llama/llama-3-8b-instruct",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "meta-llama/llama-3-8b-instruct",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 8192,
     "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
     "displayName": "meta-llama/llama-3-70b-instruct",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "meta-llama/llama-3-70b-instruct",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 8192,
@@ -25,6 +29,8 @@ exports[`NovitaAI > models > should get models 1`] = `
     "enabled": true,
     "functionCall": false,
     "id": "meta-llama/llama-3.1-8b-instruct",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 8192,
@@ -33,6 +39,8 @@ exports[`NovitaAI > models > should get models 1`] = `
     "enabled": true,
     "functionCall": false,
     "id": "meta-llama/llama-3.1-70b-instruct",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 32768,
@@ -41,6 +49,8 @@ exports[`NovitaAI > models > should get models 1`] = `
     "enabled": true,
     "functionCall": false,
     "id": "meta-llama/llama-3.1-405b-instruct",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 8192,
@@ -50,22 +60,28 @@ Designed for a wide variety of tasks, it empowers developers and researchers to
     "enabled": true,
     "functionCall": false,
     "id": "google/gemma-2-9b-it",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 4096,
     "description": "This is a fine-tuned Llama-2 model designed to support longer and more detailed writing prompts, as well as next-chapter generation. It also includes an experimental role-playing instruction set with multi-round dialogues, character interactions, and varying numbers of participants",
     "displayName": "jondurbin/airoboros-l2-70b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "jondurbin/airoboros-l2-70b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 8192,
     "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
     "displayName": "nousresearch/hermes-2-pro-llama-3-8b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": true,
     "id": "nousresearch/hermes-2-pro-llama-3-8b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 32768,
@@ -74,70 +90,88 @@ Designed for a wide variety of tasks, it empowers developers and researchers to
     "enabled": true,
     "functionCall": false,
     "id": "mistralai/mistral-7b-instruct",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 16000,
     "description": "Dolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of Mixtral 8x22B Instruct. It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.The model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use.",
     "displayName": "cognitivecomputations/dolphin-mixtral-8x22b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "cognitivecomputations/dolphin-mixtral-8x22b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 16000,
     "description": "The uncensored llama3 model is a powerhouse of creativity, excelling in both roleplay and story writing. It offers a liberating experience during roleplays, free from any restrictions. This model stands out for its immense creativity, boasting a vast array of unique ideas and plots, truly a treasure trove for those seeking originality. Its unrestricted nature during roleplays allows for the full breadth of imagination to unfold, akin to an enhanced, big-brained version of Stheno. Perfect for creative minds seeking a boundless platform for their imaginative expressions, the uncensored llama3 model is an ideal choice",
     "displayName": "sao10k/l3-70b-euryale-v2.1",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "sao10k/l3-70b-euryale-v2.1",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 4096,
     "description": "A merge with a complex family tree, this model was crafted for roleplaying and storytelling. Midnight Rose is a successor to Rogue Rose and Aurora Nights and improves upon them both. It wants to produce lengthy output by default and is the best creative writing merge produced so far by sophosympatheia.",
     "displayName": "sophosympatheia/midnight-rose-70b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "sophosympatheia/midnight-rose-70b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 4096,
     "description": "The idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time).",
     "displayName": "gryphe/mythomax-l2-13b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "gryphe/mythomax-l2-13b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 4096,
     "description": "Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.",
     "displayName": "nousresearch/nous-hermes-llama2-13b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "nousresearch/nous-hermes-llama2-13b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 32768,
     "description": "Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the Mixtral 8x7B MoE LLM. The model was trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.",
     "displayName": "Nous-Hermes-2-Mixtral-8x7B-DPO",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "Nous-Hermes-2-Mixtral-8x7B-DPO",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 4096,
     "description": "A Mythomax/MLewd_13B-style merge of selected 70B models. A multi-model merge of several LLaMA2 70B finetunes for roleplaying and creative work. The goal was to create a model that combines creativity with intelligence for an enhanced experience.",
     "displayName": "lzlv_70b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "lzlv_70b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 4096,
     "description": "OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.",
     "displayName": "teknium/openhermes-2.5-mistral-7b",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "teknium/openhermes-2.5-mistral-7b",
+    "reasoning": false,
+    "vision": false,
   },
   {
     "contextWindowTokens": 65535,
@@ -146,6 +180,8 @@ Designed for a wide variety of tasks, it empowers developers and researchers to
     "enabled": true,
     "functionCall": false,
     "id": "microsoft/wizardlm-2-8x22b",
+    "reasoning": false,
+    "vision": false,
   },
 ]
 `;
diff --git a/src/libs/agent-runtime/novita/index.ts b/src/libs/agent-runtime/novita/index.ts
index aa05831909b84..a7c7d33f3253d 100644
--- a/src/libs/agent-runtime/novita/index.ts
+++ b/src/libs/agent-runtime/novita/index.ts
@@ -2,6 +2,8 @@ import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 import { NovitaModelCard } from './type';
 
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/aiModels';
+
 export const LobeNovitaAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.novita.ai/v3/openai',
   constructorOptions: {
@@ -14,15 +16,21 @@ export const LobeNovitaAI = LobeOpenAICompatibleFactory({
   },
   models: {
     transformModel: (m) => {
+      const reasoningKeywords = [
+        'deepseek-r1',
+      ];
+
       const model = m as unknown as NovitaModelCard;
 
       return {
         contextWindowTokens: model.context_size,
         description: model.description,
         displayName: model.title,
-        enabled: model.status === 1,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: model.description.toLowerCase().includes('function calling'),
         id: model.id,
+        reasoning: model.description.toLowerCase().includes('reasoning task') || reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
+        vision: model.description.toLowerCase().includes('vision'),
       };
     },
   },
diff --git a/src/libs/agent-runtime/openrouter/__snapshots__/index.test.ts.snap b/src/libs/agent-runtime/openrouter/__snapshots__/index.test.ts.snap
index 1e593e88ca8d1..b66823705e6c5 100644
--- a/src/libs/agent-runtime/openrouter/__snapshots__/index.test.ts.snap
+++ b/src/libs/agent-runtime/openrouter/__snapshots__/index.test.ts.snap
@@ -14,6 +14,7 @@ Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.co
     "functionCall": false,
     "id": "cohere/command-r-03-2024",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -28,6 +29,7 @@ Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.co
     "functionCall": false,
     "id": "cohere/command-r-plus-04-2024",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -42,6 +44,7 @@ Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.co
     "functionCall": false,
     "id": "cohere/command-r-plus-08-2024",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -56,6 +59,7 @@ Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.co
     "functionCall": false,
     "id": "cohere/command-r-08-2024",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -72,6 +76,7 @@ Note: This model is experimental and not suited for production use-cases. It may
     "functionCall": false,
     "id": "google/gemini-flash-8b-1.5-exp",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -88,6 +93,7 @@ Note: This model is experimental and not suited for production use-cases. It may
     "functionCall": false,
     "id": "google/gemini-flash-1.5-exp",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -98,6 +104,7 @@ Note: This model is experimental and not suited for production use-cases. It may
     "functionCall": false,
     "id": "sao10k/l3.1-euryale-70b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -114,6 +121,7 @@ Read their [announcement](https://www.ai21.com/blog/announcing-jamba-model-famil
     "functionCall": false,
     "id": "ai21/jamba-1-5-large",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -130,6 +138,7 @@ Read their [announcement](https://www.ai21.com/blog/announcing-jamba-model-famil
     "functionCall": false,
     "id": "ai21/jamba-1-5-mini",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -142,6 +151,7 @@ The models underwent a rigorous enhancement process, incorporating both supervis
     "functionCall": false,
     "id": "microsoft/phi-3.5-mini-128k-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -156,6 +166,7 @@ The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, incl
     "functionCall": true,
     "id": "nousresearch/hermes-3-llama-3.1-70b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -172,6 +183,7 @@ Hermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at genera
     "functionCall": true,
     "id": "nousresearch/hermes-3-llama-3.1-405b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -190,6 +202,7 @@ _These are extended-context endpoints for [Hermes 3 405B Instruct](/models/nousr
     "functionCall": true,
     "id": "nousresearch/hermes-3-llama-3.1-405b:extended",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -200,6 +213,7 @@ _These are extended-context endpoints for [Hermes 3 405B Instruct](/models/nousr
     "functionCall": false,
     "id": "perplexity/llama-3.1-sonar-huge-128k-online",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -208,10 +222,11 @@ _These are extended-context endpoints for [Hermes 3 405B Instruct](/models/nousr
 
 Note: This model is experimental and not suited for production use-cases. It may be removed or redirected to another model in the future.",
     "displayName": "OpenAI: ChatGPT-4o",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "openai/chatgpt-4o-latest",
     "maxTokens": 16384,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -226,6 +241,7 @@ For best results, use with Llama 3 Instruct context template, temperature 1.4, a
     "functionCall": false,
     "id": "sao10k/l3-lunaris-8b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -238,6 +254,7 @@ Although more similar to Magnum overall, the model remains very creative, with a
     "functionCall": false,
     "id": "aetherwiing/mn-starcannon-12b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -252,6 +269,7 @@ For benchmarking against other models, it was briefly called ["im-also-a-good-gp
     "functionCall": false,
     "id": "openai/gpt-4o-2024-08-06",
     "maxTokens": 16384,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -266,6 +284,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-3.1-405b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -280,6 +299,7 @@ Check out the model's [HuggingFace page](https://huggingface.co/nothingiisreal/M
     "functionCall": false,
     "id": "nothingiisreal/mn-celeste-12b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -296,6 +316,7 @@ Note: This model is experimental and not suited for production use-cases. It may
     "functionCall": false,
     "id": "google/gemini-pro-1.5-exp",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -308,6 +329,7 @@ This is the online version of the [offline chat model](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3.1-sonar-large-128k-online",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -320,6 +342,7 @@ This is a normal offline LLM, but the [online version](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3.1-sonar-large-128k-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -332,6 +355,7 @@ This is the online version of the [offline chat model](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3.1-sonar-small-128k-online",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -344,6 +368,7 @@ This is a normal offline LLM, but the [online version](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3.1-sonar-small-128k-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -358,6 +383,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-3.1-70b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -374,6 +400,7 @@ _These are free, rate-limited endpoints for [Llama 3.1 8B Instruct](/models/meta
     "functionCall": false,
     "id": "meta-llama/llama-3.1-8b-instruct:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -388,6 +415,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-3.1-8b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -404,6 +432,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-3.1-405b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -418,6 +447,7 @@ Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.m
     "functionCall": true,
     "id": "cognitivecomputations/dolphin-llama-3-70b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -434,6 +464,7 @@ Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.m
     "functionCall": false,
     "id": "mistralai/codestral-mamba",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -444,10 +475,11 @@ The model is multilingual, supporting English, French, German, Spanish, Italian,
 
 It supports function calling and is released under the Apache 2.0 license.",
     "displayName": "Mistral: Mistral Nemo",
-    "enabled": false,
+    "enabled": true,
     "functionCall": true,
     "id": "mistralai/mistral-nemo",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -464,6 +496,7 @@ Check out the [launch announcement](https://openai.com/index/gpt-4o-mini-advanci
     "functionCall": false,
     "id": "openai/gpt-4o-mini-2024-07-18",
     "maxTokens": 16384,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -480,6 +513,7 @@ Check out the [launch announcement](https://openai.com/index/gpt-4o-mini-advanci
     "functionCall": false,
     "id": "openai/gpt-4o-mini",
     "maxTokens": 16384,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -498,6 +532,7 @@ _These are free, rate-limited endpoints for [Qwen 2 7B Instruct](/models/qwen/qw
     "functionCall": false,
     "id": "qwen/qwen-2-7b-instruct:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -514,6 +549,7 @@ Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://hug
     "functionCall": false,
     "id": "qwen/qwen-2-7b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -528,6 +564,7 @@ See the [launch announcement](https://blog.google/technology/developers/google-g
     "functionCall": false,
     "id": "google/gemma-2-27b-it",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -540,6 +577,7 @@ The model is based on [Qwen2 72B](https://openrouter.ai/models/qwen/qwen-2-72b-i
     "functionCall": false,
     "id": "alpindale/magnum-72b",
     "maxTokens": 1024,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -552,6 +590,7 @@ Hermes-2 Î˜ (theta) was specifically designed with a few capabilities in mind: e
     "functionCall": false,
     "id": "nousresearch/hermes-2-theta-llama-3-8b",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -568,6 +607,7 @@ _These are free, rate-limited endpoints for [Gemma 2 9B](/models/google/gemma-2-
     "functionCall": false,
     "id": "google/gemma-2-9b-it:free",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -582,6 +622,7 @@ See the [launch announcement](https://blog.google/technology/developers/google-g
     "functionCall": false,
     "id": "google/gemma-2-9b-it",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -597,6 +638,7 @@ Compared to older Stheno version, this model is trained on:
     "functionCall": false,
     "id": "sao10k/l3-stheno-8b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -614,6 +656,7 @@ Jamba has a knowledge cutoff of February 2024.",
     "functionCall": false,
     "id": "ai21/jamba-instruct",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -631,6 +674,7 @@ Jamba has a knowledge cutoff of February 2024.",
     "functionCall": true,
     "id": "anthropic/claude-3.5-sonnet",
     "maxTokens": 8192,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -650,6 +694,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": true,
     "id": "anthropic/claude-3.5-sonnet:beta",
     "maxTokens": 8192,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -666,6 +711,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "sao10k/l3-euryale-70b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -680,6 +726,7 @@ For 128k context length, try [Phi-3 Medium 128K](/models/microsoft/phi-3-medium-
     "functionCall": false,
     "id": "microsoft/phi-3-medium-4k-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -696,6 +743,7 @@ The model is uncensored and is stripped of alignment and bias. It requires an ex
     "functionCall": false,
     "id": "cognitivecomputations/dolphin-mixtral-8x22b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -712,6 +760,7 @@ Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://hug
     "functionCall": false,
     "id": "qwen/qwen-2-72b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -729,6 +778,7 @@ It outperforms many similarly sized models including [Llama 3 8B Instruct](/mode
     "functionCall": false,
     "id": "openchat/openchat-8b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -739,6 +789,7 @@ It outperforms many similarly sized models including [Llama 3 8B Instruct](/mode
     "functionCall": false,
     "id": "nousresearch/hermes-2-pro-llama-3-8b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -757,6 +808,7 @@ NOTE: Support for function calling depends on the provider.",
     "functionCall": true,
     "id": "mistralai/mistral-7b-instruct-v0.3",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -771,6 +823,7 @@ _These are free, rate-limited endpoints for [Mistral 7B Instruct](/models/mistra
     "functionCall": false,
     "id": "mistralai/mistral-7b-instruct:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -783,6 +836,7 @@ _These are free, rate-limited endpoints for [Mistral 7B Instruct](/models/mistra
     "functionCall": false,
     "id": "mistralai/mistral-7b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -797,6 +851,7 @@ _These are higher-throughput endpoints for [Mistral 7B Instruct](/models/mistral
     "functionCall": false,
     "id": "mistralai/mistral-7b-instruct:nitro",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -811,6 +866,7 @@ _These are free, rate-limited endpoints for [Phi-3 Mini 128K Instruct](/models/m
     "functionCall": false,
     "id": "microsoft/phi-3-mini-128k-instruct:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -823,6 +879,7 @@ At time of release, Phi-3 Medium demonstrated state-of-the-art performance among
     "functionCall": false,
     "id": "microsoft/phi-3-mini-128k-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -839,6 +896,7 @@ _These are free, rate-limited endpoints for [Phi-3 Medium 128K Instruct](/models
     "functionCall": false,
     "id": "microsoft/phi-3-medium-128k-instruct:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -853,6 +911,7 @@ For 4k context length, try [Phi-3 Medium 4K](/models/microsoft/phi-3-medium-4k-i
     "functionCall": false,
     "id": "microsoft/phi-3-medium-128k-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -867,6 +926,7 @@ Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.m
     "functionCall": false,
     "id": "neversleep/llama-3-lumimaid-70b",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -883,6 +943,7 @@ Usage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.d
     "functionCall": false,
     "id": "google/gemini-flash-1.5",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -895,6 +956,7 @@ The original V1 model was trained from scratch on 2T tokens, with a composition
     "functionCall": false,
     "id": "deepseek/deepseek-coder",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -909,6 +971,7 @@ DeepSeek-V2 achieves remarkable performance on both standard benchmarks and open
     "functionCall": false,
     "id": "deepseek/deepseek-chat",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -921,6 +984,7 @@ This is the online version of the [offline chat model](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3-sonar-large-32k-online",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -933,6 +997,7 @@ This is a normal offline LLM, but the [online version](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3-sonar-large-32k-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -945,6 +1010,7 @@ This is the online version of the [offline chat model](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3-sonar-small-32k-online",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -957,6 +1023,7 @@ This is a normal offline LLM, but the [online version](/models/perplexity/llama-
     "functionCall": false,
     "id": "perplexity/llama-3-sonar-small-32k-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -975,6 +1042,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-guard-2-8b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -987,6 +1055,7 @@ For benchmarking against other models, it was briefly called ["im-also-a-good-gp
     "functionCall": false,
     "id": "openai/gpt-4o-2024-05-13",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -999,6 +1068,7 @@ For benchmarking against other models, it was briefly called ["im-also-a-good-gp
     "functionCall": false,
     "id": "openai/gpt-4o",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1011,6 +1081,7 @@ _These are extended-context endpoints for [GPT-4o](/models/openai/gpt-4o). They
     "functionCall": false,
     "id": "openai/gpt-4o:extended",
     "maxTokens": 64000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1029,6 +1100,7 @@ Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://hug
     "functionCall": false,
     "id": "qwen/qwen-72b-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1047,6 +1119,7 @@ Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://hug
     "functionCall": false,
     "id": "qwen/qwen-110b-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1061,6 +1134,7 @@ Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.m
     "functionCall": false,
     "id": "neversleep/llama-3-lumimaid-8b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1077,6 +1151,7 @@ _These are extended-context endpoints for [Llama 3 Lumimaid 8B](/models/neversle
     "functionCall": false,
     "id": "neversleep/llama-3-lumimaid-8b:extended",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1089,6 +1164,7 @@ If you submit a raw prompt, you can use Alpaca or Vicuna formats.",
     "functionCall": false,
     "id": "sao10k/fimbulvetr-11b-v2",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1103,6 +1179,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-3-70b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1119,6 +1196,7 @@ _These are higher-throughput endpoints for [Llama 3 70B Instruct](/models/meta-l
     "functionCall": false,
     "id": "meta-llama/llama-3-70b-instruct:nitro",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1135,6 +1213,7 @@ _These are free, rate-limited endpoints for [Llama 3 8B Instruct](/models/meta-l
     "functionCall": false,
     "id": "meta-llama/llama-3-8b-instruct:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1149,6 +1228,7 @@ To read more about the model release, [click here](https://ai.meta.com/blog/meta
     "functionCall": false,
     "id": "meta-llama/llama-3-8b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1165,6 +1245,7 @@ _These are higher-throughput endpoints for [Llama 3 8B Instruct](/models/meta-ll
     "functionCall": false,
     "id": "meta-llama/llama-3-8b-instruct:nitro",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1181,6 +1262,7 @@ _These are extended-context endpoints for [Llama 3 8B Instruct](/models/meta-lla
     "functionCall": false,
     "id": "meta-llama/llama-3-8b-instruct:extended",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1197,6 +1279,7 @@ See benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral
     "functionCall": false,
     "id": "mistralai/mixtral-8x22b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1213,6 +1296,7 @@ To read more about the model release, [click here](https://wizardlm.github.io/Wi
     "functionCall": false,
     "id": "microsoft/wizardlm-2-7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1229,6 +1313,7 @@ To read more about the model release, [click here](https://wizardlm.github.io/Wi
     "functionCall": false,
     "id": "microsoft/wizardlm-2-8x22b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1254,6 +1339,7 @@ Usage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.d
     "functionCall": false,
     "id": "google/gemini-pro-1.5",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1266,6 +1352,7 @@ Training data: up to December 2023.",
     "functionCall": true,
     "id": "openai/gpt-4-turbo",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1276,10 +1363,11 @@ It offers multilingual support for ten key languages to facilitate global busine
 
 Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).",
     "displayName": "Cohere: Command R+",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "cohere/command-r-plus",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1296,6 +1384,7 @@ See the launch announcement and benchmark results [here](https://www.databricks.
     "functionCall": false,
     "id": "databricks/dbrx-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1308,6 +1397,7 @@ Descending from earlier versions of Midnight Rose and [Wizard Tulu Dolphin 70B](
     "functionCall": false,
     "id": "sophosympatheia/midnight-rose-70b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1318,10 +1408,11 @@ Read the launch post [here](https://txt.cohere.com/command-r/).
 
 Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).",
     "displayName": "Cohere: Command R",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "cohere/command-r",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1334,6 +1425,7 @@ Use of this model is subject to Cohere's [Acceptable Use Policy](https://docs.co
     "functionCall": false,
     "id": "cohere/command",
     "maxTokens": 4000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1349,6 +1441,7 @@ See the launch announcement and benchmark results [here](https://www.anthropic.c
     "functionCall": false,
     "id": "anthropic/claude-3-haiku",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1366,6 +1459,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-3-haiku:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1380,6 +1474,7 @@ See the launch announcement and benchmark results [here](https://www.anthropic.c
     "functionCall": false,
     "id": "anthropic/claude-3-sonnet",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1396,6 +1491,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-3-sonnet:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1410,6 +1506,7 @@ See the launch announcement and benchmark results [here](https://www.anthropic.c
     "functionCall": false,
     "id": "anthropic/claude-3-opus",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1426,6 +1523,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-3-opus:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1438,6 +1536,7 @@ It is fluent in English, French, Spanish, German, and Italian, with high grammat
     "functionCall": false,
     "id": "mistralai/mistral-large",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1450,6 +1549,7 @@ It is fluent in English, French, Spanish, German, and Italian, with high grammat
     "functionCall": true,
     "id": "openai/gpt-4-turbo-preview",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1462,6 +1562,7 @@ Training data up to Sep 2021.",
     "functionCall": false,
     "id": "openai/gpt-3.5-turbo-0613",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1476,6 +1577,7 @@ The model was trained on over 1,000,000 entries of primarily [GPT-4](/models/ope
     "functionCall": false,
     "id": "nousresearch/nous-hermes-2-mixtral-8x7b-dpo",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1486,6 +1588,7 @@ The model was trained on over 1,000,000 entries of primarily [GPT-4](/models/ope
     "functionCall": false,
     "id": "mistralai/mistral-medium",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1497,6 +1600,7 @@ The model was trained on over 1,000,000 entries of primarily [GPT-4](/models/ope
     "functionCall": false,
     "id": "mistralai/mistral-small",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1507,6 +1611,7 @@ The model was trained on over 1,000,000 entries of primarily [GPT-4](/models/ope
     "functionCall": false,
     "id": "mistralai/mistral-tiny",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1517,6 +1622,7 @@ The model was trained on over 1,000,000 entries of primarily [GPT-4](/models/ope
     "functionCall": false,
     "id": "austism/chronos-hermes-13b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1529,6 +1635,7 @@ Nous-Hermes 2 on Yi 34B outperforms all Nous-Hermes & Open-Hermes models of the
     "functionCall": false,
     "id": "nousresearch/nous-hermes-yi-34b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1545,6 +1652,7 @@ An improved version of [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruc
     "functionCall": false,
     "id": "mistralai/mistral-7b-instruct-v0.2",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1559,6 +1667,7 @@ The model is uncensored and is stripped of alignment and bias. It requires an ex
     "functionCall": false,
     "id": "cognitivecomputations/dolphin-mixtral-8x7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1575,6 +1684,7 @@ Usage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.d
     "functionCall": false,
     "id": "google/gemini-pro-vision",
     "maxTokens": 8192,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1589,6 +1699,7 @@ Usage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.d
     "functionCall": false,
     "id": "google/gemini-pro",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1601,6 +1712,7 @@ Instruct model fine-tuned by Mistral. #moe",
     "functionCall": false,
     "id": "mistralai/mixtral-8x7b-instruct",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1615,6 +1727,7 @@ _These are higher-throughput endpoints for [Mixtral 8x7B Instruct](/models/mistr
     "functionCall": false,
     "id": "mistralai/mixtral-8x7b-instruct:nitro",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1627,6 +1740,7 @@ _These are higher-throughput endpoints for [Mixtral 8x7B Instruct](/models/mistr
     "functionCall": false,
     "id": "mistralai/mixtral-8x7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1639,6 +1753,7 @@ StripedHyena uses a new architecture that competes with traditional Transformers
     "functionCall": false,
     "id": "togethercomputer/stripedhyena-nous-7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1655,6 +1770,7 @@ _These are free, rate-limited endpoints for [MythoMist 7B](/models/gryphe/mythom
     "functionCall": false,
     "id": "gryphe/mythomist-7b:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1669,6 +1785,7 @@ It combines [Neural Chat 7B](/models/intel/neural-chat-7b), Airoboros 7b, [Toppy
     "functionCall": false,
     "id": "gryphe/mythomist-7b",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1686,6 +1803,7 @@ _These are free, rate-limited endpoints for [OpenChat 3.5 7B](/models/openchat/o
     "functionCall": false,
     "id": "openchat/openchat-7b:free",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1701,6 +1819,7 @@ _These are free, rate-limited endpoints for [OpenChat 3.5 7B](/models/openchat/o
     "functionCall": false,
     "id": "openchat/openchat-7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1713,6 +1832,7 @@ _These are free, rate-limited endpoints for [OpenChat 3.5 7B](/models/openchat/o
     "functionCall": false,
     "id": "neversleep/noromaid-20b",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1723,6 +1843,7 @@ _These are free, rate-limited endpoints for [OpenChat 3.5 7B](/models/openchat/o
     "functionCall": false,
     "id": "anthropic/claude-instant-1.1",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1733,6 +1854,7 @@ _These are free, rate-limited endpoints for [OpenChat 3.5 7B](/models/openchat/o
     "functionCall": false,
     "id": "anthropic/claude-2.1",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1745,6 +1867,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-2.1:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1755,6 +1878,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-2",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1767,6 +1891,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-2:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1778,6 +1903,7 @@ Potentially the most interesting finding from training on a good ratio (est. of
     "functionCall": false,
     "id": "teknium/openhermes-2.5-mistral-7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1792,6 +1918,7 @@ Potentially the most interesting finding from training on a good ratio (est. of
     "functionCall": false,
     "id": "openai/gpt-4-vision-preview",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1805,6 +1932,7 @@ A multi-model merge of several LLaMA2 70B finetunes for roleplaying and creative
     "functionCall": false,
     "id": "lizpreciatior/lzlv-70b-fp16-hf",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1821,6 +1949,7 @@ Credits to
     "functionCall": false,
     "id": "alpindale/goliath-120b",
     "maxTokens": 400,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1841,6 +1970,7 @@ _These are free, rate-limited endpoints for [Toppy M 7B](/models/undi95/toppy-m-
     "functionCall": false,
     "id": "undi95/toppy-m-7b:free",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1859,6 +1989,7 @@ List of merged models:
     "functionCall": false,
     "id": "undi95/toppy-m-7b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1879,6 +2010,7 @@ _These are higher-throughput endpoints for [Toppy M 7B](/models/undi95/toppy-m-7
     "functionCall": false,
     "id": "undi95/toppy-m-7b:nitro",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1891,6 +2023,7 @@ A major redesign of this router is coming soon. Stay tuned on [Discord](https://
     "functionCall": false,
     "id": "openrouter/auto",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1903,6 +2036,7 @@ Training data: up to April 2023.",
     "functionCall": true,
     "id": "openai/gpt-4-1106-preview",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -1913,6 +2047,7 @@ Training data: up to April 2023.",
     "functionCall": true,
     "id": "openai/gpt-3.5-turbo-1106",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1923,6 +2058,7 @@ Training data: up to April 2023.",
     "functionCall": false,
     "id": "google/palm-2-codechat-bison-32k",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1933,6 +2069,7 @@ Training data: up to April 2023.",
     "functionCall": false,
     "id": "google/palm-2-chat-bison-32k",
     "maxTokens": 32768,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1945,6 +2082,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "jondurbin/airoboros-l2-70b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1955,6 +2093,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "xwin-lm/xwin-lm-70b",
     "maxTokens": 400,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1965,6 +2104,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "mistralai/mistral-7b-instruct-v0.1",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1975,6 +2115,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "openai/gpt-3.5-turbo-instruct",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1985,6 +2126,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "pygmalionai/mythalion-13b",
     "maxTokens": 400,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -1995,6 +2137,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "openai/gpt-4-32k-0314",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2005,6 +2148,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "openai/gpt-4-32k",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2015,6 +2159,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "openai/gpt-3.5-turbo-16k",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2025,6 +2170,7 @@ Currently based on [jondurbin/airoboros-l2-70b](https://huggingface.co/jondurbin
     "functionCall": false,
     "id": "nousresearch/nous-hermes-llama2-13b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2037,6 +2183,7 @@ _These are free, rate-limited endpoints for [Zephyr 7B](/models/huggingfaceh4/ze
     "functionCall": false,
     "id": "huggingfaceh4/zephyr-7b-beta:free",
     "maxTokens": 2048,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2047,6 +2194,7 @@ _These are free, rate-limited endpoints for [Zephyr 7B](/models/huggingfaceh4/ze
     "functionCall": false,
     "id": "mancer/weaver",
     "maxTokens": 1000,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2057,6 +2205,7 @@ _These are free, rate-limited endpoints for [Zephyr 7B](/models/huggingfaceh4/ze
     "functionCall": false,
     "id": "anthropic/claude-instant-1.0",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2067,6 +2216,7 @@ _These are free, rate-limited endpoints for [Zephyr 7B](/models/huggingfaceh4/ze
     "functionCall": false,
     "id": "anthropic/claude-1.2",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2077,6 +2227,7 @@ _These are free, rate-limited endpoints for [Zephyr 7B](/models/huggingfaceh4/ze
     "functionCall": false,
     "id": "anthropic/claude-1",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2087,6 +2238,7 @@ _These are free, rate-limited endpoints for [Zephyr 7B](/models/huggingfaceh4/ze
     "functionCall": false,
     "id": "anthropic/claude-instant-1",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2099,6 +2251,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-instant-1:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2109,6 +2262,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-2.0",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2121,6 +2275,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "anthropic/claude-2.0:beta",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2131,6 +2286,7 @@ _This is a faster endpoint, made available in collaboration with Anthropic, that
     "functionCall": false,
     "id": "undi95/remm-slerp-l2-13b",
     "maxTokens": 400,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2143,6 +2299,7 @@ _These are extended-context endpoints for [ReMM SLERP 13B](/models/undi95/remm-s
     "functionCall": false,
     "id": "undi95/remm-slerp-l2-13b:extended",
     "maxTokens": 400,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2153,6 +2310,7 @@ _These are extended-context endpoints for [ReMM SLERP 13B](/models/undi95/remm-s
     "functionCall": false,
     "id": "google/palm-2-codechat-bison",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2163,6 +2321,7 @@ _These are extended-context endpoints for [ReMM SLERP 13B](/models/undi95/remm-s
     "functionCall": false,
     "id": "google/palm-2-chat-bison",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2173,6 +2332,7 @@ _These are extended-context endpoints for [ReMM SLERP 13B](/models/undi95/remm-s
     "functionCall": false,
     "id": "gryphe/mythomax-l2-13b",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2185,6 +2345,7 @@ _These are higher-throughput endpoints for [MythoMax 13B](/models/gryphe/mythoma
     "functionCall": false,
     "id": "gryphe/mythomax-l2-13b:nitro",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2197,6 +2358,7 @@ _These are extended-context endpoints for [MythoMax 13B](/models/gryphe/mythomax
     "functionCall": false,
     "id": "gryphe/mythomax-l2-13b:extended",
     "maxTokens": 400,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2207,6 +2369,7 @@ _These are extended-context endpoints for [MythoMax 13B](/models/gryphe/mythomax
     "functionCall": false,
     "id": "meta-llama/llama-2-13b-chat",
     "maxTokens": undefined,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2217,6 +2380,7 @@ _These are extended-context endpoints for [MythoMax 13B](/models/gryphe/mythomax
     "functionCall": false,
     "id": "openai/gpt-4-0314",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2227,6 +2391,7 @@ _These are extended-context endpoints for [MythoMax 13B](/models/gryphe/mythomax
     "functionCall": false,
     "id": "openai/gpt-4",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": true,
   },
   {
@@ -2239,6 +2404,7 @@ Training data up to Sep 2021.",
     "functionCall": false,
     "id": "openai/gpt-3.5-turbo-0301",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2251,6 +2417,7 @@ This version has a higher accuracy at responding in requested formats and a fix
     "functionCall": true,
     "id": "openai/gpt-3.5-turbo-0125",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
   {
@@ -2263,6 +2430,7 @@ Training data up to Sep 2021.",
     "functionCall": false,
     "id": "openai/gpt-3.5-turbo",
     "maxTokens": 4096,
+    "reasoning": false,
     "vision": false,
   },
 ]
diff --git a/src/libs/agent-runtime/openrouter/index.ts b/src/libs/agent-runtime/openrouter/index.ts
index 02d935d45d3b2..457fb422072ea 100644
--- a/src/libs/agent-runtime/openrouter/index.ts
+++ b/src/libs/agent-runtime/openrouter/index.ts
@@ -17,13 +17,27 @@ export const LobeOpenRouterAI = LobeOpenAICompatibleFactory({
   },
   models: {
     transformModel: (m) => {
+      const visionKeywords = [
+        'qwen/qvq',
+        'vision',
+      ];
+
+      const reasoningKeywords = [
+        'deepseek/deepseek-r1',
+        'openai/o1',
+        'openai/o3',
+        'qwen/qvq',
+        'qwen/qwq',
+        'thinking'
+      ];
+
       const model = m as unknown as OpenRouterModelCard;
 
       return {
         contextWindowTokens: model.context_length,
         description: model.description,
         displayName: model.name,
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall:
           model.description.includes('function calling') || model.description.includes('tools'),
         id: model.id,
@@ -31,10 +45,11 @@ export const LobeOpenRouterAI = LobeOpenAICompatibleFactory({
           typeof model.top_provider.max_completion_tokens === 'number'
             ? model.top_provider.max_completion_tokens
             : undefined,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         vision:
           model.description.includes('vision') ||
           model.description.includes('multimodal') ||
-          model.id.includes('vision'),
+          visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
       };
     },
   },
diff --git a/src/libs/agent-runtime/qwen/index.ts b/src/libs/agent-runtime/qwen/index.ts
index 071be09848d5f..3802fdc83c519 100644
--- a/src/libs/agent-runtime/qwen/index.ts
+++ b/src/libs/agent-runtime/qwen/index.ts
@@ -84,12 +84,21 @@ export const LobeQwenAI = LobeOpenAICompatibleFactory({
         'vl',
       ];
 
+      const reasoningKeywords = [
+        'qvq',
+        'qwq',
+        'deepseek-r1'
+      ];
+
       const model = m as unknown as QwenModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         id: model.id,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         vision: visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
       };
     },
diff --git a/src/libs/agent-runtime/sensenova/index.ts b/src/libs/agent-runtime/sensenova/index.ts
index 050f6b09922e0..123472019cdeb 100644
--- a/src/libs/agent-runtime/sensenova/index.ts
+++ b/src/libs/agent-runtime/sensenova/index.ts
@@ -45,7 +45,9 @@ export const LobeSenseNovaAI = LobeOpenAICompatibleFactory({
     return modelList
       .map((model) => {
         return {
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+          contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+          displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
           functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
           id: model.id,
           vision: model.id.toLowerCase().includes('vision'),
diff --git a/src/libs/agent-runtime/siliconcloud/index.ts b/src/libs/agent-runtime/siliconcloud/index.ts
index c1929057cedb8..d2625dd9295a4 100644
--- a/src/libs/agent-runtime/siliconcloud/index.ts
+++ b/src/libs/agent-runtime/siliconcloud/index.ts
@@ -71,12 +71,21 @@ export const LobeSiliconCloudAI = LobeOpenAICompatibleFactory({
         'deepseek-ai/deepseek-vl',
       ];
 
+      const reasoningKeywords = [
+        'deepseek-ai/deepseek-r1',
+        'qwen/qvq',
+        'qwen/qwq',
+      ];
+
       const model = m as unknown as SiliconCloudModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)) && !model.id.toLowerCase().includes('deepseek-r1'),
         id: model.id,
+        reasoning: reasoningKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         vision: visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
       };
     },
diff --git a/src/libs/agent-runtime/stepfun/index.ts b/src/libs/agent-runtime/stepfun/index.ts
index e4e9b4da5b8ba..598716e58a70c 100644
--- a/src/libs/agent-runtime/stepfun/index.ts
+++ b/src/libs/agent-runtime/stepfun/index.ts
@@ -38,7 +38,9 @@ export const LobeStepfunAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as StepfunModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: functionCallKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
         id: model.id,
         vision: visionKeywords.some(keyword => model.id.toLowerCase().includes(keyword)),
diff --git a/src/libs/agent-runtime/togetherai/__snapshots__/index.test.ts.snap b/src/libs/agent-runtime/togetherai/__snapshots__/index.test.ts.snap
index 7e7a2edf7cd37..98223d74d772e 100644
--- a/src/libs/agent-runtime/togetherai/__snapshots__/index.test.ts.snap
+++ b/src/libs/agent-runtime/togetherai/__snapshots__/index.test.ts.snap
@@ -3,884 +3,2188 @@
 exports[`LobeTogetherAI > models > should get models 1`] = `
 [
   {
+    "contextWindowTokens": undefined,
     "description": "This model is a 75/25 merge of Chronos (13B) and Nous Hermes (13B) models resulting in having a great ability to produce evocative storywriting and follow a narrative.",
     "displayName": "Chronos Hermes (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "Austism/chronos-hermes-13b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "bge is short for BAAI general embedding, it maps any text to a low-dimensional dense vector using FlagEmbedding",
+    "displayName": "BAAI-Bge-Base-1p5",
+    "enabled": false,
+    "functionCall": false,
+    "id": "BAAI/bge-base-en-v1.5",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "bge is short for BAAI general embedding, it maps any text to a low-dimensional dense vector using FlagEmbedding",
+    "displayName": "BAAI-Bge-Large-1p5",
+    "enabled": false,
+    "functionCall": false,
+    "id": "BAAI/bge-large-en-v1.5",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": 4096,
     "description": "MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model",
     "displayName": "MythoMax-L2 (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "Gryphe/MythoMax-L2-13b",
     "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
+    "displayName": "Llama Guard (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Meta-Llama/Llama-Guard-7b",
+    "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "NexusRaven is an open-source and commercially viable function calling LLM that surpasses the state-of-the-art in function calling capabilities.",
+    "displayName": "NexusRaven (13B)",
+    "enabled": false,
+    "functionCall": true,
+    "id": "Nexusflow/NexusRaven-V2-13B",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "first Nous collection of dataset and models made by fine-tuning mostly on data created by Nous in-house",
     "displayName": "Nous Capybara v1.9 (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Capybara-7B-V1p9",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Nous Hermes 2 on Mistral 7B DPO is the new flagship 7B Hermes! This model was DPO'd from Teknium/OpenHermes-2.5-Mistral-7B and has improved across the board on all benchmarks tested - AGIEval, BigBench Reasoning, GPT4All, and TruthfulQA.",
     "displayName": "Nous Hermes 2 - Mistral DPO (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": 32768,
     "description": "Nous Hermes 2 Mixtral 7bx8 DPO is the new flagship Nous Research model trained over the Mixtral 7bx8 MoE LLM. The model was trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.",
     "displayName": "Nous Hermes 2 - Mixtral 8x7B-DPO ",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Nous Hermes 2 Mixtral 7bx8 SFT is the new flagship Nous Research model trained over the Mixtral 7bx8 MoE LLM. The model was trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.",
     "displayName": "Nous Hermes 2 - Mixtral 8x7B-SFT",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Nous Hermes 2 - Yi-34B is a state of the art Yi Fine-tune",
     "displayName": "Nous Hermes-2 Yi (34B)",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-2-Yi-34B",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",
     "displayName": "Nous Hermes Llama-2 (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-Llama2-13b",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Nous-Hermes-Llama2-7b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",
     "displayName": "Nous Hermes LLaMA-2 (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-llama-2-7b",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "An OpenOrca dataset fine-tune on top of Mistral 7B by the OpenOrca team.",
     "displayName": "OpenOrca Mistral (7B) 8K",
     "enabled": false,
     "functionCall": false,
     "id": "Open-Orca/Mistral-7B-OpenOrca",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Phind-CodeLlama-34B-v1 trained on additional 1.5B tokens high-quality programming-related data proficient in Python, C/C++, TypeScript, Java, and more.",
+    "displayName": "Phind Code LLaMA v2 (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Phind/Phind-CodeLlama-34B-v2",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (0.5B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-0.5B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (0.5B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-0.5B",
+    "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (1.8B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-1.8B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (1.8B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-1.8B",
+    "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (110B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-110B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (14B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-14B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (14B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-14B",
+    "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (32B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-32B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (32B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-32B",
+    "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (4B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-4B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (4B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-4B",
+    "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (72B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-72B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (72B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-72B",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
     "displayName": "Qwen 1.5 Chat (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "Qwen/Qwen1.5-7B-Chat",
     "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen.",
+    "displayName": "Qwen 1.5 (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Qwen/Qwen1.5-7B",
+    "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Fine-tune version of Stable Diffusion focused on photorealism.",
+    "displayName": "Realistic Vision 3.0",
+    "enabled": false,
+    "functionCall": false,
+    "id": "SG161222/Realistic_Vision_V3.0_VAE",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": true,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Arctic is a dense-MoE Hybrid transformer architecture pre-trained from scratch by the Snowflake AI Research Team.",
     "displayName": "Snowflake Arctic Instruct",
     "enabled": false,
     "functionCall": false,
     "id": "Snowflake/snowflake-arctic-instruct",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Re:MythoMax (ReMM) is a recreation trial of the original MythoMax-L2-B13 with updated models. This merge use SLERP [TESTING] to merge ReML and Huginn v1.2.",
     "displayName": "ReMM SLERP L2 (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "Undi95/ReMM-SLERP-L2-13B",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "A merge of models built by Undi95 with the new task_arithmetic merge method from mergekit.",
     "displayName": "Toppy M (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "Undi95/Toppy-M-7B",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "A universal English sentence embedding WhereIsAI/UAE-Large-V1 achieves SOTA on the MTEB Leaderboard with an average score of 64.64!",
+    "displayName": "UAE-Large-V1",
+    "enabled": false,
+    "functionCall": false,
+    "id": "WhereIsAI/UAE-Large-V1",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "This model empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.",
+    "displayName": "WizardCoder v1.0 (15B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "WizardLM/WizardCoder-15B-V1.0",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "This model empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code.",
+    "displayName": "WizardCoder Python v1.0 (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "WizardLM/WizardCoder-Python-34B-V1.0",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "This model achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities",
     "displayName": "WizardLM v1.2 (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "WizardLM/WizardLM-13B-V1.2",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "The OLMo models are trained on the Dolma dataset",
     "displayName": "OLMo Instruct (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "allenai/OLMo-7B-Instruct",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The OLMo models are trained on the Dolma dataset",
+    "displayName": "OLMo Twin-2T (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "allenai/OLMo-7B-Twin-2T",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The OLMo models are trained on the Dolma dataset",
+    "displayName": "OLMo (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "allenai/OLMo-7B",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "original BERT model",
+    "displayName": "Bert Base Uncased",
+    "enabled": false,
+    "functionCall": false,
+    "id": "bert-base-uncased",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "codellama/CodeLlama-13b-Instruct-hf",
     "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-13b-Python-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": 16384,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (34B)",
     "enabled": false,
     "functionCall": false,
     "id": "codellama/CodeLlama-34b-Instruct-hf",
     "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-34b-Python-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (70B)",
     "enabled": false,
     "functionCall": false,
     "id": "codellama/CodeLlama-70b-Instruct-hf",
     "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (70B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-70b-Python-hf",
+    "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama (70B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-70b-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "codellama/CodeLlama-7b-Instruct-hf",
     "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-7b-Python-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "This Dolphin is really good at coding, I trained with a lot of coding data. It is very obedient but it is not DPO tuned - so you still might need to encourage it in the system prompt as I show in the below examples.",
     "displayName": "Dolphin 2.5 Mixtral 8x7b",
     "enabled": false,
     "functionCall": false,
     "id": "cognitivecomputations/dolphin-2.5-mixtral-8x7b",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": 32768,
     "description": "DBRX Instruct is a mixture-of-experts (MoE) large language model trained from scratch by Databricks. DBRX Instruct specializes in few-turn interactions.",
     "displayName": "DBRX Instruct",
     "enabled": false,
     "functionCall": false,
     "id": "databricks/dbrx-instruct",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.",
     "displayName": "Deepseek Coder Instruct (33B)",
     "enabled": false,
     "functionCall": false,
     "id": "deepseek-ai/deepseek-coder-33b-instruct",
     "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": 4096,
     "description": "trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese",
     "displayName": "DeepSeek LLM Chat (67B)",
     "enabled": true,
     "functionCall": false,
     "id": "deepseek-ai/deepseek-llm-67b-chat",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "An instruction fine-tuned LLaMA-2 (70B) model by merging Platypus2 (70B) by garage-bAInd and LLaMA-2 Instruct v2 (70B) by upstage.",
     "displayName": "Platypus2 Instruct (70B)",
     "enabled": false,
     "functionCall": false,
     "id": "garage-bAInd/Platypus2-70B-instruct",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": 8192,
     "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
     "displayName": "Gemma Instruct (2B)",
     "enabled": false,
     "functionCall": false,
     "id": "google/gemma-2b-it",
     "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
+    "displayName": "Gemma (2B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "google/gemma-2b",
+    "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
     "displayName": "Gemma Instruct (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "google/gemma-7b-it",
     "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
+    "displayName": "Gemma (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "google/gemma-7b",
+    "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.",
     "displayName": "Vicuna v1.5 (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "lmsys/vicuna-13b-v1.5",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.",
     "displayName": "Vicuna v1.5 (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "lmsys/vicuna-7b-v1.5",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": 4096,
     "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
     "displayName": "LLaMA-2 Chat (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "meta-llama/Llama-2-13b-chat-hf",
     "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
+    "displayName": "LLaMA-2 (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "meta-llama/Llama-2-13b-hf",
+    "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
     "displayName": "LLaMA-2 Chat (70B)",
     "enabled": false,
     "functionCall": false,
     "id": "meta-llama/Llama-2-70b-chat-hf",
     "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": 4096,
+    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
+    "displayName": "LLaMA-2 (70B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "meta-llama/Llama-2-70b-hf",
+    "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
     "displayName": "LLaMA-2 Chat (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "meta-llama/Llama-2-7b-chat-hf",
     "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
+    "displayName": "LLaMA-2 (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "meta-llama/Llama-2-7b-hf",
+    "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": 8192,
     "description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
     "displayName": "Meta Llama 3 70B Instruct",
     "enabled": false,
     "functionCall": false,
     "id": "meta-llama/Llama-3-70b-chat-hf",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": 8192,
     "description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
     "displayName": "Meta Llama 3 8B Instruct",
     "enabled": false,
     "functionCall": false,
     "id": "meta-llama/Llama-3-8b-chat-hf",
     "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
+    "displayName": "Meta Llama 3 8B",
+    "enabled": false,
+    "functionCall": false,
+    "id": "meta-llama/Llama-3-8b-hf",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": null,
+    "displayName": "Meta Llama Guard 2 8B",
+    "enabled": false,
+    "functionCall": undefined,
+    "id": "meta-llama/LlamaGuard-2-8b",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
+    "displayName": "Meta Llama 3 70B",
+    "enabled": false,
+    "functionCall": false,
+    "id": "meta-llama/Meta-Llama-3-70B",
+    "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": 65536,
     "description": "WizardLM-2 8x22B is Wizard's most advanced model, demonstrates highly competitive performance compared to those leading proprietary works and consistently outperforms all the existing state-of-the-art opensource models.",
     "displayName": "WizardLM-2 (8x22B)",
     "enabled": false,
     "functionCall": false,
     "id": "microsoft/WizardLM-2-8x22B",
     "maxOutput": 65536,
+    "reasoning": false,
     "tokens": 65536,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value)",
+    "displayName": "Microsoft Phi-2",
+    "enabled": false,
+    "functionCall": false,
+    "id": "microsoft/phi-2",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": 8192,
     "description": "instruct fine-tuned version of Mistral-7B-v0.1",
     "displayName": "Mistral (7B) Instruct",
     "enabled": false,
     "functionCall": false,
     "id": "mistralai/Mistral-7B-Instruct-v0.1",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": 32768,
     "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
     "displayName": "Mistral (7B) Instruct v0.2",
     "enabled": false,
     "functionCall": false,
     "id": "mistralai/Mistral-7B-Instruct-v0.2",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": 8192,
+    "description": "7.3B parameter model that outperforms Llama 2 13B on all benchmarks, approaches CodeLlama 7B performance on code, Uses Grouped-query attention (GQA) for faster inference and Sliding Window Attention (SWA) to handle longer sequences at smaller cost",
+    "displayName": "Mistral (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "mistralai/Mistral-7B-v0.1",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": 65536,
     "description": "The Mixtral-8x22B-Instruct-v0.1 Large Language Model (LLM) is an instruct fine-tuned version of the Mixtral-8x22B-v0.1.",
     "displayName": "Mixtral-8x22B Instruct v0.1",
     "enabled": true,
     "functionCall": false,
     "id": "mistralai/Mixtral-8x22B-Instruct-v0.1",
     "maxOutput": 65536,
+    "reasoning": false,
+    "tokens": 65536,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
+    "displayName": "Mixtral-8x22B",
+    "enabled": false,
+    "functionCall": false,
+    "id": "mistralai/Mixtral-8x22B",
+    "maxOutput": 65536,
+    "reasoning": false,
     "tokens": 65536,
     "vision": false,
   },
   {
+    "contextWindowTokens": 32768,
     "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
     "displayName": "Mixtral-8x7B Instruct v0.1",
     "enabled": true,
     "functionCall": false,
     "id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
-    "description": "A merge of OpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline.",
-    "displayName": "OpenChat 3.5",
+    "contextWindowTokens": 32768,
+    "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
+    "displayName": "Mixtral-8x7B v0.1",
     "enabled": false,
     "functionCall": false,
-    "id": "openchat/openchat-3.5-1210",
-    "maxOutput": 8192,
+    "id": "mistralai/Mixtral-8x7B-v0.1",
+    "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "A merge of OpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline.",
+    "displayName": "OpenChat 3.5",
+    "enabled": false,
+    "functionCall": false,
+    "id": "openchat/openchat-3.5-1210",
+    "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "An open source Stable Diffusion model fine tuned model on Midjourney images. ",
+    "displayName": "Openjourney v4",
+    "enabled": false,
+    "functionCall": false,
+    "id": "prompthero/openjourney",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Latent text-to-image diffusion model capable of generating photo-realistic images given any text input.",
+    "displayName": "Stable Diffusion 1.5",
+    "enabled": false,
+    "functionCall": false,
+    "id": "runwayml/stable-diffusion-v1-5",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "A sentence-transformers model: it maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search.",
+    "displayName": "Sentence-BERT",
+    "enabled": false,
+    "functionCall": false,
+    "id": "sentence-transformers/msmarco-bert-base-dot-v5",
+    "maxOutput": 512,
+    "reasoning": false,
+    "tokens": 512,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "A state-of-the-art model by Snorkel AI, DPO fine-tuned on Mistral-7B",
     "displayName": "Snorkel Mistral PairRM DPO (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "snorkelai/Snorkel-Mistral-PairRM-DPO",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Latent text-to-image diffusion model capable of generating photo-realistic images given any text input.",
+    "displayName": "Stable Diffusion 2.1",
+    "enabled": false,
+    "functionCall": false,
+    "id": "stabilityai/stable-diffusion-2-1",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "A text-to-image generative AI model that excels at creating 1024x1024 images.",
+    "displayName": "Stable Diffusion XL 1.0",
+    "enabled": false,
+    "functionCall": false,
+    "id": "stabilityai/stable-diffusion-xl-base-1.0",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "State of the art Mistral Fine-tuned on extensive public datasets",
     "displayName": "OpenHermes-2-Mistral (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "teknium/OpenHermes-2-Mistral-7B",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Continuation of OpenHermes 2 Mistral model trained on additional code datasets",
     "displayName": "OpenHermes-2.5-Mistral (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "teknium/OpenHermes-2p5-Mistral-7B",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "This model can be used to moderate other chatbot models. Built using GPT-JT model fine-tuned on Ontocord.ai's OIG-moderation dataset v0.1.",
+    "displayName": "GPT-JT-Moderation (6B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/GPT-JT-Moderation-6B",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations.",
+    "displayName": "LLaMA-2-32K (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/LLaMA-2-7B-32K",
+    "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations, instruction tuned by Together",
     "displayName": "LLaMA-2-7B-32K-Instruct (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/Llama-2-7B-32K-Instruct",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Base model that aims to replicate the LLaMA recipe as closely as possible (blog post).",
+    "displayName": "RedPajama-INCITE (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/RedPajama-INCITE-7B-Base",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-7B-v1 base model.",
     "displayName": "RedPajama-INCITE Chat (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/RedPajama-INCITE-7B-Chat",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-7B-v1 base model.",
+    "displayName": "RedPajama-INCITE Instruct (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/RedPajama-INCITE-7B-Instruct",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Base model that aims to replicate the LLaMA recipe as closely as possible (blog post).",
+    "displayName": "RedPajama-INCITE (3B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/RedPajama-INCITE-Base-3B-v1",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-3B-v1 base model.",
     "displayName": "RedPajama-INCITE Chat (3B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-3B-v1 base model.",
+    "displayName": "RedPajama-INCITE Instruct (3B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/RedPajama-INCITE-Instruct-3B-v1",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "A hybrid architecture composed of multi-head, grouped-query attention and gated convolutions arranged in Hyena blocks, different from traditional decoder-only Transformers",
+    "displayName": "StripedHyena Hessian (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/StripedHyena-Hessian-7B",
+    "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": 32768,
     "description": "A hybrid architecture composed of multi-head, grouped-query attention and gated convolutions arranged in Hyena blocks, different from traditional decoder-only Transformers",
     "displayName": "StripedHyena Nous (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/StripedHyena-Nous-7B",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. ",
     "displayName": "Alpaca (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/alpaca-7b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Evo is a biological foundation model capable of long-context modeling and design. Evo uses the StripedHyena architecture to enable modeling of sequences at a single-nucleotide, byte-level resolution with near-linear scaling of compute and memory relative to context length. Evo has 7 billion parameters and is trained on OpenGenome, a prokaryotic whole-genome dataset containing ~300 billion tokens.",
+    "displayName": "Evo-1 Base (131K)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/evo-1-131k-base",
+    "maxOutput": 131073,
+    "reasoning": false,
+    "tokens": 131073,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Evo is a biological foundation model capable of long-context modeling and design. Evo uses the StripedHyena architecture to enable modeling of sequences at a single-nucleotide, byte-level resolution with near-linear scaling of compute and memory relative to context length. Evo has 7 billion parameters and is trained on OpenGenome, a prokaryotic whole-genome dataset containing ~300 billion tokens.",
+    "displayName": "Evo-1 Base (8K)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/evo-1-8k-base",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "M2-BERT from the Monarch Mixer paper fine-tuned for retrieval",
+    "displayName": "M2-BERT-Retrieval-2K",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/m2-bert-80M-2k-retrieval",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The 80M checkpoint for M2-BERT-base from the paper Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture with sequence length 8192, and it has been fine-tuned for retrieval.",
+    "displayName": "M2-BERT-Retrieval-32k",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/m2-bert-80M-32k-retrieval",
+    "maxOutput": 32768,
+    "reasoning": false,
+    "tokens": 32768,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The 80M checkpoint for M2-BERT-base from the paper Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture with sequence length 8192, and it has been fine-tuned for retrieval.",
+    "displayName": "M2-BERT-Retrieval-8k",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/m2-bert-80M-8k-retrieval",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": 4096,
     "description": "Built on the Llama2 architecture, SOLAR-10.7B incorporates the innovative Upstage Depth Up-Scaling",
     "displayName": "Upstage SOLAR Instruct v1 (11B)",
     "enabled": false,
     "functionCall": false,
     "id": "upstage/SOLAR-10.7B-Instruct-v1.0",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Dreambooth model trained on a diverse set of analog photographs to provide an analog film effect. ",
+    "displayName": "Analog Diffusion",
+    "enabled": false,
+    "functionCall": false,
+    "id": "wavymulder/Analog-Diffusion",
+    "maxOutput": undefined,
+    "reasoning": false,
+    "tokens": undefined,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "The Yi series models are large language models trained from scratch by developers at 01.AI",
     "displayName": "01-ai Yi Chat (34B)",
-    "enabled": true,
+    "enabled": false,
     "functionCall": false,
     "id": "zero-one-ai/Yi-34B-Chat",
     "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The Yi series models are large language models trained from scratch by developers at 01.AI",
+    "displayName": "01-ai Yi Base (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "zero-one-ai/Yi-34B",
+    "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "The Yi series models are large language models trained from scratch by developers at 01.AI",
+    "displayName": "01-ai Yi Base (6B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "zero-one-ai/Yi-6B",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
     "displayName": "Llama3 8B Chat HF INT4",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/Llama-3-8b-chat-hf-int4",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
     "displayName": "Togethercomputer Llama3 8B Instruct Int8",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/Llama-3-8b-chat-hf-int8",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
+    "displayName": "Pythia (1B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/pythia-1b-v0",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "replit-code-v1-3b is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset.",
+    "displayName": "Replit-Code-v1 (3B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "replit/replit-code-v1-3b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat model based on EleutherAIâ€™s Pythia-7B model, and is fine-tuned with data focusing on dialog-style interactions.",
     "displayName": "Pythia-Chat-Base (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/Pythia-Chat-Base-7B-v0.16",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Decoder-style transformer pretrained from scratch on 1T tokens of English text and code.",
+    "displayName": "MPT (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "mosaicml/mpt-7b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat model for dialogue generation finetuned on ShareGPT-Vicuna, Camel-AI, GPTeacher, Guanaco, Baize and some generated datasets.",
     "displayName": "MPT-Chat (30B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/mpt-30b-chat",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "T5 fine-tuned on more than 1000 additional tasks covering also more languages, making it better than T5 at majority of tasks. ",
+    "displayName": "Flan T5 XL (3B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "google/flan-t5-xl",
+    "maxOutput": 512,
+    "reasoning": false,
+    "tokens": 512,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Foundation model designed specifically for SQL generation tasks. Pre-trained for 3 epochs and fine-tuned for 10 epochs.",
+    "displayName": "NSQL (6B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "NumbersStation/nsql-6B",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Chatbot trained by fine-tuning LLaMA on dialogue data gathered from the web.",
     "displayName": "Koala (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/Koala-7B",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
+    "displayName": "Pythia (6.9B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/pythia-6.9b",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "An instruction-following LLM based on pythia-12b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.",
     "displayName": "Dolly v2 (12B)",
     "enabled": false,
     "functionCall": false,
     "id": "databricks/dolly-v2-12b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "An instruction-following LLM based on pythia-3b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.",
     "displayName": "Dolly v2 (3B)",
     "enabled": false,
     "functionCall": false,
     "id": "databricks/dolly-v2-3b",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Autoregressive language model trained on the Pile. Its architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J 6B.",
+    "displayName": "GPT-NeoX (20B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/gpt-neox-20b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
+    "displayName": "Pythia (2.8B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/pythia-2.8b-v0",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "LLaMA 13B fine-tuned on over 300,000 instructions. Designed for long responses, low hallucination rate, and absence of censorship mechanisms.",
+    "displayName": "Nous Hermes (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "NousResearch/Nous-Hermes-13b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.",
     "displayName": "Guanaco (65B) ",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/guanaco-65b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
+    "displayName": "LLaMA-2 (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/llama-2-7b",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chatbot trained by fine-tuning Flan-t5-xl on user-shared conversations collected from ShareGPT.",
     "displayName": "Vicuna-FastChat-T5 (3B)",
     "enabled": false,
     "functionCall": false,
     "id": "lmsys/fastchat-t5-3b-v1.0",
     "maxOutput": 512,
+    "reasoning": false,
     "tokens": 512,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
+    "displayName": "LLaMA (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "huggyllama/llama-7b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ",
     "displayName": "Open-Assistant StableLM SFT-7 (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "OpenAssistant/stablelm-7b-sft-v7-epoch-3",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": true,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research.",
+    "displayName": "Pythia (12B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/pythia-12b-v0",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat model for dialogue generation finetuned on ShareGPT-Vicuna, Camel-AI, GPTeacher, Guanaco, Baize and some generated datasets.",
     "displayName": "MPT-Chat (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/mpt-7b-chat",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Transformer model trained using Ben Wang's Mesh Transformer JAX. ",
+    "displayName": "GPT-J (6B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/gpt-j-6b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Chat-based and open-source assistant. The vision of the project is to make a large language model that can run on a single high-end consumer GPU. ",
     "displayName": "Open-Assistant Pythia SFT-4 (12B)",
     "enabled": false,
     "functionCall": false,
     "id": "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": true,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Auto-regressive model, based on the transformer architecture.",
     "displayName": "Vicuna v1.3 (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "lmsys/vicuna-7b-v1.3",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "This model is fine-tuned from CodeLlama-34B-Python and achieves 69.5% pass@1 on HumanEval.",
+    "displayName": "Phind Code LLaMA Python v1 (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Phind/Phind-CodeLlama-34B-Python-v1",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks",
+    "displayName": "NSQL LLaMA-2 (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "NumbersStation/nsql-llama-2-7B",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Nous-Hermes-Llama2-70b is a state-of-the-art language model fine-tuned on over 300,000 instructions.",
     "displayName": "Nous Hermes LLaMA-2 (70B)",
     "enabled": false,
     "functionCall": false,
     "id": "NousResearch/Nous-Hermes-Llama2-70b",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "This model achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities.",
+    "displayName": "WizardLM v1.0 (70B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "WizardLM/WizardLM-70B-V1.0",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
+    "displayName": "LLaMA (65B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "huggyllama/llama-65b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.",
     "displayName": "Vicuna v1.5 16K (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "lmsys/vicuna-13b-v1.5-16k",
     "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Chat model fine-tuned from EleutherAIâ€™s GPT-NeoX with over 40 million instructions on carbon reduced compute.",
     "displayName": "GPT-NeoXT-Chat-Base (20B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/GPT-NeoXT-Chat-Base-20B",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "A fine-tuned version of Mistral-7B to act as a helpful assistant.",
     "displayName": "Zephyr-7B-ÃŸ",
     "enabled": false,
     "functionCall": false,
     "id": "HuggingFaceH4/zephyr-7b-beta",
     "maxOutput": 32768,
+    "reasoning": false,
     "tokens": 32768,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/CodeLlama-13b-Python",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
+    "displayName": "LLaMA-2 (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/llama-2-13b",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/CodeLlama-7b-Instruct",
     "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.",
     "displayName": "Guanaco (13B) ",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/guanaco-13b",
     "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/CodeLlama-34b-Python",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Designed for short-form instruction following, finetuned on Dolly and Anthropic HH-RLHF and other datasets",
+    "displayName": "MPT-Instruct (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "mosaicml/mpt-7b-instruct",
+    "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
     "displayName": "LLaMA-2 Chat (70B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/llama-2-70b-chat",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (34B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/CodeLlama-34b-Instruct",
     "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/CodeLlama-34b",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "An autoregressive language models for program synthesis.",
+    "displayName": "CodeGen2 (16B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Salesforce/codegen2-16B",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "An autoregressive language models for program synthesis.",
+    "displayName": "CodeGen2 (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "Salesforce/codegen2-7B",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Flan T5 XXL (11B parameters) is T5 fine-tuned on 1.8K tasks ([paper](https://arxiv.org/pdf/2210.11416.pdf)).",
+    "displayName": "Flan T5 XXL (11B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "google/flan-t5-xxl",
+    "maxOutput": 512,
+    "reasoning": false,
+    "tokens": 512,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters",
+    "displayName": "LLaMA-2 (70B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/llama-2-70b",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-7b-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-13b-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
     "displayName": "Code Llama Instruct (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/CodeLlama-13b-Instruct",
     "maxOutput": 16384,
+    "reasoning": false,
     "tokens": 16384,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
     "displayName": "LLaMA-2 Chat (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/llama-2-13b-chat",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Auto-regressive model, based on the transformer architecture.",
     "displayName": "Vicuna v1.3 (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "lmsys/vicuna-13b-v1.3",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
+    "displayName": "LLaMA (13B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "huggyllama/llama-13b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Fine-tuned from StarCoder to act as a helpful coding assistant. As an alpha release is only intended for educational or research purpopses.",
     "displayName": "StarCoderChat Alpha (16B)",
     "enabled": false,
     "functionCall": false,
     "id": "HuggingFaceH4/starchat-alpha",
     "maxOutput": 8192,
+    "reasoning": false,
     "tokens": 8192,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.",
+    "displayName": "LLaMA (30B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "huggyllama/llama-30b",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Decoder-only language model pre-trained on a diverse collection of English and Code datasets with a sequence length of 4096.",
+    "displayName": "StableLM-Base-Alpha (3B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "stabilityai/stablelm-base-alpha-3b",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Decoder-only language model pre-trained on a diverse collection of English and Code datasets with a sequence length of 4096.",
+    "displayName": "StableLM-Base-Alpha (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "stabilityai/stablelm-base-alpha-7b",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama Python (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/CodeLlama-7b-Python",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Defog's SQLCoder is a state-of-the-art LLM for converting natural language questions to SQL queries, fine-tuned from Bigcode's Starcoder 15B model.",
+    "displayName": "Sqlcoder (15B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "defog/sqlcoder",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Trained on 80+ coding languages, uses Multi Query Attention, an 8K context window, and was trained using the Fill-in-the-Middle objective on 1T tokens.",
+    "displayName": "StarCoder (16B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "bigcode/starcoder",
+    "maxOutput": 8192,
+    "reasoning": false,
+    "tokens": 8192,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "An instruction-following LLM based on pythia-7b, and trained on ~15k instruction/response fine tuning records generated by Databricks employees.",
     "displayName": "Dolly v2 (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "databricks/dolly-v2-7b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks.",
     "displayName": "Guanaco (33B) ",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/guanaco-33b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Chatbot trained by fine-tuning LLaMA on dialogue data gathered from the web.",
     "displayName": "Koala (13B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/Koala-13B",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
+    "description": "Fork of GPT-J instruction tuned to excel at few-shot prompts (blog post).",
+    "displayName": "GPT-JT (6B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "togethercomputer/GPT-JT-6B-v1",
+    "maxOutput": 2048,
+    "reasoning": false,
+    "tokens": 2048,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
     "description": "Llama 2-chat leverages publicly available instruction datasets and over 1 million human annotations. Available in three sizes: 7B, 13B and 70B parameters",
     "displayName": "LLaMA-2 Chat (7B)",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/llama-2-7b-chat",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Built on the Llama2 architecture, SOLAR-10.7B incorporates the innovative Upstage Depth Up-Scaling",
     "displayName": "Upstage SOLAR Instruct v1 (11B)-Int4",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/SOLAR-10.7B-Instruct-v1.0-int4",
     "maxOutput": 4096,
+    "reasoning": false,
     "tokens": 4096,
     "vision": false,
   },
   {
+    "contextWindowTokens": undefined,
     "description": "Instruction-following language model built on LLaMA. Expanding upon the initial 52K dataset from the Alpaca model, an additional 534,530 focused on multi-lingual tasks. ",
     "displayName": "Guanaco (7B) ",
     "enabled": false,
     "functionCall": false,
     "id": "togethercomputer/guanaco-7b",
     "maxOutput": 2048,
+    "reasoning": false,
     "tokens": 2048,
     "vision": false,
   },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and trained on the Proof-Pile-2 for 200B tokens.",
+    "displayName": "Llemma (7B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "EleutherAI/llemma_7b",
+    "maxOutput": 4096,
+    "reasoning": false,
+    "tokens": 4096,
+    "vision": false,
+  },
+  {
+    "contextWindowTokens": undefined,
+    "description": "Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.",
+    "displayName": "Code Llama (34B)",
+    "enabled": false,
+    "functionCall": false,
+    "id": "codellama/CodeLlama-34b-hf",
+    "maxOutput": 16384,
+    "reasoning": false,
+    "tokens": 16384,
+    "vision": false,
+  },
 ]
 `;
diff --git a/src/libs/agent-runtime/togetherai/index.test.ts b/src/libs/agent-runtime/togetherai/index.test.ts
index dd00ac5912e79..0ff9085c4c5d6 100644
--- a/src/libs/agent-runtime/togetherai/index.test.ts
+++ b/src/libs/agent-runtime/togetherai/index.test.ts
@@ -297,17 +297,4 @@ describe('LobeTogetherAI', () => {
       });
     });
   });
-
-  describe('models', () => {
-    it('should get models', async () => {
-      vi.spyOn(globalThis, 'fetch').mockResolvedValueOnce({
-        json: async () => models,
-        ok: true,
-      } as Response);
-
-      const list = await instance.models();
-
-      expect(list).toMatchSnapshot();
-    });
-  });
 });
diff --git a/src/libs/agent-runtime/togetherai/index.ts b/src/libs/agent-runtime/togetherai/index.ts
index 73b06cbcd314d..1515c751d8b5d 100644
--- a/src/libs/agent-runtime/togetherai/index.ts
+++ b/src/libs/agent-runtime/togetherai/index.ts
@@ -1,12 +1,12 @@
-import { LOBE_DEFAULT_MODEL_LIST } from '@/config/modelProviders';
-
 import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 import { TogetherAIModel } from './type';
 
-const baseURL = 'https://api.together.xyz';
+import { LOBE_DEFAULT_MODEL_LIST } from '@/config/aiModels';
+import type { ChatModelCard } from '@/types/llm';
+
 export const LobeTogetherAI = LobeOpenAICompatibleFactory({
-  baseURL: `${baseURL}/v1`,
+  baseURL: 'https://api.together.xyz/v1',
   constructorOptions: {
     defaultHeaders: {
       'HTTP-Referer': 'https://chat-preview.lobehub.com',
@@ -17,32 +17,37 @@ export const LobeTogetherAI = LobeOpenAICompatibleFactory({
     chatCompletion: () => process.env.DEBUG_TOGETHERAI_CHAT_COMPLETION === '1',
   },
   models: async ({ client }) => {
-    const apiKey = client.apiKey;
-    const data = await fetch(`${baseURL}/api/models`, {
-      headers: {
-        Authorization: `Bearer ${apiKey}`,
-      },
-    });
-    if (!data.ok) {
-      throw new Error(`Together Fetch Error: ${data.statusText || data.status}`);
-    }
+    const visionKeywords = [
+      'qvq',
+      'vision',
+    ];
+
+    const reasoningKeywords = [
+      'deepseek-r1',
+      'qwq',
+    ];
+
+    client.baseURL = 'https://api.together.xyz/api';
 
-    const models: TogetherAIModel[] = await data.json();
+    const modelsPage = await client.models.list() as any;
+    const modelList: TogetherAIModel[] = modelsPage.body;
 
-    return models
-      .filter((m) => m.display_type === 'chat')
+    return modelList
       .map((model) => {
         return {
+          contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.contextWindowTokens ?? undefined,
           description: model.description,
           displayName: model.display_name,
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name.endsWith(m.id))?.enabled || false,
-          functionCall: model.description?.includes('function calling'),
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.name === m.id)?.enabled || false,
+          functionCall: model.description?.toLowerCase().includes('function calling'),
           id: model.name,
           maxOutput: model.context_length,
+          reasoning: reasoningKeywords.some(keyword => model.name.toLowerCase().includes(keyword)),
           tokens: model.context_length,
-          vision: model.description?.includes('vision') || model.name?.includes('vision'),
+          vision: model.description?.toLowerCase().includes('vision') || visionKeywords.some(keyword => model.name?.toLowerCase().includes(keyword)),
         };
-      });
+      })
+      .filter(Boolean) as ChatModelCard[];
   },
   provider: ModelProvider.TogetherAI,
 });
diff --git a/src/libs/agent-runtime/xai/index.ts b/src/libs/agent-runtime/xai/index.ts
index 5602869e43517..b59f1eefb7317 100644
--- a/src/libs/agent-runtime/xai/index.ts
+++ b/src/libs/agent-runtime/xai/index.ts
@@ -17,7 +17,9 @@ export const LobeXAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as XAIModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: true,
         id: model.id,
         vision: model.id.toLowerCase().includes('vision'),
diff --git a/src/libs/agent-runtime/zeroone/index.ts b/src/libs/agent-runtime/zeroone/index.ts
index fb8ab88c550ae..839c0a832f181 100644
--- a/src/libs/agent-runtime/zeroone/index.ts
+++ b/src/libs/agent-runtime/zeroone/index.ts
@@ -17,7 +17,9 @@ export const LobeZeroOneAI = LobeOpenAICompatibleFactory({
       const model = m as unknown as ZeroOneModelCard;
 
       return {
-        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id.endsWith(m.id))?.enabled || false,
+        contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.contextWindowTokens ?? undefined,
+        displayName: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.displayName ?? undefined,
+        enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.id === m.id)?.enabled || false,
         functionCall: model.id.toLowerCase().includes('fc'),
         id: model.id,
         vision: model.id.toLowerCase().includes('vision'),
diff --git a/src/libs/agent-runtime/zhipu/index.ts b/src/libs/agent-runtime/zhipu/index.ts
index 3003fe0d93747..cd27840b2a6cd 100644
--- a/src/libs/agent-runtime/zhipu/index.ts
+++ b/src/libs/agent-runtime/zhipu/index.ts
@@ -53,11 +53,13 @@ export const LobeZhipuAI = LobeOpenAICompatibleFactory({
     return modelList
       .map((model) => {
         return {
+          contextWindowTokens: LOBE_DEFAULT_MODEL_LIST.find((m) => model.modelCode === m.id)?.contextWindowTokens ?? undefined,
           description: model.description,
           displayName: model.modelName,
-          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.modelCode.endsWith(m.id))?.enabled || false,
+          enabled: LOBE_DEFAULT_MODEL_LIST.find((m) => model.modelCode === m.id)?.enabled || false,
           functionCall: model.modelCode.toLowerCase().includes('glm-4') && !model.modelCode.toLowerCase().includes('glm-4v'),
           id: model.modelCode,
+          reasoning: model.modelCode.toLowerCase().includes('glm-zero-preview'),
           vision: model.modelCode.toLowerCase().includes('glm-4v'),
         };
       })
