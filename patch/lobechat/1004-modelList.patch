diff --git a/src/const/settings/llm.ts b/src/const/settings/llm.ts
index 4aa0edb245e8..839563c93829 100644
--- a/src/const/settings/llm.ts
+++ b/src/const/settings/llm.ts
@@ -1,180 +1,8 @@
-import {
-  Ai21ProviderCard,
-  Ai360ProviderCard,
-  AnthropicProviderCard,
-  BaichuanProviderCard,
-  BedrockProviderCard,
-  CloudflareProviderCard,
-  DeepSeekProviderCard,
-  FireworksAIProviderCard,
-  GithubProviderCard,
-  GoogleProviderCard,
-  GroqProviderCard,
-  HuggingFaceProviderCard,
-  HunyuanProviderCard,
-  MinimaxProviderCard,
-  MistralProviderCard,
-  MoonshotProviderCard,
-  NovitaProviderCard,
-  OllamaProviderCard,
-  OpenAIProviderCard,
-  OpenRouterProviderCard,
-  PerplexityProviderCard,
-  QwenProviderCard,
-  SenseNovaProviderCard,
-  SiliconCloudProviderCard,
-  SparkProviderCard,
-  StepfunProviderCard,
-  TaichuProviderCard,
-  TogetherAIProviderCard,
-  UpstageProviderCard,
-  WenxinProviderCard,
-  XAIProviderCard,
-  ZeroOneProviderCard,
-  ZhiPuProviderCard,
-  filterEnabledModels,
-} from '@/config/modelProviders';
 import { ModelProvider } from '@/libs/agent-runtime';
-import { UserModelProviderConfig } from '@/types/user/settings';
 
-export const DEFAULT_LLM_CONFIG: UserModelProviderConfig = {
-  ai21: {
-    enabled: false,
-    enabledModels: filterEnabledModels(Ai21ProviderCard),
-  },
-  ai360: {
-    enabled: false,
-    enabledModels: filterEnabledModels(Ai360ProviderCard),
-  },
-  anthropic: {
-    enabled: false,
-    enabledModels: filterEnabledModels(AnthropicProviderCard),
-  },
-  azure: {
-    enabled: false,
-  },
-  baichuan: {
-    enabled: false,
-    enabledModels: filterEnabledModels(BaichuanProviderCard),
-  },
-  bedrock: {
-    enabled: false,
-    enabledModels: filterEnabledModels(BedrockProviderCard),
-  },
-  cloudflare: {
-    enabled: false,
-    enabledModels: filterEnabledModels(CloudflareProviderCard),
-  },
-  deepseek: {
-    enabled: false,
-    enabledModels: filterEnabledModels(DeepSeekProviderCard),
-  },
-  fireworksai: {
-    enabled: false,
-    enabledModels: filterEnabledModels(FireworksAIProviderCard),
-  },
-  github: {
-    enabled: false,
-    enabledModels: filterEnabledModels(GithubProviderCard),
-  },
-  google: {
-    enabled: false,
-    enabledModels: filterEnabledModels(GoogleProviderCard),
-  },
-  groq: {
-    enabled: false,
-    enabledModels: filterEnabledModels(GroqProviderCard),
-  },
-  huggingface: {
-    enabled: false,
-    enabledModels: filterEnabledModels(HuggingFaceProviderCard),
-  },
-  hunyuan: {
-    enabled: false,
-    enabledModels: filterEnabledModels(HunyuanProviderCard),
-  },
-  minimax: {
-    enabled: false,
-    enabledModels: filterEnabledModels(MinimaxProviderCard),
-  },
-  mistral: {
-    enabled: false,
-    enabledModels: filterEnabledModels(MistralProviderCard),
-  },
-  moonshot: {
-    enabled: false,
-    enabledModels: filterEnabledModels(MoonshotProviderCard),
-  },
-  novita: {
-    enabled: false,
-    enabledModels: filterEnabledModels(NovitaProviderCard),
-  },
-  ollama: {
-    enabled: true,
-    enabledModels: filterEnabledModels(OllamaProviderCard),
-    fetchOnClient: true,
-  },
-  openai: {
-    enabled: true,
-    enabledModels: filterEnabledModels(OpenAIProviderCard),
-  },
-  openrouter: {
-    enabled: false,
-    enabledModels: filterEnabledModels(OpenRouterProviderCard),
-  },
-  perplexity: {
-    enabled: false,
-    enabledModels: filterEnabledModels(PerplexityProviderCard),
-  },
-  qwen: {
-    enabled: false,
-    enabledModels: filterEnabledModels(QwenProviderCard),
-  },
-  sensenova: {
-    enabled: false,
-    enabledModels: filterEnabledModels(SenseNovaProviderCard),
-  },
-  siliconcloud: {
-    enabled: false,
-    enabledModels: filterEnabledModels(SiliconCloudProviderCard),
-  },
-  spark: {
-    enabled: false,
-    enabledModels: filterEnabledModels(SparkProviderCard),
-  },
-  stepfun: {
-    enabled: false,
-    enabledModels: filterEnabledModels(StepfunProviderCard),
-  },
-  taichu: {
-    enabled: false,
-    enabledModels: filterEnabledModels(TaichuProviderCard),
-  },
-  togetherai: {
-    enabled: false,
-    enabledModels: filterEnabledModels(TogetherAIProviderCard),
-  },
-  upstage: {
-    enabled: false,
-    enabledModels: filterEnabledModels(UpstageProviderCard),
-  },
-  wenxin: {
-    enabled: false,
-    enabledModels: filterEnabledModels(WenxinProviderCard),
-  },
-  xai: {
-    enabled: false,
-    enabledModels: filterEnabledModels(XAIProviderCard),
-  },
-  zeroone: {
-    enabled: false,
-    enabledModels: filterEnabledModels(ZeroOneProviderCard),
-  },
-  zhipu: {
-    enabled: false,
-    enabledModels: filterEnabledModels(ZhiPuProviderCard),
-  },
-};
+import { genUserLLMConfig } from '@/utils/genLLMConfig'
+
+export const DEFAULT_LLM_CONFIG = genUserLLMConfig();
 
 export const DEFAULT_MODEL = 'gpt-4o-mini';
 export const DEFAULT_EMBEDDING_MODEL = 'text-embedding-3-small';
diff --git a/src/server/globalConfig/index.ts b/src/server/globalConfig/index.ts
index 6fec71178d7b..996bbca20dab 100644
--- a/src/server/globalConfig/index.ts
+++ b/src/server/globalConfig/index.ts
@@ -2,156 +2,17 @@ import { appEnv, getAppConfig } from '@/config/app';
 import { authEnv } from '@/config/auth';
 import { fileEnv } from '@/config/file';
 import { langfuseEnv } from '@/config/langfuse';
-import { getLLMConfig } from '@/config/llm';
-import {
-  Ai21ProviderCard,
-  Ai360ProviderCard,
-  AnthropicProviderCard,
-  BaichuanProviderCard,
-  BedrockProviderCard,
-  DeepSeekProviderCard,
-  FireworksAIProviderCard,
-  GithubProviderCard,
-  GoogleProviderCard,
-  GroqProviderCard,
-  HuggingFaceProviderCard,
-  HunyuanProviderCard,
-  MinimaxProviderCard,
-  MistralProviderCard,
-  MoonshotProviderCard,
-  NovitaProviderCard,
-  OllamaProviderCard,
-  OpenAIProviderCard,
-  OpenRouterProviderCard,
-  PerplexityProviderCard,
-  QwenProviderCard,
-  SenseNovaProviderCard,
-  SiliconCloudProviderCard,
-  SparkProviderCard,
-  StepfunProviderCard,
-  TaichuProviderCard,
-  TogetherAIProviderCard,
-  UpstageProviderCard,
-  WenxinProviderCard,
-  XAIProviderCard,
-  ZeroOneProviderCard,
-  ZhiPuProviderCard,
-} from '@/config/modelProviders';
 import { enableNextAuth } from '@/const/auth';
 import { parseSystemAgent } from '@/server/globalConfig/parseSystemAgent';
 import { GlobalServerConfig } from '@/types/serverConfig';
-import { extractEnabledModels, transformToChatModelCards } from '@/utils/parseModels';
 
 import { parseAgentConfig } from './parseDefaultAgent';
 
+import { genServerLLMConfig } from '@/utils/genLLMConfig'
+
 export const getServerGlobalConfig = () => {
   const { ACCESS_CODES, DEFAULT_AGENT_CONFIG } = getAppConfig();
 
-  const {
-    ENABLED_OPENAI,
-    OPENAI_MODEL_LIST,
-
-    ENABLED_MOONSHOT,
-    MOONSHOT_MODEL_LIST,
-
-    ENABLED_ZHIPU,
-    ZHIPU_MODEL_LIST,
-
-    ENABLED_AWS_BEDROCK,
-    AWS_BEDROCK_MODEL_LIST,
-
-    ENABLED_GOOGLE,
-    GOOGLE_MODEL_LIST,
-
-    ENABLED_GROQ,
-    GROQ_MODEL_LIST,
-
-    ENABLED_GITHUB,
-    GITHUB_MODEL_LIST,
-
-    ENABLED_HUNYUAN,
-    HUNYUAN_MODEL_LIST,
-
-    ENABLED_DEEPSEEK,
-    DEEPSEEK_MODEL_LIST,
-
-    ENABLED_PERPLEXITY,
-    PERPLEXITY_MODEL_LIST,
-
-    ENABLED_ANTHROPIC,
-    ANTHROPIC_MODEL_LIST,
-
-    ENABLED_MINIMAX,
-    MINIMAX_MODEL_LIST,
-
-    ENABLED_MISTRAL,
-    MISTRAL_MODEL_LIST,
-
-    ENABLED_NOVITA,
-    NOVITA_MODEL_LIST,
-
-    ENABLED_QWEN,
-    QWEN_MODEL_LIST,
-
-    ENABLED_STEPFUN,
-    STEPFUN_MODEL_LIST,
-
-    ENABLED_BAICHUAN,
-    BAICHUAN_MODEL_LIST,
-
-    ENABLED_TAICHU,
-
-    ENABLED_CLOUDFLARE,
-
-    TAICHU_MODEL_LIST,
-
-    ENABLED_AI21,
-    AI21_MODEL_LIST,
-
-    ENABLED_AI360,
-    AI360_MODEL_LIST,
-
-    ENABLED_SENSENOVA,
-    SENSENOVA_MODEL_LIST,
-
-    ENABLED_SILICONCLOUD,
-    SILICONCLOUD_MODEL_LIST,
-
-    ENABLED_UPSTAGE,
-    UPSTAGE_MODEL_LIST,
-
-    ENABLED_SPARK,
-    SPARK_MODEL_LIST,
-
-    ENABLED_AZURE_OPENAI,
-    AZURE_MODEL_LIST,
-
-    ENABLED_OLLAMA,
-    OLLAMA_MODEL_LIST,
-    OLLAMA_PROXY_URL,
-
-    ENABLED_OPENROUTER,
-    OPENROUTER_MODEL_LIST,
-
-    ENABLED_ZEROONE,
-    ZEROONE_MODEL_LIST,
-
-    ENABLED_TOGETHERAI,
-    TOGETHERAI_MODEL_LIST,
-
-    ENABLED_FIREWORKSAI,
-    FIREWORKSAI_MODEL_LIST,
-
-    ENABLED_WENXIN,
-    WENXIN_MODEL_LIST,
-
-    ENABLED_HUGGINGFACE,
-    HUGGINGFACE_MODEL_LIST,
-
-    ENABLED_XAI,
-    XAI_MODEL_LIST,
-  } = getLLMConfig();
-
   const config: GlobalServerConfig = {
     defaultAgent: {
       config: parseAgentConfig(DEFAULT_AGENT_CONFIG),
@@ -159,275 +20,7 @@ export const getServerGlobalConfig = () => {
     enableUploadFileToServer: !!fileEnv.S3_SECRET_ACCESS_KEY,
     enabledAccessCode: ACCESS_CODES?.length > 0,
     enabledOAuthSSO: enableNextAuth,
-    languageModel: {
-      ai21: {
-        enabled: ENABLED_AI21,
-        enabledModels: extractEnabledModels(AI21_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: Ai21ProviderCard.chatModels,
-          modelString: AI21_MODEL_LIST,
-        }),
-      },
-      ai360: {
-        enabled: ENABLED_AI360,
-        enabledModels: extractEnabledModels(AI360_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: Ai360ProviderCard.chatModels,
-          modelString: AI360_MODEL_LIST,
-        }),
-      },
-      anthropic: {
-        enabled: ENABLED_ANTHROPIC,
-        enabledModels: extractEnabledModels(ANTHROPIC_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: AnthropicProviderCard.chatModels,
-          modelString: ANTHROPIC_MODEL_LIST,
-        }),
-      },
-      azure: {
-        enabled: ENABLED_AZURE_OPENAI,
-        enabledModels: extractEnabledModels(AZURE_MODEL_LIST, true),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: [],
-          modelString: AZURE_MODEL_LIST,
-          withDeploymentName: true,
-        }),
-      },
-      baichuan: {
-        enabled: ENABLED_BAICHUAN,
-        enabledModels: extractEnabledModels(BAICHUAN_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: BaichuanProviderCard.chatModels,
-          modelString: BAICHUAN_MODEL_LIST,
-        }),
-      },
-      bedrock: {
-        enabled: ENABLED_AWS_BEDROCK,
-        enabledModels: extractEnabledModels(AWS_BEDROCK_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: BedrockProviderCard.chatModels,
-          modelString: AWS_BEDROCK_MODEL_LIST,
-        }),
-      },
-      cloudflare: { enabled: ENABLED_CLOUDFLARE },
-      deepseek: {
-        enabled: ENABLED_DEEPSEEK,
-        enabledModels: extractEnabledModels(DEEPSEEK_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: DeepSeekProviderCard.chatModels,
-          modelString: DEEPSEEK_MODEL_LIST,
-        }),
-      },
-      fireworksai: {
-        enabled: ENABLED_FIREWORKSAI,
-        enabledModels: extractEnabledModels(FIREWORKSAI_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: FireworksAIProviderCard.chatModels,
-          modelString: FIREWORKSAI_MODEL_LIST,
-        }),
-      },
-      github: {
-        enabled: ENABLED_GITHUB,
-        enabledModels: extractEnabledModels(GITHUB_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: GithubProviderCard.chatModels,
-          modelString: GITHUB_MODEL_LIST,
-        }),
-      },
-      google: {
-        enabled: ENABLED_GOOGLE,
-        enabledModels: extractEnabledModels(GOOGLE_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: GoogleProviderCard.chatModels,
-          modelString: GOOGLE_MODEL_LIST,
-        }),
-      },
-      groq: {
-        enabled: ENABLED_GROQ,
-        enabledModels: extractEnabledModels(GROQ_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: GroqProviderCard.chatModels,
-          modelString: GROQ_MODEL_LIST,
-        }),
-      },
-      huggingface: {
-        enabled: ENABLED_HUGGINGFACE,
-        enabledModels: extractEnabledModels(HUGGINGFACE_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: HuggingFaceProviderCard.chatModels,
-          modelString: HUGGINGFACE_MODEL_LIST,
-        }),
-      },
-      hunyuan: {
-        enabled: ENABLED_HUNYUAN,
-        enabledModels: extractEnabledModels(HUNYUAN_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: HunyuanProviderCard.chatModels,
-          modelString: HUNYUAN_MODEL_LIST,
-        }),
-      },
-      minimax: {
-        enabled: ENABLED_MINIMAX,
-        enabledModels: extractEnabledModels(MINIMAX_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: MinimaxProviderCard.chatModels,
-          modelString: MINIMAX_MODEL_LIST,
-        }),
-      },
-      mistral: {
-        enabled: ENABLED_MISTRAL,
-        enabledModels: extractEnabledModels(MISTRAL_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: MistralProviderCard.chatModels,
-          modelString: MISTRAL_MODEL_LIST,
-        }),
-      },
-      moonshot: {
-        enabled: ENABLED_MOONSHOT,
-        enabledModels: extractEnabledModels(MOONSHOT_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: MoonshotProviderCard.chatModels,
-          modelString: MOONSHOT_MODEL_LIST,
-        }),
-      },
-      novita: {
-        enabled: ENABLED_NOVITA,
-        enabledModels: extractEnabledModels(NOVITA_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: NovitaProviderCard.chatModels,
-          modelString: NOVITA_MODEL_LIST,
-        }),
-      },
-      ollama: {
-        enabled: ENABLED_OLLAMA,
-        enabledModels: extractEnabledModels(OLLAMA_MODEL_LIST),
-        fetchOnClient: !OLLAMA_PROXY_URL,
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: OllamaProviderCard.chatModels,
-          modelString: OLLAMA_MODEL_LIST,
-        }),
-      },
-      openai: {
-        enabled: ENABLED_OPENAI,
-        enabledModels: extractEnabledModels(OPENAI_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: OpenAIProviderCard.chatModels,
-          modelString: OPENAI_MODEL_LIST,
-        }),
-      },
-      openrouter: {
-        enabled: ENABLED_OPENROUTER,
-        enabledModels: extractEnabledModels(OPENROUTER_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: OpenRouterProviderCard.chatModels,
-          modelString: OPENROUTER_MODEL_LIST,
-        }),
-      },
-      perplexity: {
-        enabled: ENABLED_PERPLEXITY,
-        enabledModels: extractEnabledModels(PERPLEXITY_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: PerplexityProviderCard.chatModels,
-          modelString: PERPLEXITY_MODEL_LIST,
-        }),
-      },
-      qwen: {
-        enabled: ENABLED_QWEN,
-        enabledModels: extractEnabledModels(QWEN_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: QwenProviderCard.chatModels,
-          modelString: QWEN_MODEL_LIST,
-        }),
-      },
-      sensenova: {
-        enabled: ENABLED_SENSENOVA,
-        enabledModels: extractEnabledModels(SENSENOVA_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: SenseNovaProviderCard.chatModels,
-          modelString: SENSENOVA_MODEL_LIST,
-        }),
-      },
-      siliconcloud: {
-        enabled: ENABLED_SILICONCLOUD,
-        enabledModels: extractEnabledModels(SILICONCLOUD_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: SiliconCloudProviderCard.chatModels,
-          modelString: SILICONCLOUD_MODEL_LIST,
-        }),
-      },
-      spark: {
-        enabled: ENABLED_SPARK,
-        enabledModels: extractEnabledModels(SPARK_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: SparkProviderCard.chatModels,
-          modelString: SPARK_MODEL_LIST,
-        }),
-      },
-      stepfun: {
-        enabled: ENABLED_STEPFUN,
-        enabledModels: extractEnabledModels(STEPFUN_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: StepfunProviderCard.chatModels,
-          modelString: STEPFUN_MODEL_LIST,
-        }),
-      },
-      taichu: {
-        enabled: ENABLED_TAICHU,
-        enabledModels: extractEnabledModels(TAICHU_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: TaichuProviderCard.chatModels,
-          modelString: TAICHU_MODEL_LIST,
-        }),
-      },
-      togetherai: {
-        enabled: ENABLED_TOGETHERAI,
-        enabledModels: extractEnabledModels(TOGETHERAI_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: TogetherAIProviderCard.chatModels,
-          modelString: TOGETHERAI_MODEL_LIST,
-        }),
-      },
-      upstage: {
-        enabled: ENABLED_UPSTAGE,
-        enabledModels: extractEnabledModels(UPSTAGE_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: UpstageProviderCard.chatModels,
-          modelString: UPSTAGE_MODEL_LIST,
-        }),
-      },
-      wenxin: {
-        enabled: ENABLED_WENXIN,
-        enabledModels: extractEnabledModels(WENXIN_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: WenxinProviderCard.chatModels,
-          modelString: WENXIN_MODEL_LIST,
-        }),
-      },
-      xai: {
-        enabled: ENABLED_XAI,
-        enabledModels: extractEnabledModels(XAI_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: XAIProviderCard.chatModels,
-          modelString: XAI_MODEL_LIST,
-        }),
-      },
-      zeroone: {
-        enabled: ENABLED_ZEROONE,
-        enabledModels: extractEnabledModels(ZEROONE_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: ZeroOneProviderCard.chatModels,
-          modelString: ZEROONE_MODEL_LIST,
-        }),
-      },
-      zhipu: {
-        enabled: ENABLED_ZHIPU,
-        enabledModels: extractEnabledModels(ZHIPU_MODEL_LIST),
-        serverModelCards: transformToChatModelCards({
-          defaultChatModels: ZhiPuProviderCard.chatModels,
-          modelString: ZHIPU_MODEL_LIST,
-        }),
-      },
-    },
+    languageModel: genServerLLMConfig(),
     oAuthSSOProviders: authEnv.NEXT_AUTH_SSO_PROVIDERS.trim().split(/[,，]/),
     systemAgent: parseSystemAgent(appEnv.SYSTEM_AGENT),
     telemetry: {
diff --git a/src/utils/genLLMConfig.test.ts b/src/utils/genLLMConfig.test.ts
new file mode 100644
index 000000000000..226e5a6d03b7
--- /dev/null
+++ b/src/utils/genLLMConfig.test.ts
@@ -0,0 +1,75 @@
+import { describe, expect, it, vi } from 'vitest';
+
+import { genServerLLMConfig } from './genLLMConfig';
+
+// Mock ModelProvider enum
+vi.mock('@/libs/agent-runtime', () => ({
+  ModelProvider: {
+    Azure: 'azure',
+    Bedrock: 'bedrock',
+    Ollama: 'ollama',
+  }
+}));
+
+// Mock ProviderCards
+vi.mock('@/config/modelProviders', () => ({
+  azureProviderCard: {
+    chatModels: [],
+  },
+  bedrockProviderCard: {
+    chatModels: ['bedrockModel1', 'bedrockModel2'],
+  },
+  ollamaProviderCard: {
+    chatModels: ['ollamaModel1', 'ollamaModel2'],
+  },
+}));
+
+// Mock LLM config
+vi.mock('@/config/llm', () => ({
+  getLLMConfig: () => ({
+    ENABLED_AZURE_OPENAI: true,
+    ENABLED_AWS_BEDROCK: true,
+    ENABLED_OLLAMA: true,
+    AZURE_MODEL_LIST: 'azureModels',
+    AWS_BEDROCK_MODEL_LIST: 'bedrockModels',
+    OLLAMA_MODEL_LIST: 'ollamaModels',
+    OLLAMA_PROXY_URL: '',
+  }),
+}));
+
+// Mock parse models utils
+vi.mock('@/utils/parseModels', () => ({
+  extractEnabledModels: (modelString: string, withDeploymentName?: boolean) => {
+    // Returns different format if withDeploymentName is true
+    return withDeploymentName ? [`${modelString}_withDeployment`] : [modelString];
+  },
+  transformToChatModelCards: ({ defaultChatModels, modelString, withDeploymentName }: any) => {
+    // Simulate transformation based on withDeploymentName
+    return withDeploymentName ? [`${modelString}_transformed`] : defaultChatModels;
+  },
+}));
+
+describe('genServerLLMConfig', () => {
+  it('should generate correct LLM config for Azure, Bedrock, and Ollama', () => {
+    const config = genServerLLMConfig();
+    
+    expect(config.azure).toEqual({
+      enabled: true,
+      enabledModels: ['azureModels_withDeployment'],
+      serverModelCards: ['azureModels_transformed'],
+    });
+
+    expect(config.bedrock).toEqual({
+      enabled: true,
+      enabledModels: ['bedrockModels'],
+      serverModelCards: ['bedrockModel1', 'bedrockModel2'],
+    });
+
+    expect(config.ollama).toEqual({
+      enabled: true,
+      enabledModels: ['ollamaModels'], 
+      fetchOnClient: true,
+      serverModelCards: ['ollamaModel1', 'ollamaModel2'],
+    });
+  });
+});
diff --git a/src/utils/genLLMConfig.ts b/src/utils/genLLMConfig.ts
new file mode 100644
index 000000000000..26503ba07152
--- /dev/null
+++ b/src/utils/genLLMConfig.ts
@@ -0,0 +1,75 @@
+import { getLLMConfig } from '@/config/llm';
+import * as ProviderCards from '@/config/modelProviders';
+
+import { ModelProvider } from '@/libs/agent-runtime';
+
+import { extractEnabledModels, transformToChatModelCards } from '@/utils/parseModels';
+
+import { ModelProviderCard } from '@/types/llm';
+import { UserModelProviderConfig } from '@/types/user/settings';
+
+export const genServerLLMConfig = () => {
+  const llmConfig = getLLMConfig() as Record<string, any>;
+
+  const specificConfig: Record<any, any> = {
+    azure: {
+      enabledKey: 'ENABLED_AZURE_OPENAI',
+      withDeploymentName: true,
+    },
+    bedrock: {
+      enabledKey: 'ENABLED_AWS_BEDROCK',
+      modelListKey: 'AWS_BEDROCK_MODEL_LIST',
+    },
+    ollama: {
+      fetchOnClient: !llmConfig.OLLAMA_PROXY_URL,
+    },
+  };
+
+  return Object.values(ModelProvider).reduce((config, provider) => {
+    const providerUpperCase = provider.toUpperCase();
+    const providerCard = ProviderCards[`${provider}ProviderCard` as keyof typeof ProviderCards] as ModelProviderCard;
+    const providerConfig = specificConfig[provider as keyof typeof specificConfig] || {};
+
+    config[provider] = {
+      enabled: llmConfig[providerConfig.enabledKey || `ENABLED_${providerUpperCase}`],
+      enabledModels: extractEnabledModels(
+        llmConfig[providerConfig.modelListKey || `${providerUpperCase}_MODEL_LIST`],
+        providerConfig.withDeploymentName || false,
+      ),
+      serverModelCards: transformToChatModelCards({
+        defaultChatModels: (providerCard as ModelProviderCard)?.chatModels || [],
+        modelString: llmConfig[providerConfig.modelListKey || `${providerUpperCase}_MODEL_LIST`],
+        withDeploymentName: providerConfig.withDeploymentName || false,
+      }),
+      ...(providerConfig.fetchOnClient !== undefined && { fetchOnClient: providerConfig.fetchOnClient }),
+    };
+
+    return config;
+  }, {} as Record<ModelProvider, any>);
+};
+
+export const genUserLLMConfig = (): UserModelProviderConfig => {
+  const specificConfig: Record<any, any> = {
+    ollama: {
+      enabled: true,
+      fetchOnClient: true,
+    },
+    openai: {
+      enabled: true,
+    },
+  };
+
+  return Object.keys(ModelProvider).reduce((config, providerKey) => {
+    const provider = ModelProvider[providerKey as keyof typeof ModelProvider];
+    const providerCard = ProviderCards[`${providerKey}ProviderCard` as keyof typeof ProviderCards] as ModelProviderCard;
+    const providerConfig = specificConfig[provider as keyof typeof specificConfig] || {};
+
+    config[provider] = {
+      enabled: providerConfig.enabled !== undefined ? providerConfig.enabled : false,
+      enabledModels: providerCard ? ProviderCards.filterEnabledModels(providerCard) : [],
+      ...(providerConfig.fetchOnClient !== undefined && { fetchOnClient: providerConfig.fetchOnClient }),
+    };
+
+    return config;
+  }, {} as UserModelProviderConfig);
+};
