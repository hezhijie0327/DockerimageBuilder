diff --git a/src/config/aiModels/ai360.ts b/src/config/aiModels/ai360.ts
index 10c6613306348..d4d22dceee25f 100644
--- a/src/config/aiModels/ai360.ts
+++ b/src/config/aiModels/ai360.ts
@@ -52,6 +52,7 @@ const ai360ChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8000,
     description:
@@ -64,6 +65,9 @@ const ai360ChatModels: AIChatModelCard[] = [
       input: 2,
       output: 5,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/config/aiModels/baichuan.ts b/src/config/aiModels/baichuan.ts
index 5139377289bc0..b62f2081e9155 100644
--- a/src/config/aiModels/baichuan.ts
+++ b/src/config/aiModels/baichuan.ts
@@ -4,6 +4,7 @@ const baichuanChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_768,
     description:
@@ -17,11 +18,15 @@ const baichuanChatModels: AIChatModelCard[] = [
       input: 100,
       output: 100,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_768,
     description:
@@ -35,11 +40,15 @@ const baichuanChatModels: AIChatModelCard[] = [
       input: 15,
       output: 15,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_768,
     description:
@@ -53,11 +62,15 @@ const baichuanChatModels: AIChatModelCard[] = [
       input: 0.98,
       output: 0.98,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_768,
     description:
@@ -70,6 +83,9 @@ const baichuanChatModels: AIChatModelCard[] = [
       input: 12,
       output: 12,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/config/aiModels/hunyuan.ts b/src/config/aiModels/hunyuan.ts
index 6ba991859be4b..3ab62c0023b02 100644
--- a/src/config/aiModels/hunyuan.ts
+++ b/src/config/aiModels/hunyuan.ts
@@ -20,6 +20,9 @@ const hunyuanChatModels: AIChatModelCard[] = [
     type: 'chat',
   },
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 32_000,
     description:
       '采用更优的路由策略，同时缓解了负载均衡和专家趋同的问题。长文方面，大海捞针指标达到99.9%。MOE-32K 性价比相对更高，在平衡效果、价格的同时，可对实现对长文本输入的处理。',
@@ -33,9 +36,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 2,
     },
     releasedAt: '2025-02-10',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 256_000,
     description:
       '采用更优的路由策略，同时缓解了负载均衡和专家趋同的问题。长文方面，大海捞针指标达到99.9%。MOE-256K 在长度和效果上进一步突破，极大的扩展了可输入长度。',
@@ -49,11 +58,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 2,
     },
     releasedAt: '2025-02-10',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_000,
     description:
@@ -68,11 +81,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 50,
     },
     releasedAt: '2025-01-10',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_000,
     description:
@@ -86,11 +103,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 50,
     },
     releasedAt: '2025-01-10',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_000,
     description:
@@ -104,11 +125,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 50,
     },
     releasedAt: '2025-01-10',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_000,
     description:
@@ -122,9 +147,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 50,
     },
     releasedAt: '2024-11-20',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 32_000,
     description:
       'Hunyuan-large 模型总参数量约 389B，激活参数量约 52B，是当前业界参数规模最大、效果最好的 Transformer 架构的开源 MoE 模型。',
@@ -138,9 +169,15 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 12,
     },
     releasedAt: '2025-02-10',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 134_000,
     description:
       '擅长处理长文任务如文档摘要和文档问答等，同时也具备处理通用文本生成任务的能力。在长文本的分析和生成上表现优异，能有效应对复杂和详尽的长文内容处理需求。',
@@ -154,6 +191,9 @@ const hunyuanChatModels: AIChatModelCard[] = [
       output: 18,
     },
     releasedAt: '2024-12-18',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/config/aiModels/moonshot.ts b/src/config/aiModels/moonshot.ts
index 4f1bc3b7c5d24..d9be341bf7da5 100644
--- a/src/config/aiModels/moonshot.ts
+++ b/src/config/aiModels/moonshot.ts
@@ -4,6 +4,7 @@ const moonshotChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 131_072,
@@ -19,11 +20,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       output: 60,
     },
     releasedAt: '2025-02-17',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 131_072,
     description:
@@ -35,11 +40,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       input: 60,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -51,11 +60,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       input: 12,
       output: 12,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_768,
     description:
@@ -67,11 +80,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       input: 24,
       output: 24,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 131_072,
     description:
@@ -83,11 +100,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       input: 60,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 8192,
@@ -101,11 +122,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       output: 12,
     },
     releasedAt: '2025-01-14',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 32_768,
@@ -119,11 +144,15 @@ const moonshotChatModels: AIChatModelCard[] = [
       output: 24,
     },
     releasedAt: '2025-01-14',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 131_072,
@@ -137,6 +166,9 @@ const moonshotChatModels: AIChatModelCard[] = [
       output: 60,
     },
     releasedAt: '2025-01-14',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
 ];
diff --git a/src/config/aiModels/spark.ts b/src/config/aiModels/spark.ts
index c31a9b8b583fc..99f1990d858e4 100644
--- a/src/config/aiModels/spark.ts
+++ b/src/config/aiModels/spark.ts
@@ -2,6 +2,9 @@ import { AIChatModelCard } from '@/types/aiModel';
 
 const sparkChatModels: AIChatModelCard[] = [
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 8192,
     description:
       'Spark Lite 是一款轻量级大语言模型，具备极低的延迟与高效的处理能力，完全免费开放，支持实时在线搜索功能。其快速响应的特性使其在低算力设备上的推理应用和模型微调中表现出色，为用户带来出色的成本效益和智能体验，尤其在知识问答、内容生成及搜索场景下表现不俗。',
@@ -9,9 +12,15 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'lite',
     maxOutput: 4096,
+    settings: {
+      searchImpl: 'internal',
+    },
     type: 'chat',
   },
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 8192,
     description:
       'Spark Pro 是一款为专业领域优化的高性能大语言模型，专注数学、编程、医疗、教育等多个领域，并支持联网搜索及内置天气、日期等插件。其优化后模型在复杂知识问答、语言理解及高层次文本创作中展现出色表现和高效性能，是适合专业应用场景的理想选择。',
@@ -19,9 +28,15 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'generalv3',
     maxOutput: 8192,
+    settings: {
+      searchImpl: 'internal',
+    },
     type: 'chat',
   },
   {
+    abilities: {
+      search: true,
+    },
     contextWindowTokens: 131_072,
     description:
       'Spark Pro 128K 配置了特大上下文处理能力，能够处理多达128K的上下文信息，特别适合需通篇分析和长期逻辑关联处理的长文内容，可在复杂文本沟通中提供流畅一致的逻辑与多样的引用支持。',
@@ -29,11 +44,15 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'pro-128k',
     maxOutput: 4096,
+    settings: {
+      searchImpl: 'internal',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -42,11 +61,15 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'generalv3.5',
     maxOutput: 8192,
+    settings: {
+      searchImpl: 'internal',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_768,
     description:
@@ -55,11 +78,15 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: 'max-32k',
     maxOutput: 8192,
+    settings: {
+      searchImpl: 'internal',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -68,6 +95,9 @@ const sparkChatModels: AIChatModelCard[] = [
     enabled: true,
     id: '4.0Ultra',
     maxOutput: 8192,
+    settings: {
+      searchImpl: 'internal',
+    },
     type: 'chat',
   },
 ];
diff --git a/src/config/aiModels/stepfun.ts b/src/config/aiModels/stepfun.ts
index 71b40d48d74e8..4559a4181c30c 100644
--- a/src/config/aiModels/stepfun.ts
+++ b/src/config/aiModels/stepfun.ts
@@ -6,6 +6,7 @@ const stepfunChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8000,
     description: '高速模型，适合实时对话。',
@@ -17,11 +18,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 1,
       output: 4,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8000,
     description: '小型模型，适合轻量级任务。',
@@ -33,11 +38,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 5,
       output: 20,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 32_000,
     description: '支持中等长度的对话，适用于多种应用场景。',
@@ -49,11 +58,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 15,
       output: 70,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: '平衡性能与成本，适合一般场景。',
@@ -65,11 +78,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 40,
       output: 200,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 256_000,
     description: '具备超长上下文处理能力，尤其适合长文档分析。',
@@ -80,11 +97,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 95,
       output: 300,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 16_000,
     description: '支持大规模上下文交互，适合复杂对话场景。',
@@ -96,11 +117,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 38,
       output: 120,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8000,
     description: 
@@ -114,11 +139,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       output: 2,
     },
     releasedAt: '2025-01-14',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 16_000,
     description: 'step-2模型的实验版本，包含最新的特性，滚动更新中。不推荐在正式生产环境使用。',
@@ -131,11 +160,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       output: 120,
     },
     releasedAt: '2025-01-15',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 8000,
@@ -148,11 +181,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 5,
       output: 20,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 32_000,
@@ -164,11 +201,15 @@ const stepfunChatModels: AIChatModelCard[] = [
       input: 15,
       output: 70,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
       vision: true,
     },
     contextWindowTokens: 32_000,
@@ -182,6 +223,9 @@ const stepfunChatModels: AIChatModelCard[] = [
       output: 70,
     },
     releasedAt: '2025-01-22',
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/config/aiModels/zhipu.ts b/src/config/aiModels/zhipu.ts
index 71f708aa7f071..6b1dc50837a9b 100644
--- a/src/config/aiModels/zhipu.ts
+++ b/src/config/aiModels/zhipu.ts
@@ -20,6 +20,7 @@ const zhipuChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: 'GLM-4-Flash 是处理简单任务的理想选择，速度最快且免费。',
@@ -31,11 +32,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 0,
       output: 0,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: 'GLM-4-FlashX 是Flash的增强版本，超快推理速度。',
@@ -47,11 +52,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 0.1,
       output: 0.1,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 1_024_000,
     description: 'GLM-4-Long 支持超长文本输入，适合记忆型任务与大规模文档处理。',
@@ -62,11 +71,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 1,
       output: 1,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: 'GLM-4-Air 是性价比高的版本，性能接近GLM-4，提供快速度和实惠的价格。',
@@ -78,11 +91,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 1,
       output: 1,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description: 'GLM-4-AirX 提供 GLM-4-Air 的高效版本，推理速度可达其2.6倍。',
@@ -94,11 +111,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 10,
       output: 10,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description:
@@ -110,11 +131,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 100,
       output: 100,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: 'GLM-4-Plus 作为高智能旗舰，具备强大的处理长文本和复杂任务的能力，性能全面提升。',
@@ -126,11 +151,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 50,
       output: 50,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: 'GLM-4-0520 是最新模型版本，专为高度复杂和多样化任务设计，表现卓越。',
@@ -141,11 +170,15 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 100,
       output: 100,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description: 'GLM-4 是发布于2024年1月的旧旗舰版本，目前已被更强的 GLM-4-0520 取代。',
@@ -156,6 +189,9 @@ const zhipuChatModels: AIChatModelCard[] = [
       input: 100,
       output: 100,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/libs/agent-runtime/ai360/index.ts b/src/libs/agent-runtime/ai360/index.ts
index 11b345bfbb7e9..ea140fbbe8ea1 100644
--- a/src/libs/agent-runtime/ai360/index.ts
+++ b/src/libs/agent-runtime/ai360/index.ts
@@ -13,9 +13,22 @@ export const LobeAi360AI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.360.cn/v1',
   chatCompletion: {
     handlePayload: (payload) => {
+      const { enabledSearch, tools, ...rest } = payload;
+
+      const ai360Tools = enabledSearch ? [
+        ...(tools || []),
+        {
+          type: "web_search",
+          web_search: {
+            search_mode: "auto",
+          },
+        }
+      ] : tools;
+
       return {
-        ...payload,
-        stream: !payload.tools,
+        ...rest,
+        stream: !ai360Tools,
+        tools: ai360Tools,
       } as any;
     },
   },
diff --git a/src/libs/agent-runtime/baichuan/index.ts b/src/libs/agent-runtime/baichuan/index.ts
index cd36499769f4f..aa21caae51b7d 100644
--- a/src/libs/agent-runtime/baichuan/index.ts
+++ b/src/libs/agent-runtime/baichuan/index.ts
@@ -1,5 +1,3 @@
-import OpenAI from 'openai';
-
 import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
@@ -17,14 +15,26 @@ export const LobeBaichuanAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.baichuan-ai.com/v1',
   chatCompletion: {
     handlePayload: (payload: ChatStreamPayload) => {
-      const { temperature, ...rest } = payload;
+      const { enabledSearch, temperature, tools, ...rest } = payload;
+
+      const baichuanTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          type: "web_search",
+          web_search: {
+            enable: true,
+            search_mode: process.env.BAICHUAN_SEARCH_MODE || "performance_first", // performance_first or quality_first
+          },
+        }
+      ] : tools;
 
       return {
         ...rest,
         // [baichuan] frequency_penalty must be between 1 and 2.
         frequency_penalty: undefined,
         temperature: temperature !== undefined ? temperature / 2 : undefined,
-      } as OpenAI.ChatCompletionCreateParamsStreaming;
+        tools: baichuanTools,
+      } as any;
     },
   },
   debug: {
diff --git a/src/libs/agent-runtime/hunyuan/index.ts b/src/libs/agent-runtime/hunyuan/index.ts
index 1abe27d4cc879..728e43157dcde 100644
--- a/src/libs/agent-runtime/hunyuan/index.ts
+++ b/src/libs/agent-runtime/hunyuan/index.ts
@@ -9,6 +9,24 @@ export interface HunyuanModelCard {
 
 export const LobeHunyuanAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.hunyuan.cloud.tencent.com/v1',
+  chatCompletion: {
+    handlePayload: (payload) => {
+      const { enabledSearch, ...rest } = payload;
+
+      return {
+        ...rest,
+        ...(enabledSearch && {
+          /*
+          citation: true,
+          enable_multimedia: true,
+          search_info: true
+          */
+          enable_enhancement: true,
+          enable_speed_search: process.env.HUNYUAN_ENABLE_SPEED_SEARCH === '1',
+        }),
+      } as any;
+    },
+  },
   debug: {
     chatCompletion: () => process.env.DEBUG_HUNYUAN_CHAT_COMPLETION === '1',
   },
diff --git a/src/libs/agent-runtime/minimax/index.ts b/src/libs/agent-runtime/minimax/index.ts
index 6778951558d36..a4a7e0c5637e8 100644
--- a/src/libs/agent-runtime/minimax/index.ts
+++ b/src/libs/agent-runtime/minimax/index.ts
@@ -12,23 +12,14 @@ export const LobeMinimaxAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.minimax.chat/v1',
   chatCompletion: {
     handlePayload: (payload) => {
-      const { temperature, top_p, ...params } = payload;
+      const { max_tokens, temperature, top_p, ...params } = payload;
 
       return {
         ...params,
         frequency_penalty: undefined,
-        max_tokens: payload.max_tokens !== undefined ? payload.max_tokens : getMinimaxMaxOutputs(payload.model),
+        max_tokens: max_tokens !== undefined ? max_tokens : getMinimaxMaxOutputs(payload.model),
         presence_penalty: undefined,
-        stream: true,
         temperature: temperature === undefined || temperature <= 0 ? undefined : temperature / 2,
-        tools: params.tools?.map((tool) => ({
-          function: {
-            description: tool.function.description,
-            name: tool.function.name,
-            parameters: JSON.stringify(tool.function.parameters),
-          },
-          type: 'function',
-        })),
         top_p: top_p !== undefined && top_p > 0 && top_p <= 1 ? top_p : undefined,
       } as any;
     },
diff --git a/src/libs/agent-runtime/moonshot/index.ts b/src/libs/agent-runtime/moonshot/index.ts
index 133ce09219380..3472e931312b1 100644
--- a/src/libs/agent-runtime/moonshot/index.ts
+++ b/src/libs/agent-runtime/moonshot/index.ts
@@ -1,5 +1,3 @@
-import OpenAI from 'openai';
-
 import { ChatStreamPayload, ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
@@ -13,12 +11,23 @@ export const LobeMoonshotAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.moonshot.cn/v1',
   chatCompletion: {
     handlePayload: (payload: ChatStreamPayload) => {
-      const { temperature, ...rest } = payload;
+      const { enabledSearch, temperature, tools, ...rest } = payload;
+
+      const moonshotTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          function: {
+            name: "$web_search",
+          },
+          type: "builtin_function",
+        }
+      ] : tools;
 
       return {
         ...rest,
         temperature: temperature !== undefined ? temperature / 2 : undefined,
-      } as OpenAI.ChatCompletionCreateParamsStreaming;
+        tools: moonshotTools,
+      } as any;
     },
   },
   debug: {
diff --git a/src/libs/agent-runtime/qwen/index.ts b/src/libs/agent-runtime/qwen/index.ts
index 1822b044e1555..3ca2582327f1d 100644
--- a/src/libs/agent-runtime/qwen/index.ts
+++ b/src/libs/agent-runtime/qwen/index.ts
@@ -50,6 +50,10 @@ export const LobeQwenAI = LobeOpenAICompatibleFactory({
         ...(enabledSearch && {
           enable_search: enabledSearch,
           search_options: {
+            /*
+            enable_citation: true,
+            enable_source: true,
+            */
             search_strategy: process.env.QWEN_SEARCH_STRATEGY || 'standard', // standard or pro
           },
         }),
diff --git a/src/libs/agent-runtime/stepfun/index.ts b/src/libs/agent-runtime/stepfun/index.ts
index 03ed6c9bde8c7..945a5b53241fa 100644
--- a/src/libs/agent-runtime/stepfun/index.ts
+++ b/src/libs/agent-runtime/stepfun/index.ts
@@ -11,9 +11,22 @@ export const LobeStepfunAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://api.stepfun.com/v1',
   chatCompletion: {
     handlePayload: (payload) => {
+      const { enabledSearch, tools, ...rest } = payload;
+
+      const stepfunTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          function: {
+            description: "use web_search to search information on the internet",
+          },
+          type: "web_search",
+        }
+      ] : tools;
+
       return {
-        ...payload,
-        stream: !payload.tools,
+        ...rest,
+        stream: !stepfunTools,
+        tools: stepfunTools,
       } as any;
     },
   },
diff --git a/src/libs/agent-runtime/zhipu/index.ts b/src/libs/agent-runtime/zhipu/index.ts
index 3e5f5e1c40606..0995d4f0d9c1c 100644
--- a/src/libs/agent-runtime/zhipu/index.ts
+++ b/src/libs/agent-runtime/zhipu/index.ts
@@ -1,6 +1,4 @@
-import OpenAI from 'openai';
-
-import { ChatStreamPayload, ModelProvider } from '../types';
+import { ModelProvider } from '../types';
 import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 import type { ChatModelCard } from '@/types/llm';
@@ -14,9 +12,21 @@ export interface ZhipuModelCard {
 export const LobeZhipuAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://open.bigmodel.cn/api/paas/v4',
   chatCompletion: {
-    handlePayload: ({ max_tokens, model, temperature, top_p, ...payload }: ChatStreamPayload) =>
-      ({
-        ...payload,
+    handlePayload: (payload) => {
+      const { enabledSearch, max_tokens, model, temperature, tools, top_p, ...rest } = payload;
+
+      const zhipuTools = enabledSearch ? [
+        ...(tools || []),
+        {
+          type: "web_search",
+          web_search: {
+            enable: true,
+          },
+        }
+      ] : tools;
+
+      return {
+        ...rest,
         max_tokens: 
           max_tokens === undefined ? undefined :
           (model.includes('glm-4v') && Math.min(max_tokens, 1024)) ||
@@ -24,6 +34,7 @@ export const LobeZhipuAI = LobeOpenAICompatibleFactory({
           max_tokens,
         model,
         stream: true,
+        tools: zhipuTools,
         ...(model === 'glm-4-alltools'
           ? {
               temperature:
@@ -36,7 +47,8 @@ export const LobeZhipuAI = LobeOpenAICompatibleFactory({
               temperature: temperature !== undefined ? temperature / 2 : undefined,
               top_p,
             }),
-      }) as OpenAI.ChatCompletionCreateParamsStreaming,
+      } as any;
+    },
   },
   constructorOptions: {
     defaultHeaders: {
