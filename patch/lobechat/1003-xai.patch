diff --git a/src/app/(main)/settings/llm/ProviderList/providers.tsx b/src/app/(main)/settings/llm/ProviderList/providers.tsx
index a2e24524f98e..57b762f94cbc 100644
--- a/src/app/(main)/settings/llm/ProviderList/providers.tsx
+++ b/src/app/(main)/settings/llm/ProviderList/providers.tsx
@@ -23,6 +23,7 @@ import {
   TaichuProviderCard,
   TogetherAIProviderCard,
   UpstageProviderCard,
+  XAIProviderCard,
   ZeroOneProviderCard,
   ZhiPuProviderCard,
 } from '@/config/modelProviders';
@@ -67,6 +68,7 @@ export const useProviderList = (): ProviderItem[] => {
       MistralProviderCard,
       Ai21ProviderCard,
       UpstageProviderCard,
+      XAIProviderCard,
       QwenProviderCard,
       WenxinProvider,
       HunyuanProviderCard,
diff --git a/src/config/llm.ts b/src/config/llm.ts
index 8060a708198f..342cc341237a 100644
--- a/src/config/llm.ts
+++ b/src/config/llm.ts
@@ -149,6 +149,10 @@ export const getLLMConfig = () => {
       SENSENOVA_ACCESS_KEY_ID: z.string().optional(),
       SENSENOVA_ACCESS_KEY_SECRET: z.string().optional(),
       SENSENOVA_MODEL_LIST: z.string().optional(),
+
+      ENABLED_XAI: z.boolean(),
+      XAI_API_KEY: z.string().optional(),
+      XAI_MODEL_LIST: z.string().optional(),
     },
     runtimeEnv: {
       API_KEY_SELECT_MODE: process.env.API_KEY_SELECT_MODE,
@@ -295,6 +299,10 @@ export const getLLMConfig = () => {
       SENSENOVA_ACCESS_KEY_ID: process.env.SENSENOVA_ACCESS_KEY_ID,
       SENSENOVA_ACCESS_KEY_SECRET: process.env.SENSENOVA_ACCESS_KEY_SECRET,
       SENSENOVA_MODEL_LIST: process.env.SENSENOVA_MODEL_LIST,
+
+      ENABLED_XAI: !!process.env.XAI_API_KEY,
+      XAI_API_KEY: process.env.XAI_API_KEY,
+      XAI_MODEL_LIST: process.env.XAI_MODEL_LIST,
     },
   });
 };
diff --git a/src/config/modelProviders/index.ts b/src/config/modelProviders/index.ts
index 2237ef877b7c..d32255c415e6 100644
--- a/src/config/modelProviders/index.ts
+++ b/src/config/modelProviders/index.ts
@@ -30,6 +30,7 @@ import TaichuProvider from './taichu';
 import TogetherAIProvider from './togetherai';
 import UpstageProvider from './upstage';
 import WenxinProvider from './wenxin';
+import XAIProvider from './xai';
 import ZeroOneProvider from './zeroone';
 import ZhiPuProvider from './zhipu';
 
@@ -52,6 +53,7 @@ export const LOBE_DEFAULT_MODEL_LIST: ChatModelCard[] = [
   PerplexityProvider.chatModels,
   AnthropicProvider.chatModels,
   HuggingFaceProvider.chatModels,
+  XAIProvider.chatModels,
   ZeroOneProvider.chatModels,
   StepfunProvider.chatModels,
   NovitaProvider.chatModels,
@@ -86,6 +88,7 @@ export const DEFAULT_MODEL_PROVIDER_LIST = [
   MistralProvider,
   Ai21Provider,
   UpstageProvider,
+  XAIProvider,
   QwenProvider,
   WenxinProvider,
   HunyuanProvider,
@@ -141,5 +144,6 @@ export { default as TaichuProviderCard } from './taichu';
 export { default as TogetherAIProviderCard } from './togetherai';
 export { default as UpstageProviderCard } from './upstage';
 export { default as WenxinProviderCard } from './wenxin';
+export { default as XAIProviderCard } from './xai';
 export { default as ZeroOneProviderCard } from './zeroone';
 export { default as ZhiPuProviderCard } from './zhipu';
diff --git a/src/config/modelProviders/xai.ts b/src/config/modelProviders/xai.ts
new file mode 100644
index 000000000000..2971961c17bf
--- /dev/null
+++ b/src/config/modelProviders/xai.ts
@@ -0,0 +1,26 @@
+import { ModelProviderCard } from '@/types/llm';
+
+const XAI: ModelProviderCard = {
+  chatModels: [
+    {
+      description: 'Comparable performance to Grok 2 but with improved efficiency, speed and capabilities.',
+      displayName: 'Grok Beta',
+      enabled: true,
+      functionCall: true,
+      id: 'grok-beta',
+      pricing: {
+        input: 5,
+        output: 15,
+      },
+      tokens: 131_072,
+    },
+  ],
+  checkModel: 'grok-beta',
+  id: 'xai',
+  modelList: { showModelFetcher: true },
+  modelsUrl: 'https://docs.x.ai/docs#models',
+  name: 'xAI',
+  url: 'https://console.x.ai',
+};
+
+export default XAI;
diff --git a/src/const/settings/llm.ts b/src/const/settings/llm.ts
index 9c478db2e95b..39ec2155b4a7 100644
--- a/src/const/settings/llm.ts
+++ b/src/const/settings/llm.ts
@@ -28,6 +28,7 @@ import {
   TogetherAIProviderCard,
   UpstageProviderCard,
   WenxinProviderCard,
+  XAIProviderCard,
   ZeroOneProviderCard,
   ZhiPuProviderCard,
   filterEnabledModels,
@@ -156,6 +157,10 @@ export const DEFAULT_LLM_CONFIG: UserModelProviderConfig = {
     enabled: false,
     enabledModels: filterEnabledModels(WenxinProviderCard),
   },
+  xai: {
+    enabled: false,
+    enabledModels: filterEnabledModels(XAIProviderCard),
+  },
   zeroone: {
     enabled: false,
     enabledModels: filterEnabledModels(ZeroOneProviderCard),
diff --git a/src/libs/agent-runtime/AgentRuntime.ts b/src/libs/agent-runtime/AgentRuntime.ts
index ed6bc9ae16d3..5cbc88df2c2b 100644
--- a/src/libs/agent-runtime/AgentRuntime.ts
+++ b/src/libs/agent-runtime/AgentRuntime.ts
@@ -41,6 +41,7 @@ import {
   TextToSpeechPayload,
 } from './types';
 import { LobeUpstageAI } from './upstage';
+import { LobeXAI } from './xai';
 import { LobeZeroOneAI } from './zeroone';
 import { LobeZhipuAI } from './zhipu';
 
@@ -154,6 +155,7 @@ class AgentRuntime {
       taichu: Partial<ClientOptions>;
       togetherai: Partial<ClientOptions>;
       upstage: Partial<ClientOptions>;
+      xai: Partial<ClientOptions>;
       zeroone: Partial<ClientOptions>;
       zhipu: Partial<ClientOptions>;
     }>,
@@ -321,6 +323,11 @@ class AgentRuntime {
         runtimeModel = await LobeSenseNovaAI.fromAPIKey(params.sensenova);
         break;
       }
+
+      case ModelProvider.XAI: {
+        runtimeModel = new LobeXAI(params.xai);
+        break;
+      }
     }
 
     return new AgentRuntime(runtimeModel);
diff --git a/src/libs/agent-runtime/types/type.ts b/src/libs/agent-runtime/types/type.ts
index db64c94f23fb..d2a9b75a5518 100644
--- a/src/libs/agent-runtime/types/type.ts
+++ b/src/libs/agent-runtime/types/type.ts
@@ -52,6 +52,7 @@ export enum ModelProvider {
   TogetherAI = 'togetherai',
   Upstage = 'upstage',
   Wenxin = 'wenxin',
+  XAI = 'xai',
   ZeroOne = 'zeroone',
   ZhiPu = 'zhipu',
 }
diff --git a/src/libs/agent-runtime/xai/index.ts b/src/libs/agent-runtime/xai/index.ts
new file mode 100644
index 000000000000..ed52caa342b4
--- /dev/null
+++ b/src/libs/agent-runtime/xai/index.ts
@@ -0,0 +1,10 @@
+import { ModelProvider } from '../types';
+import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
+
+export const LobeXAI = LobeOpenAICompatibleFactory({
+  baseURL: 'https://api.x.ai/v1',
+  debug: {
+    chatCompletion: () => process.env.DEBUG_XAI_CHAT_COMPLETION === '1',
+  },
+  provider: ModelProvider.XAI,
+});
diff --git a/src/server/globalConfig/index.ts b/src/server/globalConfig/index.ts
index 09b458389310..baef16f873b8 100644
--- a/src/server/globalConfig/index.ts
+++ b/src/server/globalConfig/index.ts
@@ -33,6 +33,7 @@ import {
   TogetherAIProviderCard,
   UpstageProviderCard,
   WenxinProviderCard,
+  XAIProviderCard,
   ZeroOneProviderCard,
   ZhiPuProviderCard,
 } from '@/config/modelProviders';
@@ -143,6 +144,9 @@ export const getServerGlobalConfig = () => {
 
     ENABLED_HUGGINGFACE,
     HUGGINGFACE_MODEL_LIST,
+
+    ENABLED_XAI,
+    XAI_MODEL_LIST,
   } = getLLMConfig();
 
   const config: GlobalServerConfig = {
@@ -395,6 +399,14 @@ export const getServerGlobalConfig = () => {
           modelString: WENXIN_MODEL_LIST,
         }),
       },
+      xai: {
+        enabled: ENABLED_XAI,
+        enabledModels: extractEnabledModels(XAI_MODEL_LIST),
+        serverModelCards: transformToChatModelCards({
+          defaultChatModels: XAIProviderCard.chatModels,
+          modelString: XAI_MODEL_LIST,
+        }),
+      },
       zeroone: {
         enabled: ENABLED_ZEROONE,
         enabledModels: extractEnabledModels(ZEROONE_MODEL_LIST),
diff --git a/src/server/modules/AgentRuntime/index.ts b/src/server/modules/AgentRuntime/index.ts
index 2043e97423fe..e98141cd3ffe 100644
--- a/src/server/modules/AgentRuntime/index.ts
+++ b/src/server/modules/AgentRuntime/index.ts
@@ -271,6 +271,13 @@ const getLlmOptionsFromPayload = (provider: string, payload: JWTPayload) => {
 
       const apiKey = sensenovaAccessKeyID + ':' + sensenovaAccessKeySecret;
 
+      return { apiKey };
+    }
+    case ModelProvider.XAI: {
+      const { XAI_API_KEY } = getLLMConfig();
+
+      const apiKey = apiKeyManager.pick(payload?.apiKey || XAI_API_KEY);
+
       return { apiKey };
     }
   }
diff --git a/src/types/user/settings/keyVaults.ts b/src/types/user/settings/keyVaults.ts
index 8ff980fa055f..42539c87436d 100644
--- a/src/types/user/settings/keyVaults.ts
+++ b/src/types/user/settings/keyVaults.ts
@@ -59,6 +59,7 @@ export interface UserKeyVaults {
   togetherai?: OpenAICompatibleKeyVault;
   upstage?: OpenAICompatibleKeyVault;
   wenxin?: WenxinKeyVault;
+  xai?: OpenAICompatibleKeyVault;
   zeroone?: OpenAICompatibleKeyVault;
   zhipu?: OpenAICompatibleKeyVault;
 }
