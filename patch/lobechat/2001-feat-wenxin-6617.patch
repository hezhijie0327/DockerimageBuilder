diff --git a/src/config/aiModels/wenxin.ts b/src/config/aiModels/wenxin.ts
index cd193788f9e53..431a8e2dc2c77 100644
--- a/src/config/aiModels/wenxin.ts
+++ b/src/config/aiModels/wenxin.ts
@@ -4,6 +4,7 @@ const wenxinChatModels: AIChatModelCard[] = [
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -16,11 +17,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 0.8,
       output: 2,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -32,11 +37,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 0.8,
       output: 2,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description:
@@ -49,11 +58,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 0.8,
       output: 2,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -66,11 +79,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 30,
       output: 90,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -82,11 +99,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 30,
       output: 90,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -99,11 +120,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 20,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 128_000,
     description:
@@ -116,11 +141,15 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 20,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
     abilities: {
       functionCall: true,
+      search: true,
     },
     contextWindowTokens: 8192,
     description:
@@ -132,6 +161,9 @@ const wenxinChatModels: AIChatModelCard[] = [
       input: 20,
       output: 60,
     },
+    settings: {
+      searchImpl: 'params',
+    },
     type: 'chat',
   },
   {
diff --git a/src/libs/agent-runtime/utils/streams/openai.ts b/src/libs/agent-runtime/utils/streams/openai.ts
index 3cac3e173f294..414f1f11567cb 100644
--- a/src/libs/agent-runtime/utils/streams/openai.ts
+++ b/src/libs/agent-runtime/utils/streams/openai.ts
@@ -128,7 +128,6 @@ export const transformOpenAIStream = (
 
       if (typeof content === 'string') {
         // in Perplexity api, the citation is in every chunk, but we only need to return it once
-
         if ('citations' in chunk && !!chunk.citations && !streamContext?.returnedPplxCitation) {
           streamContext.returnedPplxCitation = true;
 
@@ -142,6 +141,23 @@ export const transformOpenAIStream = (
           ];
         }
 
+        // in Wenxin api, the citation is in the first and last chunk
+        if ('search_results' in chunk && !!chunk.search_results && !streamContext?.returnedWenxinCitation) {
+          streamContext.returnedWenxinCitation = true;
+
+          const citations = (chunk.search_results as any[]).map((item) => {
+            return {
+              title: item.title,
+              url: item.url
+            } as CitationItem;
+          });
+
+          return [
+            { data: { citations }, id: chunk.id, type: 'grounding' },
+            { data: content, id: chunk.id, type: 'text' },
+          ];
+        }
+
         return { data: content, id: chunk.id, type: 'text' };
       }
     }
diff --git a/src/libs/agent-runtime/utils/streams/protocol.ts b/src/libs/agent-runtime/utils/streams/protocol.ts
index fb345e070a805..d45e7143a2f50 100644
--- a/src/libs/agent-runtime/utils/streams/protocol.ts
+++ b/src/libs/agent-runtime/utils/streams/protocol.ts
@@ -12,6 +12,7 @@ export interface StreamContext {
    * this flag is used to check if the pplx citation is returned,and then not return it again
    */
   returnedPplxCitation?: boolean;
+  returnedWenxinCitation?: boolean;
   thinking?: {
     id: string;
     name: string;
diff --git a/src/libs/agent-runtime/wenxin/index.ts b/src/libs/agent-runtime/wenxin/index.ts
index c324e9b9bc695..85bed26c08c5b 100644
--- a/src/libs/agent-runtime/wenxin/index.ts
+++ b/src/libs/agent-runtime/wenxin/index.ts
@@ -3,6 +3,22 @@ import { LobeOpenAICompatibleFactory } from '../utils/openaiCompatibleFactory';
 
 export const LobeWenxinAI = LobeOpenAICompatibleFactory({
   baseURL: 'https://qianfan.baidubce.com/v2',
+  chatCompletion: {
+    handlePayload: (payload) => {
+      const { enabledSearch, ...rest } = payload;
+
+      return {
+        ...rest,
+        ...(enabledSearch && {
+          web_search: {
+            enable: true,
+            enable_citation: true,
+            enable_trace: true,
+          }
+        }),
+      } as any;
+    },
+  },
   debug: {
     chatCompletion: () => process.env.DEBUG_WENXIN_CHAT_COMPLETION === '1',
   },
